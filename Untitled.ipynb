{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-ladder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-madrid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "buried-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pororo import Pororo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "temporal-namibia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Summarizer = Pororo(task='summary', model='extractive', lang='kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decent-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = '네이버랩스 김지원님께, 안녕하세요. 카이스트 윤국진 교수님 연구실에 박사과정 박대희입니다.2월달 월례미팅 일정을 정하고자 연락드렸습니다.교수님과 과제 수행원들 시간을 조사해봤을때 2월 9일 (화) 오후 2시 30분 혹은 같은날 오후 4시에 시간이 가능할 것같습니다.연구원님들께서는 어느 일정이 편하신지 알려주시면 감사하겠습니다.혹시 두 일정 모두 안되신다면 다른 날짜를 알려주시면 다시 조정해보도록 하겠습니다.감사합니다. 박대희 드림'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "molecular-extreme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카이스트 윤국진 교수님 연구실에 박사과정 박대희입니다. 2월달 월례미팅 일정을 정하고자 연락드렸습니다. 교수님과 과제 수행원들 시간을 조사해봤을때 2월 9일 (화) 오후 2시 30분 혹은 같은날 오후 4시에 시간이 가능할 것같습니다.\n"
     ]
    }
   ],
   "source": [
    "sample_summ = Summarizer(sample)\n",
    "print(sample_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "every-light",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decb4c9f226e44baa08979f5535570c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a55b5b766e428f83b704975a554777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02326274c9994d489c9260ab925a6b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7d7c140d9649d8a6052d52cfd05bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df4b5dde41d404bb113ba4217c35407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/682k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Summarizer2 = Pororo(task='summary', model='abstractive', lang='kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "treated-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카이스트 윤국진 교수님 연구실에 박사과정 박대희 교수가 2월달 월례미팅 일정을 정하고자 연락이 왔는데 9일 (화) 오후 2시 30분 혹은 같은날 오후 4시에 시간이 가능할 것 같다고 한다.\n"
     ]
    }
   ],
   "source": [
    "sample_summ2 = Summarizer2(sample_summ)\n",
    "print(sample_summ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spanish-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Summarizer3 = Pororo(task='summary', model='bullet', lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "structural-photography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2월 9일 오후 2시 30분 혹은 같은날 오후 4시에 시간 가능', ' 2월 9일 연구실에 박사과정 박대희 교수님 연구실']\n"
     ]
    }
   ],
   "source": [
    "sample_summ3 = Summarizer3(sample)\n",
    "print(sample_summ3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dried-profession",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'네이버랩스 김지원님께, 안녕하세요. 카이스트 윤국진 교수님 연구실에 박사과정 박대희입니다.2월달 월례미팅 일정을 정하고자 연락드렸습니다.교수님과 과제 수행원들 시간을 조사해봤을때 2월 9일 (화) 오후 2시 30분 혹은 같은날 오후 4시에 시간이 가능할 것같습니다.연구원님들께서는 어느 일정이 편하신지 알려주시면 감사하겠습니다.혹시 두 일정 모두 안되신다면 다른 날짜를 알려주시면 다시 조정해보도록 하겠습니다.감사합니다. 박대희 드림'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-comedy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cognitive-revolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'네이버랩스 김지원님께, 안녕하세요. 카이스트 윤국진 교수님 연구실에 박사과정 박대희입니다.2월달 월례미팅 일정을 정하고자 연락드렸습니다.교수님과 과제 수행원들 시간을 조사해봤을때 2월 9일 (화) 오후 2시 30분 혹은 같은날 오후 4시에 시간이 가능할 것같습니다.연구원님들께서는 어느 일정이 편하신지 알려주시면 감사하겠습니다.혹시 두 일정 모두 안되신다면 다른 날짜를 알려주시면 다시 조정해보도록 하겠습니다.감사합니다. 박대희 드림'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-patio",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-sense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-signature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-clause",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ruled-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vertical-pacific",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/q10')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sophisticated-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redmon_You_Only_Look_CVPR_2016_paper.pdf  Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "current-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_name = 'Redmon_You_Only_Look_CVPR_2016_paper.pdf'\n",
    "pdf = PdfFileReader(pdf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stunning-imagination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.getNumPages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bored-rebel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Title': 'You Only Look Once: Unified, Real-Time Object Detection',\n",
       " '/Producer': 'PyPDF2',\n",
       " '/Author': 'Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi',\n",
       " '/Subject': '2016 IEEE Conference on Computer Vision and Pattern Recognition'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.documentInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "studied-oxford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You Only Look Once: Unified, Real-Time Object Detection'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.documentInfo.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "chief-farming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('/Title', 'You Only Look Once: Unified, Real-Time Object Detection'), ('/Producer', 'PyPDF2'), ('/Author', 'Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi'), ('/Subject', '2016 IEEE Conference on Computer Vision and Pattern Recognition')])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.documentInfo.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "continuous-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page = pdf.getPage(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "urban-orlando",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makingpredictions.Unlikeslidingwindowandregion\n",
      "\n",
      "proposal-basedtechniques,YOLOseestheentireimage\n",
      "\n",
      "duringtrainingandtesttimesoitimplicitlyencodescontex-\n",
      "\n",
      "tualinformationaboutclassesaswellastheirappearance.\n",
      "\n",
      "FastR-CNN,atopdetectionmethod[\n",
      "14],mistakesback-\n",
      "groundpatchesinanimageforobjectsbecauseitcan'tsee\n",
      "\n",
      "thelargercontext.YOLOmakeslessthanhalfthenumber\n",
      "\n",
      "ofbackgrounderrorscomparedtoFastR-CNN.\n",
      "Third,YOLOlearnsgeneralizablerepresentationsofob-\n",
      "jects.Whentrainedonnaturalimagesandtestedonart-\n",
      "\n",
      "work,YOLOoutperformstopdetectionmethodslikeDPM\n",
      "\n",
      "andR-CNNbyawidemargin.SinceYOLOishighlygen-\n",
      "\n",
      "eralizableitislesslikelytobreakdownwhenappliedto\n",
      "\n",
      "newdomainsorunexpectedinputs.\n",
      "YOLOstilllagsbehindstate-of-the-artdetectionsystems\n",
      "inaccuracy.Whileitcanquicklyidentifyobjectsinim-\n",
      "\n",
      "agesitstrugglestopreciselylocalizesomeobjects,espe-\n",
      "\n",
      "ciallysmallones.Weexaminethesetradeoffsfurtherinour\n",
      "\n",
      "experiments.\n",
      "Allofourtrainingandtestingcodeisopensource.A\n",
      "varietyofpretrainedmodelsarealsoavailabletodownload.\n",
      "\n",
      "2.Detection\n",
      "Weunifytheseparatecomponentsofobjectdetection\n",
      "intoasingleneuralnetwork.Ournetworkusesfeatures\n",
      "\n",
      "fromtheentireimagetopredicteachboundingbox.Italso\n",
      "\n",
      "predictsallboundingboxesacrossallclassesforanim-\n",
      "\n",
      "agesimultaneously.Thismeansournetworkreasonsglob-\n",
      "\n",
      "allyaboutthefullimageandalltheobjectsintheimage.\n",
      "\n",
      "TheYOLOdesignenablesend-to-endtrainingandreal-\n",
      "\n",
      "timespeedswhilemaintaininghighaverageprecision.\n",
      "Oursystemdividestheinputimageintoan\n",
      "S\n",
      "\n",
      "S\n",
      "grid.Ifthecenterofanobjectfallsintoagridcell,thatgridcell\n",
      "\n",
      "isresponsiblefordetectingthatobject.\n",
      "Eachgridcellpredicts\n",
      "B\n",
      "boundingboxesand\n",
      "scoresforthoseboxes.Thesescoreshow\n",
      "\n",
      "themodelisthattheboxcontainsanobjectand\n",
      "\n",
      "alsohowaccurateitthinkstheboxisthatitpredicts.For-\n",
      "\n",
      "mallyweas\n",
      "Pr(\n",
      "Object)\n",
      "IOUtruth\n",
      "pred.Ifno\n",
      "objectexistsinthatcell,thescoresshouldbe\n",
      "\n",
      "zero.Otherwisewewantthescoretoequalthe\n",
      "\n",
      "intersectionoverunion(IOU)betweenthepredictedbox\n",
      "\n",
      "andthegroundtruth.\n",
      "Eachboundingboxconsistsof5predictions:\n",
      "x\n",
      ",y\n",
      ",w\n",
      ",h\n",
      ",andThe\n",
      "(\n",
      "x;y\n",
      ")\n",
      "coordinatesrepresentthecenter\n",
      "oftheboxrelativetotheboundsofthegridcell.Thewidth\n",
      "\n",
      "andheightarepredictedrelativetothewholeimage.Finally\n",
      "\n",
      "thepredictionrepresentstheIOUbetweenthe\n",
      "\n",
      "predictedboxandanygroundtruthbox.\n",
      "Eachgridcellalsopredicts\n",
      "C\n",
      "conditionalclassproba-\n",
      "bilities,Pr(\n",
      "Classi\n",
      "j\n",
      "Object)\n",
      ".Theseprobabilitiesarecondi-\n",
      "tionedonthegridcellcontaininganobject.Weonlypredict\n",
      "onesetofclassprobabilitiespergridcell,regardlessofthe\n",
      "\n",
      "numberofboxes\n",
      "B\n",
      ".Attesttimewemultiplytheconditionalclassprobabili-\n",
      "tiesandtheindividualboxpredictions,\n",
      "Pr(\n",
      "Classi\n",
      "j\n",
      "Object)\n",
      "\n",
      "Pr(\n",
      "Object)\n",
      "\n",
      "IOUtruth\n",
      "pred=Pr(\n",
      "Classi\n",
      ")\n",
      "\n",
      "IOUtruth\n",
      "pred(1)whichgivesusscoresforeach\n",
      "\n",
      "box.Thesescoresencodeboththeprobabilityofthatclass\n",
      "\n",
      "appearingintheboxandhowwellthepredictedboxthe\n",
      "\n",
      "object.Figure2:\n",
      "TheModel.\n",
      "Oursystemmodelsdetectionasaregres-\n",
      "sionproblem.Itdividestheimageintoan\n",
      "S\n",
      "\n",
      "S\n",
      "gridandforeach\n",
      "gridcellpredicts\n",
      "B\n",
      "boundingboxes,forthoseboxes,\n",
      "andC\n",
      "classprobabilities.Thesepredictionsareencodedasan\n",
      "S\n",
      "\n",
      "S\n",
      "\n",
      "(\n",
      "B\n",
      "\n",
      "5+\n",
      "C\n",
      ")\n",
      "tensor.\n",
      "ForevaluatingYOLOonP\n",
      "ASCAL\n",
      "VOC,weuse\n",
      "S\n",
      "=7\n",
      ",B\n",
      "=2\n",
      ".P\n",
      "ASCAL\n",
      "VOChas20labelledclassesso\n",
      "C\n",
      "=20\n",
      ".Ourpredictionisa\n",
      "7\n",
      "7\n",
      "30tensor.\n",
      "2.1.NetworkDesign\n",
      "Weimplementthismodelasaconvolutionalneuralnet-\n",
      "workandevaluateitontheP\n",
      "ASCAL\n",
      "VOCdetectiondataset\n",
      "[9].Theinitialconvolutionallayersofthenetworkextract\n",
      "featuresfromtheimagewhilethefullyconnectedlayers\n",
      "\n",
      "predicttheoutputprobabilitiesandcoordinates.\n",
      "OurnetworkarchitectureisinspiredbytheGoogLeNet\n",
      "modelforimage[\n",
      "33].Ournetworkhas24\n",
      "convolutionallayersfollowedby2fullyconnectedlayers.\n",
      "\n",
      "InsteadoftheinceptionmodulesusedbyGoogLeNet,we\n",
      "\n",
      "simplyuse\n",
      "1\n",
      "1reductionlayersfollowedby\n",
      "3\n",
      "3convo-\n",
      "lutionallayers,similartoLinetal[\n",
      "22].Thefullnetworkis\n",
      "showninFigure\n",
      "3.WealsotrainafastversionofYOLOdesignedtopush\n",
      "theboundariesoffastobjectdetection.FastYOLOusesa\n",
      "\n",
      "neuralnetworkwithfewerconvolutionallayers(9instead\n",
      "\n",
      "of24)andfewerinthoselayers.Otherthanthesize\n",
      "\n",
      "ofthenetwork,alltrainingandtestingparametersarethe\n",
      "\n",
      "samebetweenYOLOandFastYOLO.\n",
      "780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(first_page.extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "silver-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouOnlyLookOnce:\n",
      "Real-TimeObjectDetection\n",
      "JosephRedmon\n",
      "\n",
      ",SantoshDivvala\n",
      "y\n",
      ",RossGirshick\n",
      "{\n",
      ",AliFarhadi\n",
      "y\n",
      "UniversityofWashington\n",
      "\n",
      ",AllenInstituteforAI\n",
      "y\n",
      ",FacebookAIResearch\n",
      "{\n",
      "http://pjreddie.com/yolo/AbstractWepresentYOLO,anewapproachtoobjectdetection.\n",
      "Priorworkonobjectdetectionrepurposesstoper-\n",
      "\n",
      "formdetection.Instead,weframeobjectdetectionasare-\n",
      "\n",
      "gressionproblemtospatiallyseparatedboundingboxesand\n",
      "\n",
      "associatedclassprobabilities.Asingleneuralnetworkpre-\n",
      "\n",
      "dictsboundingboxesandclassprobabilitiesdirectlyfrom\n",
      "\n",
      "fullimagesinoneevaluation.Sincethewholedetection\n",
      "\n",
      "pipelineisasinglenetwork,itcanbeoptimizedend-to-end\n",
      "\n",
      "directlyondetectionperformance.\n",
      "Ourarchitectureisextremelyfast.Ourbase\n",
      "YOLOmodelprocessesimagesinreal-timeat45frames\n",
      "\n",
      "persecond.Asmallerversionofthenetwork,FastYOLO,\n",
      "\n",
      "processesanastounding155framespersecondwhile\n",
      "\n",
      "stillachievingdoublethemAPofotherreal-timedetec-\n",
      "\n",
      "tors.Comparedtostate-of-the-artdetectionsystems,YOLO\n",
      "\n",
      "makesmorelocalizationerrorsbutislesslikelytopredict\n",
      "\n",
      "falsepositivesonbackground.Finally,YOLOlearnsvery\n",
      "\n",
      "generalrepresentationsofobjects.Itoutperformsotherde-\n",
      "\n",
      "tectionmethods,includingDPMandR-CNN,whengener-\n",
      "\n",
      "alizingfromnaturalimagestootherdomainslikeartwork.\n",
      "\n",
      "1.Introduction\n",
      "Humansglanceatanimageandinstantlyknowwhatob-\n",
      "jectsareintheimage,wheretheyare,andhowtheyinter-\n",
      "\n",
      "act.Thehumanvisualsystemisfastandaccurate,allow-\n",
      "\n",
      "ingustoperformcomplextaskslikedrivingwithlittlecon-\n",
      "\n",
      "sciousthought.Fast,accuratealgorithmsforobjectdetec-\n",
      "\n",
      "tionwouldallowcomputerstodrivecarswithoutspecial-\n",
      "\n",
      "izedsensors,enableassistivedevicestoconveyreal-time\n",
      "\n",
      "sceneinformationtohumanusers,andunlockthepotential\n",
      "\n",
      "forgeneralpurpose,responsiveroboticsystems.\n",
      "Currentdetectionsystemsrepurposetoper-\n",
      "formdetection.Todetectanobject,thesesystemstakea\n",
      "\n",
      "forthatobjectandevaluateitatvariouslocations\n",
      "\n",
      "andscalesinatestimage.Systemslikedeformableparts\n",
      "\n",
      "models(DPM)useaslidingwindowapproachwherethe\n",
      "\n",
      "isrunatevenlyspacedlocationsovertheentire\n",
      "\n",
      "image[\n",
      "10].MorerecentapproacheslikeR-CNNuseregionproposal\n",
      "1. Resize image.\n",
      "\n",
      "2. Run convolutional network.\n",
      "\n",
      "3. Non-max suppression.\n",
      "Dog: 0.30Person: 0.64\n",
      "Horse: 0.28Figure1:\n",
      "TheYOLODetectionSystem.\n",
      "Processingimages\n",
      "withYOLOissimpleandstraightforward.Oursystem(1)resizes\n",
      "\n",
      "theinputimageto\n",
      "448\n",
      "\n",
      "448\n",
      ",(2)runsasingleconvolutionalnet-\n",
      "workontheimage,and(3)thresholdstheresultingdetectionsby\n",
      "\n",
      "themodel's\n",
      "\n",
      "methodstogeneratepotentialboundingboxesinanim-\n",
      "\n",
      "ageandthenrunaontheseproposedboxes.After\n",
      "\n",
      "post-processingisusedtothebound-\n",
      "\n",
      "ingboxes,eliminateduplicatedetections,andrescorethe\n",
      "\n",
      "boxesbasedonotherobjectsinthescene[\n",
      "13].Thesecom-\n",
      "plexpipelinesareslowandhardtooptimizebecauseeach\n",
      "\n",
      "individualcomponentmustbetrainedseparately.\n",
      "Wereframeobjectdetectionasasingleregressionprob-\n",
      "lem,straightfromimagepixelstoboundingboxcoordi-\n",
      "\n",
      "natesandclassprobabilities.Usingoursystem,youonly\n",
      "\n",
      "lookonce(YOLO)atanimagetopredictwhatobjectsare\n",
      "\n",
      "presentandwheretheyare.\n",
      "YOLOisrefreshinglysimple:seeFigure\n",
      "1.Asin-\n",
      "gleconvolutionalnetworksimultaneouslypredictsmulti-\n",
      "\n",
      "pleboundingboxesandclassprobabilitiesforthoseboxes.\n",
      "\n",
      "YOLOtrainsonfullimagesanddirectlyoptimizesdetec-\n",
      "\n",
      "tionperformance.Thismodelhasseveral\n",
      "\n",
      "overtraditionalmethodsofobjectdetection.\n",
      "First,YOLOisextremelyfast.Sinceweframedetection\n",
      "asaregressionproblemwedon'tneedacomplexpipeline.\n",
      "\n",
      "Wesimplyrunourneuralnetworkonanewimageattest\n",
      "\n",
      "timetopredictdetections.Ourbasenetworkrunsat45\n",
      "\n",
      "framespersecondwithnobatchprocessingonaTitanX\n",
      "\n",
      "GPUandafastversionrunsatmorethan150fps.This\n",
      "\n",
      "meanswecanprocessstreamingvideoinreal-timewith\n",
      "\n",
      "lessthan25millisecondsoflatency.Furthermore,YOLO\n",
      "\n",
      "achievesmorethantwicethemeanaverageprecisionof\n",
      "\n",
      "otherreal-timesystems.Forademoofoursystemrunning\n",
      "\n",
      "inreal-timeonawebcampleaseseeourprojectwebpage:\n",
      "http://pjreddie.com/yolo/.Second,YOLOreasonsgloballyabouttheimagewhen\n",
      "1779\n",
      "\n",
      "makingpredictions.Unlikeslidingwindowandregion\n",
      "\n",
      "proposal-basedtechniques,YOLOseestheentireimage\n",
      "\n",
      "duringtrainingandtesttimesoitimplicitlyencodescontex-\n",
      "\n",
      "tualinformationaboutclassesaswellastheirappearance.\n",
      "\n",
      "FastR-CNN,atopdetectionmethod[\n",
      "14],mistakesback-\n",
      "groundpatchesinanimageforobjectsbecauseitcan'tsee\n",
      "\n",
      "thelargercontext.YOLOmakeslessthanhalfthenumber\n",
      "\n",
      "ofbackgrounderrorscomparedtoFastR-CNN.\n",
      "Third,YOLOlearnsgeneralizablerepresentationsofob-\n",
      "jects.Whentrainedonnaturalimagesandtestedonart-\n",
      "\n",
      "work,YOLOoutperformstopdetectionmethodslikeDPM\n",
      "\n",
      "andR-CNNbyawidemargin.SinceYOLOishighlygen-\n",
      "\n",
      "eralizableitislesslikelytobreakdownwhenappliedto\n",
      "\n",
      "newdomainsorunexpectedinputs.\n",
      "YOLOstilllagsbehindstate-of-the-artdetectionsystems\n",
      "inaccuracy.Whileitcanquicklyidentifyobjectsinim-\n",
      "\n",
      "agesitstrugglestopreciselylocalizesomeobjects,espe-\n",
      "\n",
      "ciallysmallones.Weexaminethesetradeoffsfurtherinour\n",
      "\n",
      "experiments.\n",
      "Allofourtrainingandtestingcodeisopensource.A\n",
      "varietyofpretrainedmodelsarealsoavailabletodownload.\n",
      "\n",
      "2.Detection\n",
      "Weunifytheseparatecomponentsofobjectdetection\n",
      "intoasingleneuralnetwork.Ournetworkusesfeatures\n",
      "\n",
      "fromtheentireimagetopredicteachboundingbox.Italso\n",
      "\n",
      "predictsallboundingboxesacrossallclassesforanim-\n",
      "\n",
      "agesimultaneously.Thismeansournetworkreasonsglob-\n",
      "\n",
      "allyaboutthefullimageandalltheobjectsintheimage.\n",
      "\n",
      "TheYOLOdesignenablesend-to-endtrainingandreal-\n",
      "\n",
      "timespeedswhilemaintaininghighaverageprecision.\n",
      "Oursystemdividestheinputimageintoan\n",
      "S\n",
      "\n",
      "S\n",
      "grid.Ifthecenterofanobjectfallsintoagridcell,thatgridcell\n",
      "\n",
      "isresponsiblefordetectingthatobject.\n",
      "Eachgridcellpredicts\n",
      "B\n",
      "boundingboxesand\n",
      "scoresforthoseboxes.Thesescoreshow\n",
      "\n",
      "themodelisthattheboxcontainsanobjectand\n",
      "\n",
      "alsohowaccurateitthinkstheboxisthatitpredicts.For-\n",
      "\n",
      "mallyweas\n",
      "Pr(\n",
      "Object)\n",
      "IOUtruth\n",
      "pred.Ifno\n",
      "objectexistsinthatcell,thescoresshouldbe\n",
      "\n",
      "zero.Otherwisewewantthescoretoequalthe\n",
      "\n",
      "intersectionoverunion(IOU)betweenthepredictedbox\n",
      "\n",
      "andthegroundtruth.\n",
      "Eachboundingboxconsistsof5predictions:\n",
      "x\n",
      ",y\n",
      ",w\n",
      ",h\n",
      ",andThe\n",
      "(\n",
      "x;y\n",
      ")\n",
      "coordinatesrepresentthecenter\n",
      "oftheboxrelativetotheboundsofthegridcell.Thewidth\n",
      "\n",
      "andheightarepredictedrelativetothewholeimage.Finally\n",
      "\n",
      "thepredictionrepresentstheIOUbetweenthe\n",
      "\n",
      "predictedboxandanygroundtruthbox.\n",
      "Eachgridcellalsopredicts\n",
      "C\n",
      "conditionalclassproba-\n",
      "bilities,Pr(\n",
      "Classi\n",
      "j\n",
      "Object)\n",
      ".Theseprobabilitiesarecondi-\n",
      "tionedonthegridcellcontaininganobject.Weonlypredict\n",
      "onesetofclassprobabilitiespergridcell,regardlessofthe\n",
      "\n",
      "numberofboxes\n",
      "B\n",
      ".Attesttimewemultiplytheconditionalclassprobabili-\n",
      "tiesandtheindividualboxpredictions,\n",
      "Pr(\n",
      "Classi\n",
      "j\n",
      "Object)\n",
      "\n",
      "Pr(\n",
      "Object)\n",
      "\n",
      "IOUtruth\n",
      "pred=Pr(\n",
      "Classi\n",
      ")\n",
      "\n",
      "IOUtruth\n",
      "pred(1)whichgivesusscoresforeach\n",
      "\n",
      "box.Thesescoresencodeboththeprobabilityofthatclass\n",
      "\n",
      "appearingintheboxandhowwellthepredictedboxthe\n",
      "\n",
      "object.Figure2:\n",
      "TheModel.\n",
      "Oursystemmodelsdetectionasaregres-\n",
      "sionproblem.Itdividestheimageintoan\n",
      "S\n",
      "\n",
      "S\n",
      "gridandforeach\n",
      "gridcellpredicts\n",
      "B\n",
      "boundingboxes,forthoseboxes,\n",
      "andC\n",
      "classprobabilities.Thesepredictionsareencodedasan\n",
      "S\n",
      "\n",
      "S\n",
      "\n",
      "(\n",
      "B\n",
      "\n",
      "5+\n",
      "C\n",
      ")\n",
      "tensor.\n",
      "ForevaluatingYOLOonP\n",
      "ASCAL\n",
      "VOC,weuse\n",
      "S\n",
      "=7\n",
      ",B\n",
      "=2\n",
      ".P\n",
      "ASCAL\n",
      "VOChas20labelledclassesso\n",
      "C\n",
      "=20\n",
      ".Ourpredictionisa\n",
      "7\n",
      "7\n",
      "30tensor.\n",
      "2.1.NetworkDesign\n",
      "Weimplementthismodelasaconvolutionalneuralnet-\n",
      "workandevaluateitontheP\n",
      "ASCAL\n",
      "VOCdetectiondataset\n",
      "[9].Theinitialconvolutionallayersofthenetworkextract\n",
      "featuresfromtheimagewhilethefullyconnectedlayers\n",
      "\n",
      "predicttheoutputprobabilitiesandcoordinates.\n",
      "OurnetworkarchitectureisinspiredbytheGoogLeNet\n",
      "modelforimage[\n",
      "33].Ournetworkhas24\n",
      "convolutionallayersfollowedby2fullyconnectedlayers.\n",
      "\n",
      "InsteadoftheinceptionmodulesusedbyGoogLeNet,we\n",
      "\n",
      "simplyuse\n",
      "1\n",
      "1reductionlayersfollowedby\n",
      "3\n",
      "3convo-\n",
      "lutionallayers,similartoLinetal[\n",
      "22].Thefullnetworkis\n",
      "showninFigure\n",
      "3.WealsotrainafastversionofYOLOdesignedtopush\n",
      "theboundariesoffastobjectdetection.FastYOLOusesa\n",
      "\n",
      "neuralnetworkwithfewerconvolutionallayers(9instead\n",
      "\n",
      "of24)andfewerinthoselayers.Otherthanthesize\n",
      "\n",
      "ofthenetwork,alltrainingandtestingparametersarethe\n",
      "\n",
      "samebetweenYOLOandFastYOLO.\n",
      "780\n",
      "\n",
      "Figure3:\n",
      "TheArchitecture.\n",
      "Ourdetectionnetworkhas24convolutionallayersfollowedby2fullyconnectedlayers.Alternating\n",
      "1\n",
      "\n",
      "1\n",
      "convolutionallayersreducethefeaturesspacefromprecedinglayers.WepretraintheconvolutionallayersontheImageNet\n",
      "\n",
      "taskathalftheresolution(\n",
      "224\n",
      "\n",
      "224\n",
      "inputimage)andthendoubletheresolutionfordetection.\n",
      "Theoutputofournetworkisthe\n",
      "7\n",
      "7\n",
      "30tensorofpredictions.\n",
      "\n",
      "2.2.Training\n",
      "WepretrainourconvolutionallayersontheImageNet\n",
      "1000-classcompetitiondataset[\n",
      "29].Forpretrainingweuse\n",
      "the20convolutionallayersfromFigure\n",
      "3followedbya\n",
      "average-poolinglayerandafullyconnectedlayer.Wetrain\n",
      "\n",
      "thisnetworkforapproximatelyaweekandachieveasingle\n",
      "\n",
      "croptop-5accuracyof88%ontheImageNet2012valida-\n",
      "\n",
      "tionset,comparabletotheGoogLeNetmodelsinCaffe's\n",
      "\n",
      "ModelZoo[\n",
      "24].Wethenconvertthemodeltoperformdetection.Renet\n",
      "al.showthataddingbothconvolutionalandconnectedlay-\n",
      "\n",
      "erstopretrainednetworkscanimproveperformance[\n",
      "28].Followingtheirexample,weaddfourconvolutionallay-\n",
      "\n",
      "ersandtwofullyconnectedlayerswithrandomlyinitialized\n",
      "\n",
      "weights.Detectionoftenrequiresvisualinfor-\n",
      "\n",
      "mationsoweincreasetheinputresolutionofthenetwork\n",
      "\n",
      "from224\n",
      "224to448\n",
      "448.Ourlayerpredictsbothclassprobabilitiesand\n",
      "boundingboxcoordinates.Wenormalizetheboundingbox\n",
      "\n",
      "widthandheightbytheimagewidthandheightsothatthey\n",
      "\n",
      "fallbetween0and1.Weparametrizetheboundingbox\n",
      "x\n",
      "andy\n",
      "coordinatestobeoffsetsofaparticulargridcellloca-\n",
      "tionsotheyarealsoboundedbetween0and1.\n",
      "Weusealinearactivationfunctionforthelayerand\n",
      "allotherlayersusethefollowingleakylinearacti-\n",
      "\n",
      "vation:\n",
      "˚\n",
      "(\n",
      "x\n",
      ")=\n",
      "(\n",
      "x;\n",
      "ifx>\n",
      "00:\n",
      "1x;\n",
      "otherwise(2)Weoptimizeforsum-squarederrorintheoutputofour\n",
      "model.Weusesum-squarederrorbecauseitiseasytoop-\n",
      "timize,howeveritdoesnotperfectlyalignwithourgoalof\n",
      "\n",
      "maximizingaverageprecision.Itweightslocalizationer-\n",
      "\n",
      "rorequallywitherrorwhichmaynotbeideal.\n",
      "\n",
      "Also,ineveryimagemanygridcellsdonotcontainany\n",
      "\n",
      "object.Thispushesthescoresofthosecells\n",
      "\n",
      "towardszero,oftenoverpoweringthegradientfromcells\n",
      "\n",
      "thatdocontainobjects.Thiscanleadtomodelinstability,\n",
      "\n",
      "causingtrainingtodivergeearlyon.\n",
      "Toremedythis,weincreasethelossfromboundingbox\n",
      "coordinatepredictionsanddecreasethelossfrom\n",
      "\n",
      "dencepredictionsforboxesthatdon'tcontainobjects.We\n",
      "\n",
      "usetwoparameters,\n",
      "\n",
      "coordand\n",
      "noobjtoaccomplishthis.We\n",
      "set\n",
      "coord=5\n",
      "and\n",
      "noobj=\n",
      ":\n",
      "5.Sum-squarederroralsoequallyweightserrorsinlarge\n",
      "boxesandsmallboxes.Ourerrormetricshouldthat\n",
      "\n",
      "smalldeviationsinlargeboxesmatterlessthaninsmall\n",
      "\n",
      "boxes.Topartiallyaddressthiswepredictthesquareroot\n",
      "\n",
      "oftheboundingboxwidthandheightinsteadofthewidth\n",
      "\n",
      "andheightdirectly.\n",
      "YOLOpredictsmultipleboundingboxespergridcell.\n",
      "Attrainingtimeweonlywantoneboundingboxpredictor\n",
      "\n",
      "toberesponsibleforeachobject.Weassignonepredictor\n",
      "\n",
      "tobeﬁresponsibleﬂforpredictinganobjectbasedonwhich\n",
      "\n",
      "predictionhasthehighestcurrentIOUwiththeground\n",
      "\n",
      "truth.Thisleadstospecializationbetweentheboundingbox\n",
      "\n",
      "predictors.Eachpredictorgetsbetteratpredictingcertain\n",
      "\n",
      "sizes,aspectratios,orclassesofobject,improvingoverall\n",
      "\n",
      "recall.Duringtrainingweoptimizethefollowing,multi-part\n",
      "781\n",
      "\n",
      "lossfunction:\n",
      "\n",
      "coordS\n",
      "2\n",
      "Xi\n",
      "=0\n",
      "BXj\n",
      "=0\n",
      "1obj\n",
      "ij\n",
      "h\n",
      "(\n",
      "x\n",
      "i\n",
      "\n",
      "^\n",
      "x\n",
      "i\n",
      ")\n",
      "2\n",
      "+(\n",
      "y\n",
      "i\n",
      "\n",
      "^\n",
      "y\n",
      "i\n",
      ")\n",
      "2\n",
      "i\n",
      "+\n",
      "coordS\n",
      "2\n",
      "Xi\n",
      "=0\n",
      "BXj\n",
      "=0\n",
      "1obj\n",
      "ij\n",
      "\n",
      "\n",
      "p\n",
      "w\n",
      "i\n",
      "\n",
      "p^\n",
      "w\n",
      "i\n",
      "\n",
      "2\n",
      "+\n",
      "ph\n",
      "i\n",
      "\n",
      "q^\n",
      "h\n",
      "i\n",
      "\n",
      "2\n",
      "\n",
      "+S\n",
      "2\n",
      "Xi\n",
      "=0\n",
      "BXj\n",
      "=0\n",
      "1obj\n",
      "ij\n",
      "\n",
      "C\n",
      "i\n",
      "\n",
      "^\n",
      "C\n",
      "i\n",
      "\n",
      "2\n",
      "+\n",
      "noobjS\n",
      "2\n",
      "Xi\n",
      "=0\n",
      "BXj\n",
      "=0\n",
      "1noobj\n",
      "ij\n",
      "\n",
      "C\n",
      "i\n",
      "\n",
      "^\n",
      "C\n",
      "i\n",
      "\n",
      "2\n",
      "+S\n",
      "2\n",
      "Xi\n",
      "=0\n",
      "1obj\n",
      "i\n",
      "Xc\n",
      "2\n",
      "classes(\n",
      "p\n",
      "i\n",
      "(\n",
      "c\n",
      ")\n",
      "\n",
      "^\n",
      "p\n",
      "i\n",
      "(\n",
      "c\n",
      "))\n",
      "2\n",
      "(3)where1obj\n",
      "i\n",
      "denotesifobjectappearsincell\n",
      "i\n",
      "and1obj\n",
      "ij\n",
      "de-notesthatthe\n",
      "j\n",
      "thboundingboxpredictorincell\n",
      "i\n",
      "isﬁre-\n",
      "sponsibleﬂforthatprediction.\n",
      "Notethatthelossfunctiononlypenalizes\n",
      "errorifanobjectispresentinthatgridcell(hencethecon-\n",
      "\n",
      "ditionalclassprobabilitydiscussedearlier).Italsoonlype-\n",
      "\n",
      "nalizesboundingboxcoordinateerrorifthatpredictoris\n",
      "\n",
      "ﬁresponsibleﬂforthegroundtruthbox(i.e.hasthehighest\n",
      "\n",
      "IOUofanypredictorinthatgridcell).\n",
      "Wetrainthenetworkforabout135epochsonthetrain-\n",
      "ingandvalidationdatasetsfromP\n",
      "ASCAL\n",
      "VOC2007and\n",
      "2012.Whentestingon2012wealsoincludetheVOC2007\n",
      "\n",
      "testdatafortraining.Throughouttrainingweuseabatch\n",
      "\n",
      "sizeof64,amomentumof\n",
      "0:\n",
      "9andadecayof\n",
      "0:\n",
      "0005.Ourlearningratescheduleisasfollows:Forthe\n",
      "epochsweslowlyraisethelearningratefrom\n",
      "10\n",
      "3\n",
      "to10\n",
      "2\n",
      ".Ifwestartatahighlearningrateourmodeloftendiverges\n",
      "\n",
      "duetounstablegradients.Wecontinuetrainingwith\n",
      "10\n",
      "2\n",
      "for75epochs,then\n",
      "10\n",
      "3\n",
      "for30epochs,and\n",
      "10\n",
      "4\n",
      "for30epochs.\n",
      "Toavoidovweusedropoutandextensivedata\n",
      "augmentation.Adropoutlayerwithrate=.5afterthe\n",
      "\n",
      "connectedlayerpreventsco-adaptationbetweenlayers[\n",
      "18].Fordataaugmentationweintroducerandomscalingand\n",
      "\n",
      "translationsofupto20%oftheoriginalimagesize.We\n",
      "\n",
      "alsorandomlyadjusttheexposureandsaturationoftheim-\n",
      "\n",
      "agebyuptoafactorof\n",
      "1:\n",
      "5intheHSVcolorspace.\n",
      "2.3.Inference\n",
      "Justlikeintraining,predictingdetectionsforatestimage\n",
      "onlyrequiresonenetworkevaluation.OnP\n",
      "ASCAL\n",
      "VOCthe\n",
      "networkpredicts98boundingboxesperimageandclass\n",
      "\n",
      "probabilitiesforeachbox.YOLOisextremelyfastattest\n",
      "\n",
      "timesinceitonlyrequiresasinglenetworkevaluation,un-\n",
      "\n",
      "like-basedmethods.\n",
      "Thegriddesignenforcesspatialdiversityinthebound-\n",
      "ingboxpredictions.Oftenitisclearwhichgridcellan\n",
      "\n",
      "objectfallsintoandthenetworkonlypredictsoneboxfor\n",
      "\n",
      "eachobject.However,somelargeobjectsorobjectsnear\n",
      "theborderofmultiplecellscanbewelllocalizedbymulti-\n",
      "\n",
      "plecells.Non-maximalsuppressioncanbeusedtothese\n",
      "\n",
      "multipledetections.Whilenotcriticaltoperformanceasit\n",
      "\n",
      "isforR-CNNorDPM,non-maximalsuppressionadds2-\n",
      "\n",
      "3%inmAP.\n",
      "\n",
      "2.4.LimitationsofYOLO\n",
      "YOLOimposesstrongspatialconstraintsonbounding\n",
      "boxpredictionssinceeachgridcellonlypredictstwoboxes\n",
      "\n",
      "andcanonlyhaveoneclass.Thisspatialconstraintlim-\n",
      "\n",
      "itsthenumberofnearbyobjectsthatourmodelcanpre-\n",
      "\n",
      "dict.Ourmodelstruggleswithsmallobjectsthatappearin\n",
      "\n",
      "groups,suchasofbirds.\n",
      "Sinceourmodellearnstopredictboundingboxesfrom\n",
      "data,itstrugglestogeneralizetoobjectsinneworunusual\n",
      "\n",
      "aspectratiosorOurmodelalsousesrela-\n",
      "\n",
      "tivelycoarsefeaturesforpredictingboundingboxessince\n",
      "\n",
      "ourarchitecturehasmultipledownsamplinglayersfromthe\n",
      "\n",
      "inputimage.\n",
      "Finally,whilewetrainonalossfunctionthatapproxi-\n",
      "matesdetectionperformance,ourlossfunctiontreatserrors\n",
      "\n",
      "thesameinsmallboundingboxesversuslargebounding\n",
      "\n",
      "boxes.Asmallerrorinalargeboxisgenerallybenignbuta\n",
      "\n",
      "smallerrorinasmallboxhasamuchgreatereffectonIOU.\n",
      "\n",
      "Ourmainsourceoferrorisincorrectlocalizations.\n",
      "\n",
      "3.ComparisontoOtherDetectionSystems\n",
      "Objectdetectionisacoreproblemincomputervision.\n",
      "Detectionpipelinesgenerallystartbyextractingasetof\n",
      "\n",
      "robustfeaturesfrominputimages(Haar[\n",
      "25],SIFT[\n",
      "23],HOG[\n",
      "4],convolutionalfeatures[\n",
      "6]).Then,\n",
      "[35,21,13,10]orlocalizers[\n",
      "1,31]areusedtoidentify\n",
      "objectsinthefeaturespace.Theseorlocalizers\n",
      "\n",
      "areruneitherinslidingwindowfashionoverthewholeim-\n",
      "\n",
      "ageoronsomesubsetofregionsintheimage[\n",
      "34,15,38].WecomparetheYOLOdetectionsystemtoseveraltopde-\n",
      "\n",
      "tectionframeworks,highlightingkeysimilaritiesanddiffer-\n",
      "\n",
      "ences.Deformablepartsmodels.\n",
      "Deformablepartsmodels\n",
      "(DPM)useaslidingwindowapproachtoobjectdetection\n",
      "\n",
      "[10].DPMusesadisjointpipelinetoextractstaticfeatures,\n",
      "classifyregions,predictboundingboxesforhighscoring\n",
      "\n",
      "regions,etc.Oursystemreplacesallofthesedisparateparts\n",
      "\n",
      "withasingleconvolutionalneuralnetwork.Thenetwork\n",
      "\n",
      "performsfeatureextraction,boundingboxprediction,non-\n",
      "\n",
      "maximalsuppression,andcontextualreasoningallconcur-\n",
      "\n",
      "rently.Insteadofstaticfeatures,thenetworktrainsthefea-\n",
      "\n",
      "turesin-lineandoptimizesthemforthedetectiontask.Our\n",
      "\n",
      "architectureleadstoafaster,moreaccuratemodel\n",
      "\n",
      "thanDPM.\n",
      "R-CNN.R-CNNanditsvariantsuseregionproposalsin-\n",
      "steadofslidingwindowstoobjectsinimages.Selective\n",
      "782\n",
      "\n",
      "Search[\n",
      "34]generatespotentialboundingboxes,aconvolu-\n",
      "tionalnetworkextractsfeatures,anSVMscorestheboxes,a\n",
      "\n",
      "linearmodeladjuststheboundingboxes,andnon-maxsup-\n",
      "\n",
      "pressioneliminatesduplicatedetections.Eachstageofthis\n",
      "\n",
      "complexpipelinemustbepreciselytunedindependently\n",
      "\n",
      "andtheresultingsystemisveryslow,takingmorethan40\n",
      "\n",
      "secondsperimageattesttime[\n",
      "14].YOLOsharessomesimilaritieswithR-CNN.Eachgrid\n",
      "cellproposespotentialboundingboxesandscoresthose\n",
      "\n",
      "boxesusingconvolutionalfeatures.However,oursystem\n",
      "\n",
      "putsspatialconstraintsonthegridcellproposalswhich\n",
      "\n",
      "helpsmitigatemultipledetectionsofthesameobject.Our\n",
      "\n",
      "systemalsoproposesfarfewerboundingboxes,only98\n",
      "\n",
      "perimagecomparedtoabout2000fromSelectiveSearch.\n",
      "\n",
      "Finally,oursystemcombinestheseindividualcomponents\n",
      "\n",
      "intoasingle,jointlyoptimizedmodel.\n",
      "OtherFastDetectors\n",
      "FastandFasterR-CNNfocuson\n",
      "speedinguptheR-CNNframeworkbysharingcomputa-\n",
      "\n",
      "tionandusingneuralnetworkstoproposeregionsinstead\n",
      "\n",
      "ofSelectiveSearch[\n",
      "14][\n",
      "27].Whiletheyofferspeedand\n",
      "accuracyimprovementsoverR-CNN,bothstillfallshortof\n",
      "\n",
      "real-timeperformance.\n",
      "ManyresearcheffortsfocusonspeedinguptheDPM\n",
      "pipeline[\n",
      "30][\n",
      "37][\n",
      "5].TheyspeedupHOGcomputation,\n",
      "usecascades,andpushcomputationtoGPUs.However,\n",
      "\n",
      "only30HzDPM[\n",
      "30]actuallyrunsinreal-time.\n",
      "Insteadoftryingtooptimizeindividualcomponentsof\n",
      "alargedetectionpipeline,YOLOthrowsoutthepipeline\n",
      "\n",
      "entirelyandisfastbydesign.\n",
      "Detectorsforsingleclasseslikefacesorpeoplecanbe\n",
      "highlyoptimizedsincetheyhavetodealwithmuchless\n",
      "\n",
      "variation[\n",
      "36].YOLOisageneralpurposedetectorthat\n",
      "learnstodetectavarietyofobjectssimultaneously.\n",
      "DeepMultiBox.\n",
      "UnlikeR-CNN,Szegedyetal.traina\n",
      "convolutionalneuralnetworktopredictregionsofinterest\n",
      "\n",
      "[8]insteadofusingSelectiveSearch.MultiBoxcanalso\n",
      "performsingleobjectdetectionbyreplacingthe\n",
      "\n",
      "predictionwithasingleclassprediction.However,Multi-\n",
      "\n",
      "Boxcannotperformgeneralobjectdetectionandisstilljust\n",
      "\n",
      "apieceinalargerdetectionpipeline,requiringfurtherim-\n",
      "\n",
      "agepatchBothYOLOandMultiBoxusea\n",
      "\n",
      "convolutionalnetworktopredictboundingboxesinanim-\n",
      "\n",
      "agebutYOLOisacompletedetectionsystem.\n",
      "OverFeat.\n",
      "Sermanetetal.trainaconvolutionalneural\n",
      "networktoperformlocalizationandadaptthatlocalizerto\n",
      "\n",
      "performdetection[\n",
      "31].OverFeatefperformsslid-\n",
      "ingwindowdetectionbutitisstilladisjointsystem.Over-\n",
      "\n",
      "Featoptimizesforlocalization,notdetectionperformance.\n",
      "\n",
      "LikeDPM,thelocalizeronlyseeslocalinformationwhen\n",
      "\n",
      "makingaprediction.OverFeatcannotreasonaboutglobal\n",
      "\n",
      "contextandthusrequirespost-processingtopro-\n",
      "\n",
      "ducecoherentdetections.\n",
      "MultiGrasp.Ourworkissimilarindesigntoworkon\n",
      "graspdetectionbyRedmonetal[\n",
      "26].Ourgridapproachto\n",
      "boundingboxpredictionisbasedontheMultiGraspsystem\n",
      "\n",
      "forregressiontograsps.However,graspdetectionisamuch\n",
      "\n",
      "simplertaskthanobjectdetection.MultiGrasponlyneeds\n",
      "\n",
      "topredictasinglegraspableregionforanimagecontaining\n",
      "\n",
      "oneobject.Itdoesn'thavetoestimatethesize,location,\n",
      "\n",
      "orboundariesoftheobjectorpredictit'sclass,onlya\n",
      "\n",
      "regionsuitableforgrasping.YOLOpredictsbothbounding\n",
      "\n",
      "boxesandclassprobabilitiesformultipleobjectsofmulti-\n",
      "\n",
      "pleclassesinanimage.\n",
      "\n",
      "4.Experiments\n",
      "FirstwecompareYOLOwithotherreal-timedetection\n",
      "systemsonP\n",
      "ASCAL\n",
      "VOC2007.Tounderstandthediffer-\n",
      "encesbetweenYOLOandR-CNNvariantsweexplorethe\n",
      "\n",
      "errorsonVOC2007madebyYOLOandFastR-CNN,one\n",
      "\n",
      "ofthehighestperformingversionsofR-CNN[\n",
      "14].Based\n",
      "onthedifferenterrorweshowthatYOLOcanbe\n",
      "\n",
      "usedtorescoreFastR-CNNdetectionsandreducetheer-\n",
      "\n",
      "rorsfrombackgroundfalsepositives,givinga\n",
      "\n",
      "performanceboost.WealsopresentVOC2012resultsand\n",
      "\n",
      "comparemAPtocurrentstate-of-the-artmethods.Finally,\n",
      "\n",
      "weshowthatYOLOgeneralizestonewdomainsbetterthan\n",
      "\n",
      "otherdetectorsontwoartworkdatasets.\n",
      "\n",
      "4.1.ComparisontoOtherimeSystems\n",
      "Manyresearcheffortsinobjectdetectionfocusonmak-\n",
      "ingstandarddetectionpipelinesfast.[\n",
      "5][\n",
      "37][\n",
      "30][\n",
      "14][\n",
      "17][27]However,onlySadeghietal.actuallyproduceade-\n",
      "tectionsystemthatrunsinreal-time(30framespersecond\n",
      "\n",
      "orbetter)[\n",
      "30].WecompareYOLOtotheirGPUimple-\n",
      "mentationofDPMwhichrunseitherat30Hzor100Hz.\n",
      "\n",
      "Whiletheothereffortsdon'treachthereal-timemilestone\n",
      "\n",
      "wealsocomparetheirrelativemAPandspeedtoexamine\n",
      "\n",
      "theaccuracy-performancetradeoffsavailableinobjectde-\n",
      "\n",
      "tectionsystems.\n",
      "FastYOLOisthefastestobjectdetectionmethodon\n",
      "PASCAL\n",
      ";asfarasweknow,itisthefastestextantobject\n",
      "detector.With\n",
      "52:\n",
      "7%\n",
      "mAP,itismorethantwiceasaccurate\n",
      "aspriorworkonreal-timedetection.YOLOpushesmAPto\n",
      "\n",
      "63:\n",
      "4%\n",
      "whilestillmaintainingreal-timeperformance.\n",
      "WealsotrainYOLOusingVGG-16.Thismodelismore\n",
      "accuratebutalsoslowerthanYOLO.Itisuse-\n",
      "\n",
      "fulforcomparisontootherdetectionsystemsthatrelyon\n",
      "\n",
      "VGG-16butsinceitisslowerthanreal-timetherestofthe\n",
      "\n",
      "paperfocusesonourfastermodels.\n",
      "FastestDPMeffectivelyspeedsupDPMwithoutsacri-\n",
      "muchmAPbutitstillmissesreal-timeperformance\n",
      "\n",
      "byafactorof2[\n",
      "37].ItalsoislimitedbyDPM'srelatively\n",
      "lowaccuracyondetectioncomparedtoneuralnetworkap-\n",
      "\n",
      "proaches.R-CNNminusRreplacesSelectiveSearchwithstatic\n",
      "boundingboxproposals[\n",
      "20].Whileitismuchfasterthan\n",
      "783\n",
      "\n",
      "Real-TimeDetectorsTrainmAPFPS\n",
      "100HzDPM[\n",
      "30]200716.0100\n",
      "30HzDPM[\n",
      "30]200726.130\n",
      "FastYOLO2007+201252.7\n",
      "155YOLO2007+2012\n",
      "63.445LessThanReal-Time\n",
      "FastestDPM[\n",
      "37]200730.415\n",
      "R-CNNMinusR[\n",
      "20]200753.56\n",
      "FastR-CNN[\n",
      "14]2007+201270.00.5\n",
      "FasterR-CNNVGG-16[\n",
      "27]2007+201273.27\n",
      "FasterR-CNNZF[\n",
      "27]2007+201262.118\n",
      "YOLOVGG-162007+201266.421\n",
      "Table1:\n",
      "Real-TimeSystemsonP\n",
      "ASCAL\n",
      "VOC2007.\n",
      "Compar-\n",
      "ingtheperformanceandspeedoffastdetectors.FastYOLOis\n",
      "\n",
      "thefastestdetectoronrecordforP\n",
      "ASCAL\n",
      "VOCdetectionandis\n",
      "stilltwiceasaccurateasanyotherreal-timedetector.YOLOis\n",
      "\n",
      "10mAPmoreaccuratethanthefastversionwhilestillwellabove\n",
      "\n",
      "real-timeinspeed.\n",
      "\n",
      "R-CNN,itstillfallsshortofreal-timeandtakesa\n",
      "\n",
      "accuracyhitfromnothavinggoodproposals.\n",
      "FastR-CNNspeedsupthestageofR-CNN\n",
      "butitstillreliesonselectivesearchwhichcantakearound\n",
      "\n",
      "2secondsperimagetogenerateboundingboxproposals.\n",
      "\n",
      "ThusithashighmAPbutat\n",
      "0:\n",
      "5fpsitisstillfarfromreal-\n",
      "time.TherecentFasterR-CNNreplacesselectivesearchwith\n",
      "aneuralnetworktoproposeboundingboxes,similarto\n",
      "\n",
      "Szegedyetal.[\n",
      "8]Inourtests,theirmostaccuratemodel\n",
      "achieves7fpswhileasmaller,lessaccurateonerunsat\n",
      "\n",
      "18fps.TheVGG-16versionofFasterR-CNNis10mAP\n",
      "\n",
      "higherbutisalso6timesslowerthanYOLO.TheZeiler-\n",
      "\n",
      "FergusFasterR-CNNisonly2.5timesslowerthanYOLO\n",
      "\n",
      "butisalsolessaccurate.\n",
      "\n",
      "4.2.VOC2007ErrorAnalysis\n",
      "TofurtherexaminethedifferencesbetweenYOLOand\n",
      "state-of-the-artdetectors,welookatadetailedbreakdown\n",
      "\n",
      "ofresultsonVOC2007.WecompareYOLOtoFastR-\n",
      "\n",
      "CNNsinceFastR-CNNisoneofthehighestperforming\n",
      "\n",
      "detectorsonP\n",
      "ASCAL\n",
      "andit'sdetectionsarepubliclyavail-\n",
      "able.WeusethemethodologyandtoolsofHoiemetal.[\n",
      "19]ForeachcategoryattesttimewelookatthetopNpredic-\n",
      "\n",
      "tionsforthatcategory.Eachpredictioniseithercorrector\n",
      "\n",
      "itisbasedonthetypeoferror:\n",
      "Correct:correctclassandIOU\n",
      ">:\n",
      "5Localization:correctclass,\n",
      ":\n",
      "1<\n",
      "IOU<:\n",
      "5Similar:classissimilar,IOU\n",
      ">:\n",
      "1Correct: 71.6%\n",
      "Correct: 65.5%\n",
      "Loc: 8.6%Sim: 4.3%Other: 1.9%\n",
      "Background: 13.6%\n",
      "Loc: 19.0%\n",
      "Sim: 6.75%Other: 4.0%\n",
      "Background: 4.75%\n",
      "Fast R-CNN\n",
      "YOLO\n",
      "Figure4:\n",
      "ErrorAnalysis:FastR-CNNvs.YOLO\n",
      "Thesechartsshowthepercentageoflocalizationandbackgrounderrors\n",
      "\n",
      "inthetopNdetectionsforvariouscategories(N=#objectsinthat\n",
      "\n",
      "category).\n",
      "Other:classiswrong,IOU\n",
      ">:\n",
      "1Background:IOU\n",
      "<:\n",
      "1foranyobject\n",
      "Figure4showsthebreakdownofeacherrortypeaver-\n",
      "agedacrossall20classes.\n",
      "YOLOstrugglestolocalizeobjectscorrectly.Localiza-\n",
      "tionerrorsaccountformoreofYOLO'serrorsthanallother\n",
      "\n",
      "sourcescombined.FastR-CNNmakesmuchfewerlocal-\n",
      "\n",
      "izationerrorsbutfarmorebackgrounderrors.13.6%of\n",
      "\n",
      "it'stopdetectionsarefalsepositivesthatdon'tcontainany\n",
      "\n",
      "objects.FastR-CNNisalmost3xmorelikelytopredict\n",
      "\n",
      "backgrounddetectionsthanYOLO.\n",
      "\n",
      "4.3.CombiningFastandYOLO\n",
      "YOLOmakesfarfewerbackgroundmistakesthanFast\n",
      "R-CNN.ByusingYOLOtoeliminatebackgrounddetec-\n",
      "\n",
      "tionsfromFastR-CNNwegetaboostinperfor-\n",
      "\n",
      "mance.ForeveryboundingboxthatR-CNNpredictswe\n",
      "\n",
      "checktoseeifYOLOpredictsasimilarbox.Ifitdoes,we\n",
      "\n",
      "givethatpredictionaboostbasedontheprobabilitypre-\n",
      "\n",
      "dictedbyYOLOandtheoverlapbetweenthetwoboxes.\n",
      "ThebestFastR-CNNmodelachievesamAPof71.8%\n",
      "ontheVOC2007testset.WhencombinedwithYOLO,its\n",
      "mAPCombinedGain\n",
      "FastR-CNN71.8--\n",
      "FastR-CNN(2007data)\n",
      "66.972.4.6\n",
      "FastR-CNN(VGG-M)59.272.4.6\n",
      "\n",
      "FastR-CNN(CaffeNet)57.172.1.3\n",
      "\n",
      "YOLO63.4\n",
      "75.03.2\n",
      "Table2:\n",
      "ModelcombinationexperimentsonVOC2007.\n",
      "We\n",
      "examinetheeffectofcombiningvariousmodelswiththebestver-\n",
      "\n",
      "sionofFastR-CNN.OtherversionsofFastR-CNNprovideonly\n",
      "\n",
      "asmallwhileYOLOprovidesaperformance\n",
      "\n",
      "boost.784\n",
      "\n",
      "VOC2012test\n",
      "mAPaerobikebirdboatbottlebuscarcatchaircowtabledoghorsembikepersonplantsheepsofatraintv\n",
      "MRCNNMOREDATA[\n",
      "11]73.985.582.976.657.862.779.4\n",
      "77.286.6\n",
      "55.079.162.2\n",
      "87.083.484.7\n",
      "78.945.373.465.880.374.0\n",
      "HyperNetVGG\n",
      "71.484.278.573.655.653.778.7\n",
      "79.887.749.674.952.186.081.783.3\n",
      "81.848.673.5\n",
      "59.479.965.7\n",
      "HyperNetSP71.384.178.373.355.553.678.679.687.549.574.952.185.681.683.281.648.473.259.379.765.6\n",
      "FastR-CNN+YOLO\n",
      "70.783.478.573.555.843.479.173.189.449.475.557.087.580.981.074.741.871.568.582.167.2MRCNNSCNN[\n",
      "11]70.785.079.671.555.357.776.073.984.650.574.361.785.579.981.776.441.069.061.277.772.1\n",
      "FasterR-CNN[\n",
      "27]70.484.979.874.353.949.877.575.988.545.677.155.386.981.780.979.640.172.660.981.261.5\n",
      "DEEPENSCOCO70.184.079.471.651.951.174.172.188.648.373.457.886.180.080.770.446.669.6\n",
      "68.875.971.4\n",
      "NoC[\n",
      "28]68.882.879.071.652.353.774.169.084.946.974.353.185.081.379.572.238.972.459.576.768.1\n",
      "FastR-CNN[\n",
      "14]68.482.378.470.852.338.777.871.689.344.273.055.0\n",
      "87.580.580.872.035.168.365.780.464.2\n",
      "UMICHFGSSTRUCT\n",
      "66.482.976.164.144.649.470.371.284.642.768.655.882.777.179.968.741.469.060.072.066.2\n",
      "NUSNINC2000[\n",
      "7]63.880.273.861.943.743.070.367.680.741.969.751.778.275.276.965.138.668.358.068.763.3\n",
      "BabyLearning[\n",
      "7]63.278.074.261.345.742.768.266.880.240.670.049.879.074.577.964.035.367.955.768.762.6\n",
      "NUSNIN62.477.973.162.639.543.369.166.478.939.168.150.077.271.376.164.738.466.956.266.962.7\n",
      "R-CNNVGGBB[\n",
      "13]62.479.672.761.941.241.965.966.484.638.567.246.782.074.876.065.235.665.454.267.460.3\n",
      "R-CNNVGG[\n",
      "13]59.276.870.956.637.536.962.963.681.135.764.343.980.471.674.060.030.863.452.063.558.7\n",
      "YOLO\n",
      "57.977.067.257.738.322.768.355.981.436.260.848.577.272.371.363.528.952.254.873.950.8FeatureEdit[\n",
      "32]56.374.669.154.439.133.165.262.769.730.856.044.670.064.471.160.233.361.346.461.757.8\n",
      "R-CNNBB[\n",
      "13]53.371.865.852.034.132.659.660.069.827.652.041.769.661.368.357.829.657.840.959.354.1\n",
      "SDS[\n",
      "16]50.769.758.448.528.328.861.357.570.824.150.735.964.959.165.857.126.058.838.658.950.7\n",
      "R-CNN[\n",
      "13]49.668.163.846.129.427.956.657.065.926.548.739.566.257.365.453.226.254.538.150.651.6\n",
      "Table3:\n",
      "PASCAL\n",
      "VOC2012Leaderboard.\n",
      "YOLOcomparedwiththefull\n",
      "comp4(outsidedataallowed)publicleaderboardasof\n",
      "November6th,2015.Meanaverageprecisionandper-classaverageprecisionareshownforavarietyofdetectionmethods.YOLOisthe\n",
      "\n",
      "onlyreal-timedetector.FastR-CNN+YOLOistheforthhighestscoringmethod,witha2.3%boostoverFastR-CNN.\n",
      "\n",
      "mAPincreasesby3.2%to75.0%.Wealsotriedcombining\n",
      "\n",
      "thetopFastR-CNNmodelwithseveralotherversionsof\n",
      "\n",
      "FastR-CNN.Thoseensemblesproducedsmallincreasesin\n",
      "\n",
      "mAPbetween.3and.6%,seeTable\n",
      "2fordetails.\n",
      "TheboostfromYOLOisnotsimplyabyproductof\n",
      "modelensemblingsincethereislittlefromcombin-\n",
      "\n",
      "ingdifferentversionsofFastR-CNN.Rather,itisprecisely\n",
      "\n",
      "becauseYOLOmakesdifferentkindsofmistakesattest\n",
      "\n",
      "timethatitissoeffectiveatboostingFastR-CNN'sper-\n",
      "\n",
      "formance.Unfortunately,thiscombinationdoesn'tfromthe\n",
      "speedofYOLOsinceweruneachmodelseperatelyand\n",
      "\n",
      "thencombinetheresults.However,sinceYOLOissofast\n",
      "\n",
      "itdoesn'taddanycomputationaltimecompared\n",
      "\n",
      "toFastR-CNN.\n",
      "\n",
      "4.4.VOC2012Results\n",
      "OntheVOC2012testset,YOLOscores57.9%mAP.\n",
      "Thisislowerthanthecurrentstateoftheart,closerto\n",
      "\n",
      "theoriginalR-CNNusingVGG-16,seeTable\n",
      "3.Oursys-\n",
      "temstruggleswithsmallobjectscomparedtoitsclosest\n",
      "\n",
      "competitors.Oncategorieslike\n",
      "bottle,sheep,and\n",
      "tv/monitorYOLOscores8-10%lowerthanR-CNNor\n",
      "FeatureEdit.However,onothercategorieslike\n",
      "catandtrainYOLOachieveshigherperformance.\n",
      "OurcombinedFastR-CNN+YOLOmodelisoneofthe\n",
      "highestperformingdetectionmethods.FastR-CNNgets\n",
      "\n",
      "a2.3%improvementfromthecombinationwithYOLO,\n",
      "\n",
      "boostingit5spotsuponthepublicleaderboard.\n",
      "\n",
      "4.5.Generalizability:PersonDetectioninArtwork\n",
      "Academicdatasetsforobjectdetectiondrawthetraining\n",
      "andtestingdatafromthesamedistribution.Inreal-world\n",
      "\n",
      "applicationsitishardtopredictallpossibleusecasesand\n",
      "thetestdatacandivergefromwhatthesystemhasseenbe-\n",
      "\n",
      "fore[\n",
      "3].WecompareYOLOtootherdetectionsystemson\n",
      "thePicassoDataset[\n",
      "12]andthePeople-ArtDataset[\n",
      "3],two\n",
      "datasetsfortestingpersondetectiononartwork.\n",
      "Figure5showscomparativeperformancebetween\n",
      "YOLOandotherdetectionmethods.Forreference,wegive\n",
      "\n",
      "VOC2007detectionAPon\n",
      "personwhereallmodelsare\n",
      "trainedonlyonVOC2007data.OnPicassomodelsare\n",
      "\n",
      "trainedonVOC2012whileonPeople-Arttheyaretrained\n",
      "\n",
      "onVOC2010.\n",
      "R-CNNhashighAPonVOC2007.However,R-CNN\n",
      "dropsoffconsiderablywhenappliedtoartwork.R-CNN\n",
      "\n",
      "usesSelectiveSearchforboundingboxproposalswhichis\n",
      "\n",
      "tunedfornaturalimages.ThestepinR-CNNonly\n",
      "\n",
      "seessmallregionsandneedsgoodproposals.\n",
      "DPMmaintainsitsAPwellwhenappliedtoartwork.\n",
      "PriorworktheorizesthatDPMperformswellbecauseithas\n",
      "\n",
      "strongspatialmodelsoftheshapeandlayoutofobjects.\n",
      "\n",
      "ThoughDPMdoesn'tdegradeasmuchasR-CNN,itstarts\n",
      "\n",
      "fromalowerAP.\n",
      "YOLOhasgoodperformanceonVOC2007anditsAP\n",
      "degradeslessthanothermethodswhenappliedtoartwork.\n",
      "\n",
      "LikeDPM,YOLOmodelsthesizeandshapeofobjects,\n",
      "\n",
      "aswellasrelationshipsbetweenobjectsandwhereobjects\n",
      "\n",
      "commonlyappear.Artworkandnaturalimagesarevery\n",
      "\n",
      "differentonapixellevelbuttheyaresimilarintermsof\n",
      "\n",
      "thesizeandshapeofobjects,thusYOLOcanstillpredict\n",
      "\n",
      "goodboundingboxesanddetections.\n",
      "\n",
      "5.Real-TimeDetectionInTheWild\n",
      "YOLOisafast,accurateobjectdetector,makingitideal\n",
      "forcomputervisionapplications.WeconnectYOLOtoa\n",
      "\n",
      "webcamandverifythatitmaintainsreal-timeperformance,\n",
      "785\n",
      "\n",
      "Poselets\n",
      "RCNN\n",
      "D&THumansDPM\n",
      "YOLO(a)PicassoDatasetprecision-recallcurves.\n",
      "VOC2007\n",
      "PicassoPeople-ArtAPAPBest\n",
      "F\n",
      "1\n",
      "APYOLO\n",
      "59.253.30.590\n",
      "45R-CNN54.210.40.226\n",
      "26DPM43.237.80.458\n",
      "32Poselets[\n",
      "2]36.517.80.271\n",
      "D&T[\n",
      "4]-1.90.051\n",
      "(b)QuantitativeresultsontheVOC2007,Picasso,andPeople-ArtDatasets.\n",
      "ThePicassoDatasetevaluatesonbothAPandbest\n",
      "F\n",
      "1\n",
      "score.Figure5:\n",
      "GeneralizationresultsonPicassoandPeople-Artdatasets.\n",
      "Figure6:\n",
      "QualitativeResults.\n",
      "YOLOrunningonsampleartworkandnaturalimagesfromtheinternet.Itismostlyaccuratealthoughit\n",
      "doesthinkonepersonisanairplane.\n",
      "\n",
      "includingthetimetofetchimagesfromthecameraanddis-\n",
      "\n",
      "playthedetections.\n",
      "Theresultingsystemisinteractiveandengaging.While\n",
      "YOLOprocessesimagesindividually,whenattachedtoa\n",
      "\n",
      "webcamitfunctionslikeatrackingsystem,detectingob-\n",
      "\n",
      "jectsastheymovearoundandchangeinappearance.A\n",
      "\n",
      "demoofthesystemandthesourcecodecanbefoundon\n",
      "\n",
      "ourprojectwebsite:\n",
      "http://pjreddie.com/yolo/.6.Conclusion\n",
      "WeintroduceYOLO,amodelforobjectdetec-\n",
      "tion.Ourmodelissimpletoconstructandcanbetrained\n",
      "directlyonfullimages.Unlike-basedapproaches,\n",
      "\n",
      "YOLOistrainedonalossfunctionthatdirectlycorresponds\n",
      "\n",
      "todetectionperformanceandtheentiremodelistrained\n",
      "\n",
      "jointly.\n",
      "FastYOLOisthefastestgeneral-purposeobjectdetec-\n",
      "torintheliteratureandYOLOpushesthestate-of-the-artin\n",
      "\n",
      "real-timeobjectdetection.YOLOalsogeneralizeswellto\n",
      "\n",
      "newdomainsmakingitidealforapplicationsthatrelyon\n",
      "\n",
      "fast,robustobjectdetection.\n",
      "\n",
      "Acknowledgements:\n",
      "Thisworkispartiallysupportedby\n",
      "ONRN00014-13-1-0720,NSFIIS-1338054,andTheAllen\n",
      "\n",
      "DistinguishedInvestigatorAward.\n",
      "786\n",
      "\n",
      "References\n",
      "[1]M.B.BlaschkoandC.H.Lampert.Learningtolocalizeob-\n",
      "jectswithstructuredoutputregression.In\n",
      "ComputerVisionŒ\n",
      "ECCV2008\n",
      ",pages2Œ15.Springer,2008.\n",
      "4[2]L.BourdevandJ.Malik.Poselets:Bodypartdetectors\n",
      "trainedusing3dhumanposeannotations.In\n",
      "InternationalConferenceonComputerVision(ICCV)\n",
      ",2009.\n",
      "8[3]H.Cai,Q.Wu,T.Corradi,andP.Hall.Thecross-\n",
      "depictionproblem:Computervisionalgorithmsforrecog-\n",
      "\n",
      "nisingobjectsinartworkandinphotographs.\n",
      "arXivpreprint\n",
      "arXiv:1505.00110,2015.\n",
      "7[4]N.DalalandB.Triggs.Histogramsoforientedgradientsfor\n",
      "humandetection.In\n",
      "ComputerVisionandPatternRecogni-\n",
      "tion,2005.CVPR2005.IEEEComputerSocietyConference\n",
      "\n",
      "on,volume1,pages886Œ893.IEEE,2005.\n",
      "4,8[5]T.Dean,M.Ruzon,M.Segal,J.Shlens,S.Vijaya-\n",
      "narasimhan,J.Yagnik,etal.Fast,accuratedetectionof\n",
      "\n",
      "100,000objectclassesonasinglemachine.In\n",
      "ComputerVisionandPatternRecognition(CVPR),2013IEEEConfer-\n",
      "\n",
      "enceon\n",
      ",pages1814Œ1821.IEEE,2013.\n",
      "5[6]J.Donahue,Y.Jia,O.Vinyals,J.Hoffman,N.Zhang,\n",
      "E.Tzeng,andT.Darrell.Decaf:Adeepconvolutionalacti-\n",
      "\n",
      "vationfeatureforgenericvisualrecognition.\n",
      "arXivpreprint\n",
      "arXiv:1310.1531,2013.\n",
      "4[7]J.Dong,Q.Chen,S.Yan,andA.Yuille.Towards\n",
      "objectdetectionandsemanticsegmentation.In\n",
      "ComputerVisionŒECCV2014\n",
      ",pages299Œ314.Springer,2014.\n",
      "7[8]D.Erhan,C.Szegedy,A.Toshev,andD.Anguelov.Scalable\n",
      "objectdetectionusingdeepneuralnetworks.In\n",
      "ComputerVisionandPatternRecognition(CVPR),2014IEEEConfer-\n",
      "\n",
      "enceon\n",
      ",pages2155Œ2162.IEEE,2014.\n",
      "5,6[9]M.Everingham,S.M.A.Eslami,L.VanGool,C.K.I.\n",
      "Williams,J.Winn,andA.Zisserman.Thepascalvisualob-\n",
      "\n",
      "jectclasseschallenge:Aretrospective.\n",
      "InternationalJournal\n",
      "ofComputerVision\n",
      ",111(1):98Œ136,Jan.2015.\n",
      "2[10]P.F.Felzenszwalb,R.B.Girshick,D.McAllester,andD.Ra-\n",
      "manan.Objectdetectionwithdiscriminativelytrainedpart\n",
      "\n",
      "basedmodels.\n",
      "IEEETransactionsonPatternAnalysisand\n",
      "MachineIntelligence\n",
      ",32(9):1627Œ1645,2010.\n",
      "1,4[11]S.GidarisandN.Komodakis.Objectdetectionviaamulti-\n",
      "region&semanticsegmentation-awareCNNmodel.\n",
      "CoRR,abs/1505.01749,2015.\n",
      "7[12]S.Ginosar,D.Haas,T.Brown,andJ.Malik.Detectingpeo-\n",
      "pleincubistart.In\n",
      "ComputerVision-ECCV2014Workshops\n",
      ",pages101Œ116.Springer,2014.\n",
      "7[13]R.Girshick,J.Donahue,T.Darrell,andJ.Malik.Richfea-\n",
      "turehierarchiesforaccurateobjectdetectionandsemantic\n",
      "\n",
      "segmentation.In\n",
      "ComputerVisionandPatternRecognition\n",
      "(CVPR),2014IEEEConferenceon\n",
      ",pages580Œ587.IEEE,\n",
      "2014.1,4,7[14]R.B.Girshick.FastR-CNN.\n",
      "CoRR,abs/1504.08083,2015.\n",
      "2,5,6,7[15]S.Gould,T.Gao,andD.Koller.Region-basedsegmenta-\n",
      "tionandobjectdetection.In\n",
      "Advancesinneuralinformation\n",
      "processingsystems\n",
      ",pages655Œ663,2009.\n",
      "4[16]B.Hariharan,P.Arbel\n",
      "´aez,R.Girshick,andJ.Malik.Simul-\n",
      "taneousdetectionandsegmentation.In\n",
      "ComputerVisionŒ\n",
      "ECCV2014\n",
      ",pages297Œ312.Springer,2014.\n",
      "7[17]K.He,X.Zhang,S.Ren,andJ.Sun.Spatialpyramidpooling\n",
      "indeepconvolutionalnetworksforvisualrecognition.\n",
      "arXivpreprintarXiv:1406.4729\n",
      ",2014.\n",
      "5[18]G.E.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,and\n",
      "R.R.Salakhutdinov.Improvingneuralnetworksbypre-\n",
      "\n",
      "ventingco-adaptationoffeaturedetectors.\n",
      "arXivpreprint\n",
      "arXiv:1207.0580,2012.\n",
      "4[19]D.Hoiem,Y.Chodpathumwan,andQ.Dai.Diagnosingerror\n",
      "inobjectdetectors.In\n",
      "ComputerVisionŒECCV2012\n",
      ",pages\n",
      "340Œ353.Springer,2012.\n",
      "6[20]K.LencandA.Vedaldi.R-cnnminusr.\n",
      "arXivpreprint\n",
      "arXiv:1506.06981,2015.\n",
      "5,6[21]R.LienhartandJ.Maydt.Anextendedsetofhaar-likefea-\n",
      "turesforrapidobjectdetection.In\n",
      "ImageProcessing.2002.\n",
      "Proceedings.2002InternationalConferenceon\n",
      ",volume1,\n",
      "pagesIŒ900.IEEE,2002.\n",
      "4[22]M.Lin,Q.Chen,andS.Yan.Networkinnetwork.\n",
      "CoRR,abs/1312.4400,2013.\n",
      "2[23]D.G.Lowe.Objectrecognitionfromlocalscale-invariant\n",
      "features.In\n",
      "Computervision,1999.Theproceedingsofthe\n",
      "seventhIEEEinternationalconferenceon\n",
      ",volume2,pages\n",
      "1150Œ1157.Ieee,1999.\n",
      "4[24]D.Mishkin.Modelsaccuracyonimagenet2012\n",
      "val.\n",
      "https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-2012-val\n",
      ".Ac-\n",
      "cessed:2015-10-2.\n",
      "3[25]C.P.Papageorgiou,M.Oren,andT.Poggio.Ageneral\n",
      "frameworkforobjectdetection.In\n",
      "Computervision,1998.\n",
      "sixthinternationalconferenceon\n",
      ",pages555Œ562.IEEE,\n",
      "1998.4[26]J.RedmonandA.Angelova.Real-timegraspdetectionusing\n",
      "convolutionalneuralnetworks.\n",
      "CoRR,abs/1412.3128,2014.\n",
      "5[27]S.Ren,K.He,R.Girshick,andJ.Sun.Fasterr-cnn:To-\n",
      "wardsreal-timeobjectdetectionwithregionproposalnet-\n",
      "\n",
      "works.\n",
      "arXivpreprintarXiv:1506.01497\n",
      ",2015.\n",
      "5,6,7[28]S.Ren,K.He,R.B.Girshick,X.Zhang,andJ.Sun.Object\n",
      "detectionnetworksonconvolutionalfeaturemaps.\n",
      "CoRR,abs/1504.06066,2015.\n",
      "3,7[29]O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,\n",
      "S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,\n",
      "\n",
      "A.C.Berg,andL.Fei-Fei.ImageNetLargeScaleVisual\n",
      "\n",
      "RecognitionChallenge.\n",
      "InternationalJournalofComputer\n",
      "Vision(IJCV)\n",
      ",2015.\n",
      "3[30]M.A.SadeghiandD.Forsyth.30hzobjectdetectionwith\n",
      "dpmv5.In\n",
      "ComputerVisionŒECCV2014\n",
      ",pages65Œ79.\n",
      "Springer,2014.\n",
      "5,6[31]P.Sermanet,D.Eigen,X.Zhang,M.Mathieu,R.Fergus,\n",
      "andY.LeCun.Overfeat:Integratedrecognition,localiza-\n",
      "\n",
      "tionanddetectionusingconvolutionalnetworks.\n",
      "CoRR,abs/1312.6229,2013.\n",
      "4,5[32]Z.ShenandX.Xue.Domoredropoutsinpool5featuremaps\n",
      "forbetterobjectdetection.\n",
      "arXivpreprintarXiv:1409.6911\n",
      ",2014.7787\n",
      "\n",
      "[33]C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,\n",
      "D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich.\n",
      "\n",
      "Goingdeeperwithconvolutions.\n",
      "CoRR,abs/1409.4842,\n",
      "2014.2[34]J.R.Uijlings,K.E.vandeSande,T.Gevers,andA.W.\n",
      "Smeulders.Selectivesearchforobjectrecognition.\n",
      "Interna-tionaljournalofcomputervision\n",
      ",104(2):154Œ171,2013.\n",
      "4,5[35]P.ViolaandM.Jones.Robustreal-timeobjectdetection.\n",
      "InternationalJournalofComputerVision\n",
      ",4:34Œ47,2001.\n",
      "4[36]P.ViolaandM.J.Jones.Robustreal-timefacedetection.\n",
      "Internationaljournalofcomputervision\n",
      ",57(2):137Œ154,\n",
      "2004.5[37]J.Yan,Z.Lei,L.Wen,andS.Z.Li.Thefastestdeformable\n",
      "partmodelforobjectdetection.In\n",
      "ComputerVisionandPat-\n",
      "ternRecognition(CVPR),2014IEEEConferenceon\n",
      ",pages\n",
      "2497Œ2504.IEEE,2014.\n",
      "5,6[38]C.L.ZitnickandP.Doll\n",
      "´ar.Edgeboxes:Locatingobjectpro-\n",
      "posalsfromedges.In\n",
      "ComputerVisionŒECCV2014\n",
      ",pages\n",
      "391Œ405.Springer,2014.\n",
      "4788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for page in pdf.pages:\n",
    "    print(page.extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "advisory-dominant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PyPDF2.utils.ConvertFunctionsToVirtualList at 0x7efe200d3750>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-poultry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "blond-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "killing-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_writer = PdfFileWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "under-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pdf_writer.addBlankPage(width=72, height=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "surrounded-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PageObject' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-52bf5e98719c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PageObject' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "pdf_writer.write('out.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-touch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "human-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "dir_name = 'dir'\n",
    "sub_dir_name = 'sub_dir_name'\n",
    "file_name = 'file_name'\n",
    "\n",
    "dir_ = Path(dir_name)\n",
    "file = dir_ / sub_dir_name / file_name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "rural-minority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dir/sub_dir_name/file_name'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "friendly-director",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dir',)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_.parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-replica",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "institutional-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "amazing-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDF(FPDF):\n",
    "    def lines(self):\n",
    "        self.set_line_width(0.0)\n",
    "        self.line(5.0,5.0,205.0,5.0) # top one\n",
    "        self.line(5.0,292.0,205.0,292.0) # bottom one\n",
    "        self.line(5.0,5.0,5.0,292.0) # left one\n",
    "        self.line(205.0,5.0,205.0,292.0) # right one\n",
    "    \n",
    "    def titles(self):\n",
    "        self.set_xy(0.0,0.0)\n",
    "        self.set_font('Arial', 'B', 16)\n",
    "        self.set_text_color(220, 50, 50)\n",
    "        self.cell(w=210.0, h=40.0, align='C', txt=\"test\", border=0)\n",
    "    \n",
    "    def texts(self,name):\n",
    "        with open(name,'rb') as xy:\n",
    "            txt=xy.read().decode('latin-1')\n",
    "        self.set_xy(10.0,80.0)    \n",
    "        self.set_text_color(76.0, 32.0, 250.0)\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.multi_cell(0,10,txt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "italic-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "mental-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=PDF(orientation='L') # landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "working-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=PDF(unit='mm') #unit of measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "alleged-definition",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-92-94d09b15d7ef>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-92-94d09b15d7ef>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    measure{'mm','cm','pt','in'},\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pdf=PDF(format='A4')\n",
    "#page format. A4 is the default value of the format, you don't have to specify it.\n",
    "# full syntax\n",
    "PDF(orientation={'P' or 'L'},\n",
    "    measure{'mm','cm','pt','in'},\n",
    "    format{'A4','A3','A5','Letter','Legal'}\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "suspended-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default\n",
    "pdf = PDF(orientation='P', unit='mm', format='A4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "sunrise-scene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.add_page()\n",
    "pdf.output('test.pdf','F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "identified-background",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.add_page()\n",
    "pdf.lines()\n",
    "pdf.output('test.pdf','F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "official-jordan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "pdf = FPDF('P', 'mm', 'A4')\n",
    "pdf.add_page()\n",
    "pdf.set_font('Arial', 'B', 16)\n",
    "pdf.cell(40, 10, 'Hello World!')\n",
    "pdf.output('tuto1.pdf', 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text(pdf, text, font='Arial', bold=):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "stainless-objective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        # Logo\n",
    "#         self.image('logo_pb.png', 10, 8, 33)\n",
    "        # Arial bold 15\n",
    "        self.set_font('Arial', 'B', 15)\n",
    "        # Move to the right\n",
    "        self.cell(80)\n",
    "        # Title\n",
    "        self.cell(30, 10, 'Title', 1, 0, 'C')\n",
    "        # Line break\n",
    "        self.ln(20)\n",
    "\n",
    "    # Page footer\n",
    "    def footer(self):\n",
    "        # Position at 1.5 cm from bottom\n",
    "        self.set_y(-15)\n",
    "        # Arial italic 8\n",
    "        self.set_font('Arial', 'I', 8)\n",
    "        # Page number\n",
    "        self.cell(0, 10, 'Page ' + str(self.page_no()) + '/{nb}', 0, 0, 'C')\n",
    "\n",
    "# Instantiation of inherited class\n",
    "pdf = PDF()\n",
    "pdf.alias_nb_pages()\n",
    "pdf.add_page()\n",
    "pdf.set_font('Times', '', 12)\n",
    "for i in range(1, 41):\n",
    "    pdf.cell(0, 10, 'Printing line number ' + str(i), 0, 1)\n",
    "pdf.output('tuto2.pdf', 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "animated-defendant",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '20k_c1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-5c479a2a03af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_author\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Jules Verne'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_chapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A RUNAWAY REEF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'20k_c1.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_chapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'THE PROS AND CONS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'20k_c2.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tuto3.pdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-5c479a2a03af>\u001b[0m in \u001b[0;36mprint_chapter\u001b[0;34m(self, num, title, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchapter_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchapter_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-5c479a2a03af>\u001b[0m in \u001b[0;36mchapter_body\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchapter_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Read text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Times 12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '20k_c1.txt'"
     ]
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "title = '20000 Leagues Under the Seas'\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        # Arial bold 15\n",
    "        self.set_font('Arial', 'B', 15)\n",
    "        # Calculate width of title and position\n",
    "        w = self.get_string_width(title) + 6\n",
    "        self.set_x((210 - w) / 2)\n",
    "        # Colors of frame, background and text\n",
    "        self.set_draw_color(0, 80, 180)\n",
    "        self.set_fill_color(230, 230, 0)\n",
    "        self.set_text_color(220, 50, 50)\n",
    "        # Thickness of frame (1 mm)\n",
    "        self.set_line_width(1)\n",
    "        # Title\n",
    "        self.cell(w, 9, title, 1, 1, 'C', 1)\n",
    "        # Line break\n",
    "        self.ln(10)\n",
    "\n",
    "    def footer(self):\n",
    "        # Position at 1.5 cm from bottom\n",
    "        self.set_y(-15)\n",
    "        # Arial italic 8\n",
    "        self.set_font('Arial', 'I', 8)\n",
    "        # Text color in gray\n",
    "        self.set_text_color(128)\n",
    "        # Page number\n",
    "        self.cell(0, 10, 'Page ' + str(self.page_no()), 0, 0, 'C')\n",
    "\n",
    "    def chapter_title(self, num, label):\n",
    "        # Arial 12\n",
    "        self.set_font('Arial', '', 12)\n",
    "        # Background color\n",
    "        self.set_fill_color(200, 220, 255)\n",
    "        # Title\n",
    "        self.cell(0, 6, 'Chapter %d : %s' % (num, label), 0, 1, 'L', 1)\n",
    "        # Line break\n",
    "        self.ln(4)\n",
    "\n",
    "    def chapter_body(self, name):\n",
    "        # Read text file\n",
    "        with open(name, 'rb') as fh:\n",
    "            txt = fh.read().decode('latin-1')\n",
    "        # Times 12\n",
    "        self.set_font('Times', '', 12)\n",
    "        # Output justified text\n",
    "        self.multi_cell(0, 5, txt)\n",
    "        # Line break\n",
    "        self.ln()\n",
    "        # Mention in italics\n",
    "        self.set_font('', 'I')\n",
    "        self.cell(0, 5, '(end of excerpt)')\n",
    "\n",
    "    def print_chapter(self, num, title, name):\n",
    "        self.add_page()\n",
    "        self.chapter_title(num, title)\n",
    "        self.chapter_body(name)\n",
    "\n",
    "pdf = PDF()\n",
    "pdf.set_title(title)\n",
    "pdf.set_author('Jules Verne')\n",
    "pdf.print_chapter(1, 'A RUNAWAY REEF', '20k_c1.txt')\n",
    "pdf.print_chapter(2, 'THE PROS AND CONS', '20k_c2.txt')\n",
    "pdf.output('tuto3.pdf', 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-title",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "smoking-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "advised-ocean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "TingChen\n",
      "1\n",
      "SimonKornblith\n",
      "1\n",
      "MohammadNorouzi\n",
      "1\n",
      "GeoffreyHinton\n",
      "1\n",
      "Abstract\n",
      "Thispaperpresents\n",
      "SimCLR\n",
      ":asimpleframework\n",
      "forcontrastivelearningofvisualrepresentations.\n",
      "Wesimplifyrecentlyproposedcontrastiveself-\n",
      "supervisedlearningalgorithmswithoutrequiring\n",
      "specializedarchitecturesoramemorybank.In\n",
      "ordertounderstandwhatenablesthecontrastive\n",
      "predictiontaskstolearnusefulrepresentations,\n",
      "wesystematicallystudythemajorcomponentsof\n",
      "ourframework.Weshowthat(1)compositionof\n",
      "dataaugmentationsplaysacriticalroleinng\n",
      "effectivepredictivetasks,(2)introducingalearn-\n",
      "ablenonlineartransformationbetweentherepre-\n",
      "sentationandthecontrastivelosssubstantiallyim-\n",
      "provesthequalityofthelearnedrepresentations,\n",
      "and(3)contrastivelearningfromlarger\n",
      "batchsizesandmoretrainingstepscomparedto\n",
      "supervisedlearning.Bycombiningthese\n",
      "weareabletoconsiderablyoutperformprevious\n",
      "methodsforself-supervisedandsemi-supervised\n",
      "learningonImageNet.Alineartrained\n",
      "onself-supervisedrepresentationslearnedbySim-\n",
      "CLRachieves76.5%top-1accuracy,whichisa\n",
      "7%relativeimprovementoverpreviousstate-of-\n",
      "the-art,matchingtheperformanceofasupervised\n",
      "ResNet-50.Whenononly1%ofthe\n",
      "labels,weachieve85.8%top-5accuracy,outper-\n",
      "formingAlexNetwith100\n",
      "\n",
      "fewerlabels.\n",
      "1\n",
      "1.Introduction\n",
      "Learningeffectivevisualrepresentationswithouthuman\n",
      "supervisionisalong-standingproblem.Mostmainstream\n",
      "approachesfallintooneoftwoclasses:generativeordis-\n",
      "criminative.Generativeapproacheslearntogenerateor\n",
      "otherwisemodelpixelsintheinputspace(\n",
      "Hintonetal.\n",
      ",\n",
      "2006\n",
      ";\n",
      "Kingma&Welling\n",
      ",\n",
      "2013\n",
      ";\n",
      "Goodfellowetal.\n",
      ",\n",
      "2014\n",
      ").\n",
      "1\n",
      "GoogleResearch,BrainTeam.Correspondenceto:TingChen\n",
      "<iamtingchen@google.com>.\n",
      "Proceedingsofthe\n",
      "37\n",
      "th\n",
      "InternationalConferenceonMachine\n",
      "Learning\n",
      ",Vienna,Austria,PMLR119,2020.Copyright2020by\n",
      "theauthor(s).\n",
      "1\n",
      "Codeavailableat\n",
      "https://github.com/google-research/simclr\n",
      ".\n",
      "Figure1.\n",
      "ImageNetTop-1accuracyoflineartrained\n",
      "onrepresentationslearnedwithdifferentself-supervisedmeth-\n",
      "ods(pretrainedonImageNet).Graycrossindicatessupervised\n",
      "ResNet-50.Ourmethod,SimCLR,isshowninbold.\n",
      "However,pixel-levelgenerationiscomputationallyexpen-\n",
      "siveandmaynotbenecessaryforrepresentationlearning.\n",
      "Discriminativeapproacheslearnrepresentationsusingobjec-\n",
      "tivefunctionssimilartothoseusedforsupervisedlearning,\n",
      "buttrainnetworkstoperformpretexttaskswhereboththein-\n",
      "putsandlabelsarederivedfromanunlabeleddataset.Many\n",
      "suchapproacheshavereliedonheuristicstodesignpretext\n",
      "tasks(\n",
      "Doerschetal.\n",
      ",\n",
      "2015\n",
      ";\n",
      "Zhangetal.\n",
      ",\n",
      "2016\n",
      ";\n",
      "Noroozi&\n",
      "Favaro\n",
      ",\n",
      "2016\n",
      ";\n",
      "Gidarisetal.\n",
      ",\n",
      "2018\n",
      "),whichcouldlimitthe\n",
      "generalityofthelearnedrepresentations.Discriminative\n",
      "approachesbasedoncontrastivelearninginthelatentspace\n",
      "haverecentlyshowngreatpromise,achievingstate-of-the-\n",
      "artresults(\n",
      "Hadselletal.\n",
      ",\n",
      "2006\n",
      ";\n",
      "Dosovitskiyetal.\n",
      ",\n",
      "2014\n",
      ";\n",
      "Oordetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Bachmanetal.\n",
      ",\n",
      "2019\n",
      ").\n",
      "Inthiswork,weintroduceasimpleframeworkforcon-\n",
      "trastivelearningofvisualrepresentations,whichwecall\n",
      "SimCLR\n",
      ".NotonlydoesSimCLRoutperformpreviouswork\n",
      "(Figure\n",
      "1\n",
      "),butitisalsosimpler,requiringneitherspecial-\n",
      "izedarchitectures(\n",
      "Bachmanetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Hénaffetal.\n",
      ",\n",
      "2019\n",
      ")\n",
      "noramemorybank(\n",
      "Wuetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Tianetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "He\n",
      "etal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Misra&vanderMaaten\n",
      ",\n",
      "2019\n",
      ").\n",
      "Inordertounderstandwhatenablesgoodcontrastiverepre-\n",
      "sentationlearning,wesystematicallystudythemajorcom-\n",
      "ponentsofourframeworkandshowthat:\n",
      "arXiv:2002.05709v3  [cs.LG]  1 Jul 2020\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "\n",
      "Compositionofmultipledataaugmentationoperations\n",
      "iscrucialinthecontrastivepredictiontasksthat\n",
      "yieldeffectiverepresentations.Inaddition,unsupervised\n",
      "contrastivelearningfromstrongerdataaugmen-\n",
      "tationthansupervisedlearning.\n",
      "\n",
      "Introducingalearnablenonlineartransformationbe-\n",
      "tweentherepresentationandthecontrastivelosssubstan-\n",
      "tiallyimprovesthequalityofthelearnedrepresentations.\n",
      "\n",
      "Representationlearningwithcontrastivecrossentropy\n",
      "lossfromnormalizedembeddingsandanappro-\n",
      "priatelyadjustedtemperatureparameter.\n",
      "\n",
      "Contrastivelearningfromlargerbatchsizesand\n",
      "longertrainingcomparedtoitssupervisedcounterpart.\n",
      "Likesupervisedlearning,contrastivelearning\n",
      "fromdeeperandwidernetworks.\n",
      "Wecombinethesetoachieveanewstate-of-the-art\n",
      "inself-supervisedandsemi-supervisedlearningonIma-\n",
      "geNetILSVRC-2012(\n",
      "Russakovskyetal.\n",
      ",\n",
      "2015\n",
      ").Underthe\n",
      "linearevaluationprotocol,SimCLRachieves76.5%top-1\n",
      "accuracy,whichisa7%relativeimprovementoverprevious\n",
      "state-of-the-art(\n",
      "Hénaffetal.\n",
      ",\n",
      "2019\n",
      ").Whenwith\n",
      "only1%oftheImageNetlabels,SimCLRachieves85.8%\n",
      "top-5accuracy,arelativeimprovementof10%(\n",
      "Hénaffetal.\n",
      ",\n",
      "2019\n",
      ").Whenonothernaturalimage\n",
      "tiondatasets,SimCLRperformsonparwithorbetterthan\n",
      "astrongsupervisedbaseline(\n",
      "Kornblithetal.\n",
      ",\n",
      "2019\n",
      ")on10\n",
      "outof12datasets.\n",
      "2.Method\n",
      "2.1.TheContrastiveLearningFramework\n",
      "Inspiredbyrecentcontrastivelearningalgorithms(seeSec-\n",
      "tion\n",
      "7\n",
      "foranoverview),SimCLRlearnsrepresentations\n",
      "bymaximizingagreementbetweendifferentlyaugmented\n",
      "viewsofthesamedataexampleviaacontrastivelossin\n",
      "thelatentspace.AsillustratedinFigure\n",
      "2\n",
      ",thisframework\n",
      "comprisesthefollowingfourmajorcomponents.\n",
      "\n",
      "Astochastic\n",
      "dataaugmentation\n",
      "modulethattransforms\n",
      "anygivendataexamplerandomlyresultingintwocor-\n",
      "relatedviewsofthesameexample,denoted\n",
      "~\n",
      "x\n",
      "i\n",
      "and\n",
      "~\n",
      "x\n",
      "j\n",
      ",\n",
      "whichweconsiderasapositivepair.Inthiswork,we\n",
      "sequentiallyapplythreesimpleaugmentations:\n",
      "random\n",
      "cropping\n",
      "followedbyresizebacktotheoriginalsize,\n",
      "ran-\n",
      "domcolordistortions\n",
      ",and\n",
      "randomGaussianblur\n",
      ".As\n",
      "showninSection\n",
      "3\n",
      ",thecombinationofrandomcropand\n",
      "colordistortioniscrucialtoachieveagoodperformance.\n",
      "\n",
      "Aneuralnetwork\n",
      "baseencoder\n",
      "f\n",
      "(\n",
      "\n",
      ")\n",
      "thatextractsrepre-\n",
      "sentationvectorsfromaugmenteddataexamples.Our\n",
      "frameworkallowsvariouschoicesofthenetworkarchi-\n",
      "tecturewithoutanyconstraints.Weoptforsimplicity\n",
      "andadoptthecommonlyusedResNet(\n",
      "Heetal.\n",
      ",\n",
      "2016\n",
      ")\n",
      " \n",
      "Representation\n",
      "!\n",
      "x\n",
      "~\n",
      "x\n",
      "i\n",
      "~\n",
      "x\n",
      "j\n",
      "h\n",
      "i\n",
      "h\n",
      "j\n",
      "z\n",
      "i\n",
      "z\n",
      "j\n",
      "t\n",
      "˘T\n",
      "t\n",
      "0\n",
      "˘T\n",
      "f\n",
      "(\n",
      "\n",
      ")\n",
      "f\n",
      "(\n",
      "\n",
      ")\n",
      "g\n",
      "(\n",
      "\n",
      ")\n",
      "g\n",
      "(\n",
      "\n",
      ")\n",
      "Maximizeagreement\n",
      "Figure2.\n",
      "Asimpleframeworkforcontrastivelearningofvisual\n",
      "representations.Twoseparatedataaugmentationoperatorsare\n",
      "sampledfromthesamefamilyofaugmentations(\n",
      "t\n",
      "˘T\n",
      "and\n",
      "t\n",
      "0\n",
      "˘T\n",
      ")andappliedtoeachdataexampletoobtaintwocorrelated\n",
      "views.Abaseencodernetwork\n",
      "f\n",
      "(\n",
      "\n",
      ")\n",
      "andaprojectionhead\n",
      "g\n",
      "(\n",
      "\n",
      ")\n",
      "aretrainedtomaximizeagreementusingacontrastiveloss.After\n",
      "trainingiscompleted,wethrowawaytheprojectionhead\n",
      "g\n",
      "(\n",
      "\n",
      ")\n",
      "and\n",
      "useencoder\n",
      "f\n",
      "(\n",
      "\n",
      ")\n",
      "andrepresentation\n",
      "h\n",
      "fordownstreamtasks.\n",
      "toobtain\n",
      "h\n",
      "i\n",
      "=\n",
      "f\n",
      "(\n",
      "~\n",
      "x\n",
      "i\n",
      ")=ResNet(\n",
      "~\n",
      "x\n",
      "i\n",
      ")\n",
      "where\n",
      "h\n",
      "i\n",
      "2\n",
      "R\n",
      "d\n",
      "is\n",
      "theoutputaftertheaveragepoolinglayer.\n",
      "\n",
      "Asmallneuralnetwork\n",
      "projectionhead\n",
      "g\n",
      "(\n",
      "\n",
      ")\n",
      "thatmaps\n",
      "representationstothespacewherecontrastivelossis\n",
      "applied.WeuseaMLPwithonehiddenlayertoobtain\n",
      "z\n",
      "i\n",
      "=\n",
      "g\n",
      "(\n",
      "h\n",
      "i\n",
      ")=\n",
      "W\n",
      "(2)\n",
      "˙\n",
      "(\n",
      "W\n",
      "(1)\n",
      "h\n",
      "i\n",
      ")\n",
      "where\n",
      "˙\n",
      "isaReLUnon-\n",
      "linearity.Asshowninsection\n",
      "4\n",
      ",weitto\n",
      "thecontrastivelosson\n",
      "z\n",
      "i\n",
      "'sratherthan\n",
      "h\n",
      "i\n",
      "'s.\n",
      "\n",
      "A\n",
      "contrastivelossfunction\n",
      "foracontrastivepre-\n",
      "dictiontask.Givenaset\n",
      "f\n",
      "~\n",
      "x\n",
      "k\n",
      "g\n",
      "includingapositivepair\n",
      "ofexamples\n",
      "~\n",
      "x\n",
      "i\n",
      "and\n",
      "~\n",
      "x\n",
      "j\n",
      ",the\n",
      "contrastivepredictiontask\n",
      "aimstoidentify\n",
      "~\n",
      "x\n",
      "j\n",
      "in\n",
      "f\n",
      "~\n",
      "x\n",
      "k\n",
      "g\n",
      "k\n",
      "6\n",
      "=\n",
      "i\n",
      "foragiven\n",
      "~\n",
      "x\n",
      "i\n",
      ".\n",
      "Werandomlysampleaminibatchof\n",
      "N\n",
      "examplesand\n",
      "thecontrastivepredictiontaskonpairsofaugmentedexam-\n",
      "plesderivedfromtheminibatch,resultingin\n",
      "2\n",
      "N\n",
      "datapoints.\n",
      "Wedonotsamplenegativeexamplesexplicitly.Instead,\n",
      "givenapositivepair,similarto(\n",
      "Chenetal.\n",
      ",\n",
      "2017\n",
      "),wetreat\n",
      "theother\n",
      "2(\n",
      "N\n",
      "\n",
      "1)\n",
      "augmentedexampleswithinaminibatch\n",
      "asnegativeexamples.Let\n",
      "sim(\n",
      "u\n",
      ";\n",
      "v\n",
      ")=\n",
      "u\n",
      ">\n",
      "v\n",
      "=\n",
      "k\n",
      "u\n",
      "kk\n",
      "v\n",
      "k\n",
      "de-\n",
      "notethedotproductbetween\n",
      "`\n",
      "2\n",
      "normalized\n",
      "u\n",
      "and\n",
      "v\n",
      "(i.e.\n",
      "cosinesimilarity).Thenthelossfunctionforapositivepair\n",
      "ofexamples\n",
      "(\n",
      "i;j\n",
      ")\n",
      "isas\n",
      "`\n",
      "i;j\n",
      "=\n",
      "\n",
      "log\n",
      "exp(sim(\n",
      "z\n",
      "i\n",
      ";\n",
      "z\n",
      "j\n",
      ")\n",
      "=˝\n",
      ")\n",
      "P\n",
      "2\n",
      "N\n",
      "k\n",
      "=1\n",
      "1\n",
      "[\n",
      "k\n",
      "6\n",
      "=\n",
      "i\n",
      "]\n",
      "exp(sim(\n",
      "z\n",
      "i\n",
      ";\n",
      "z\n",
      "k\n",
      ")\n",
      "=˝\n",
      ")\n",
      ";\n",
      "(1)\n",
      "where\n",
      "1\n",
      "[\n",
      "k\n",
      "6\n",
      "=\n",
      "i\n",
      "]\n",
      "2f\n",
      "0\n",
      ";\n",
      "1\n",
      "g\n",
      "isanindicatorfunctionevaluatingto\n",
      "1\n",
      "iff\n",
      "k\n",
      "6\n",
      "=\n",
      "i\n",
      "and\n",
      "˝\n",
      "denotesatemperatureparameter.The\n",
      "nallossiscomputedacrossallpositivepairs,both\n",
      "(\n",
      "i;j\n",
      ")\n",
      "and\n",
      "(\n",
      "j;i\n",
      ")\n",
      ",inamini-batch.Thislosshasbeenusedin\n",
      "previouswork(\n",
      "Sohn\n",
      ",\n",
      "2016\n",
      ";\n",
      "Wuetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Oordetal.\n",
      ",\n",
      "2018\n",
      ");forconvenience,wetermit\n",
      "NT-Xent\n",
      "(thenormalized\n",
      "temperature-scaledcrossentropyloss).\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "Algorithm1\n",
      "SimCLR'smainlearningalgorithm.\n",
      "input:\n",
      "batchsize\n",
      "N\n",
      ",constant\n",
      "˝\n",
      ",structureof\n",
      "f\n",
      ",\n",
      "g\n",
      ",\n",
      "T\n",
      ".\n",
      "for\n",
      "sampledminibatch\n",
      "f\n",
      "x\n",
      "k\n",
      "g\n",
      "N\n",
      "k\n",
      "=1\n",
      "do\n",
      "forall\n",
      "k\n",
      "2f\n",
      "1\n",
      ";:::;N\n",
      "g\n",
      "do\n",
      "drawtwoaugmentationfunctions\n",
      "t\n",
      "˘T\n",
      ",\n",
      "t\n",
      "0\n",
      "˘T\n",
      "#theaugmentation\n",
      "~\n",
      "x\n",
      "2\n",
      "k\n",
      "\n",
      "1\n",
      "=\n",
      "t\n",
      "(\n",
      "x\n",
      "k\n",
      ")\n",
      "h\n",
      "2\n",
      "k\n",
      "\n",
      "1\n",
      "=\n",
      "f\n",
      "(\n",
      "~\n",
      "x\n",
      "2\n",
      "k\n",
      "\n",
      "1\n",
      ")\n",
      "#representation\n",
      "z\n",
      "2\n",
      "k\n",
      "\n",
      "1\n",
      "=\n",
      "g\n",
      "(\n",
      "h\n",
      "2\n",
      "k\n",
      "\n",
      "1\n",
      ")\n",
      "#projection\n",
      "#thesecondaugmentation\n",
      "~\n",
      "x\n",
      "2\n",
      "k\n",
      "=\n",
      "t\n",
      "0\n",
      "(\n",
      "x\n",
      "k\n",
      ")\n",
      "h\n",
      "2\n",
      "k\n",
      "=\n",
      "f\n",
      "(\n",
      "~\n",
      "x\n",
      "2\n",
      "k\n",
      ")\n",
      "#representation\n",
      "z\n",
      "2\n",
      "k\n",
      "=\n",
      "g\n",
      "(\n",
      "h\n",
      "2\n",
      "k\n",
      ")\n",
      "#projection\n",
      "endfor\n",
      "forall\n",
      "i\n",
      "2f\n",
      "1\n",
      ";:::;\n",
      "2\n",
      "N\n",
      "g\n",
      "and\n",
      "j\n",
      "2f\n",
      "1\n",
      ";:::;\n",
      "2\n",
      "N\n",
      "g\n",
      "do\n",
      "s\n",
      "i;j\n",
      "=\n",
      "z\n",
      ">\n",
      "i\n",
      "z\n",
      "j\n",
      "=\n",
      "(\n",
      "k\n",
      "z\n",
      "i\n",
      "kk\n",
      "z\n",
      "j\n",
      "k\n",
      ")\n",
      "#pairwisesimilarity\n",
      "endfor\n",
      "\n",
      "`\n",
      "(\n",
      "i;j\n",
      ")\n",
      "as\n",
      "`\n",
      "(\n",
      "i;j\n",
      ")=\n",
      "\n",
      "log\n",
      "exp(\n",
      "s\n",
      "i;j\n",
      "=˝\n",
      ")\n",
      "P\n",
      "2\n",
      "N\n",
      "k\n",
      "=1\n",
      "1\n",
      "[\n",
      "k\n",
      "6\n",
      "=\n",
      "i\n",
      "]\n",
      "exp(\n",
      "s\n",
      "i;k\n",
      "=˝\n",
      ")\n",
      "L\n",
      "=\n",
      "1\n",
      "2\n",
      "N\n",
      "P\n",
      "N\n",
      "k\n",
      "=1\n",
      "[\n",
      "`\n",
      "(2\n",
      "k\n",
      "\n",
      "1\n",
      ";\n",
      "2\n",
      "k\n",
      ")+\n",
      "`\n",
      "(2\n",
      "k;\n",
      "2\n",
      "k\n",
      "\n",
      "1)]\n",
      "updatenetworks\n",
      "f\n",
      "and\n",
      "g\n",
      "tominimize\n",
      "L\n",
      "endfor\n",
      "return\n",
      "encodernetwork\n",
      "f\n",
      "(\n",
      "\n",
      ")\n",
      ",andthrowaway\n",
      "g\n",
      "(\n",
      "\n",
      ")\n",
      "Algorithm\n",
      "1\n",
      "summarizestheproposedmethod.\n",
      "2.2.TrainingwithLargeBatchSize\n",
      "Tokeepitsimple,wedonottrainthemodelwithamemory\n",
      "bank(\n",
      "Wuetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Heetal.\n",
      ",\n",
      "2019\n",
      ").Instead,wevary\n",
      "thetrainingbatchsize\n",
      "N\n",
      "from256to8192.Abatchsize\n",
      "of8192givesus16382negativeexamplesperpositivepair\n",
      "frombothaugmentationviews.Trainingwithlargebatch\n",
      "sizemaybeunstablewhenusingstandardSGD/Momentum\n",
      "withlinearlearningratescaling(\n",
      "Goyaletal.\n",
      ",\n",
      "2017\n",
      ").To\n",
      "stabilizethetraining,weusetheLARSoptimizer(\n",
      "Youetal.\n",
      ",\n",
      "2017\n",
      ")forallbatchsizes.WetrainourmodelwithCloud\n",
      "TPUs,using32to128coresdependingonthebatchsize.\n",
      "2\n",
      "GlobalBN.\n",
      "StandardResNetsusebatchnormaliza-\n",
      "tion(\n",
      "Ioffe&Szegedy\n",
      ",\n",
      "2015\n",
      ").Indistributedtrainingwith\n",
      "dataparallelism,theBNmeanandvariancearetypically\n",
      "aggregatedlocallyperdevice.Inourcontrastivelearning,\n",
      "aspositivepairsarecomputedinthesamedevice,themodel\n",
      "canexploitthelocalinformationleakagetoimprovepre-\n",
      "dictionaccuracywithoutimprovingrepresentations.Wead-\n",
      "dressthisissuebyaggregatingBNmeanandvarianceover\n",
      "alldevicesduringthetraining.Otherapproachesinclude\n",
      "shufdataexamplesacrossdevices(\n",
      "Heetal.\n",
      ",\n",
      "2019\n",
      "),or\n",
      "replacingBNwithlayernorm(\n",
      "Hénaffetal.\n",
      ",\n",
      "2019\n",
      ").\n",
      "2\n",
      "With128TPUv3cores,ittakes\n",
      "˘\n",
      "1.5hourstotrainour\n",
      "ResNet-50withabatchsizeof4096for100epochs.\n",
      "A\n",
      "B\n",
      "(a)Globalandlocalviews.\n",
      "C\n",
      "D\n",
      "(b)Adjacentviews.\n",
      "Figure3.\n",
      "Solidrectanglesareimages,dashedrectanglesareran-\n",
      "domcrops.Byrandomlycroppingimages,wesamplecontrastive\n",
      "predictiontasksthatincludeglobaltolocalview(\n",
      "B\n",
      "!\n",
      "A\n",
      ")or\n",
      "adjacentview(\n",
      "D\n",
      "!\n",
      "C\n",
      ")prediction.\n",
      "2.3.EvaluationProtocol\n",
      "Herewelayouttheprotocolforourempiricalstudies,which\n",
      "aimtounderstanddifferentdesignchoicesinourframework.\n",
      "DatasetandMetrics.\n",
      "Mostofourstudyforunsupervised\n",
      "pretraining(learningencodernetwork\n",
      "f\n",
      "withoutlabels)\n",
      "isdoneusingtheImageNetILSVRC-2012dataset(\n",
      "Rus-\n",
      "sakovskyetal.\n",
      ",\n",
      "2015\n",
      ").Someadditionalpretrainingexperi-\n",
      "mentsonCIFAR-10(\n",
      "Krizhevsky&Hinton\n",
      ",\n",
      "2009\n",
      ")canbe\n",
      "foundinAppendix\n",
      "B.9\n",
      ".Wealsotestthepretrainedresults\n",
      "onawiderangeofdatasetsfortransferlearning.Toevalu-\n",
      "atethelearnedrepresentations,wefollowthewidelyused\n",
      "linearevaluationprotocol(\n",
      "Zhangetal.\n",
      ",\n",
      "2016\n",
      ";\n",
      "Oordetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Bachmanetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Kolesnikovetal.\n",
      ",\n",
      "2019\n",
      "),where\n",
      "alinearistrainedontopofthefrozenbasenet-\n",
      "work,andtestaccuracyisusedasaproxyforrepresentation\n",
      "quality.Beyondlinearevaluation,wealsocompareagainst\n",
      "state-of-the-artonsemi-supervisedandtransferlearning.\n",
      "Defaultsetting.\n",
      "Unlessotherwisefordataaug-\n",
      "mentationweuserandomcropandresize(withrandom\n",
      "colordistortions,andGaussianblur(fordetails,see\n",
      "Appendix\n",
      "A\n",
      ").WeuseResNet-50asthebaseencodernet-\n",
      "work,anda2-layerMLPprojectionheadtoprojectthe\n",
      "representationtoa128-dimensionallatentspace.Asthe\n",
      "loss,weuseNT-Xent,optimizedusingLARSwithlearning\n",
      "rateof4.8(\n",
      "=0\n",
      ":\n",
      "3\n",
      "\n",
      "BatchSize\n",
      "=\n",
      "256\n",
      ")andweightdecayof\n",
      "10\n",
      "\n",
      "6\n",
      ".Wetrainatbatchsize4096for100epochs.\n",
      "3\n",
      "Fur-\n",
      "thermore,weuselinearwarmupforthe10epochs,\n",
      "anddecaythelearningratewiththecosinedecayschedule\n",
      "withoutrestarts(\n",
      "Loshchilov&Hutter\n",
      ",\n",
      "2016\n",
      ").\n",
      "3.DataAugmentationforContrastive\n",
      "RepresentationLearning\n",
      "Dataaugmentationpredictivetasks.\n",
      "Whiledata\n",
      "augmentationhasbeenwidelyusedinbothsupervisedand\n",
      "unsupervisedrepresentationlearning(\n",
      "Krizhevskyetal.\n",
      ",\n",
      "3\n",
      "Althoughmaxperformanceisnotreachedin100epochs,rea-\n",
      "sonableresultsareachieved,allowingfairandefablations.\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "(a)Original\n",
      "(b)Cropandresize\n",
      "(c)Crop,resize(and\n",
      "(d)Colordistort.(drop)\n",
      "(e)Colordistort.(jitter)\n",
      "(f)Rotate\n",
      "f\n",
      "90\n",
      "\n",
      ";\n",
      "180\n",
      "\n",
      ";\n",
      "270\n",
      "\n",
      "g\n",
      "(g)Cutout\n",
      "(h)Gaussiannoise\n",
      "(i)Gaussianblur\n",
      "(j)Sobel\n",
      "Figure4.\n",
      "Illustrationsofthestudieddataaugmentationoperators.Eachaugmentationcantransformdatastochasticallywithsomeinternal\n",
      "parameters(e.g.rotationdegree,noiselevel).Notethatwe\n",
      "only\n",
      "testtheseoperatorsinablation,the\n",
      "augmentationpolicyusedtotrainour\n",
      "models\n",
      "onlyincludes\n",
      "randomcrop(withandresize)\n",
      ",\n",
      "colordistortion\n",
      ",and\n",
      "Gaussianblur\n",
      ".(Originalimagecc-by:Von.grzanka)\n",
      "2012\n",
      ";\n",
      "Hénaffetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Bachmanetal.\n",
      ",\n",
      "2019\n",
      "),ithas\n",
      "notbeenconsideredasasystematicwaytothecon-\n",
      "trastivepredictiontask.Manyexistingapproaches\n",
      "contrastivepredictiontasksbychangingthearchitecture.\n",
      "Forexample,\n",
      "Hjelmetal.\n",
      "(\n",
      "2018\n",
      ");\n",
      "Bachmanetal.\n",
      "(\n",
      "2019\n",
      ")\n",
      "achieveglobal-to-localviewpredictionviaconstrainingthe\n",
      "receptiveinthenetworkarchitecture,whereas\n",
      "Oord\n",
      "etal.\n",
      "(\n",
      "2018\n",
      ");\n",
      "Hénaffetal.\n",
      "(\n",
      "2019\n",
      ")achieveneighboringview\n",
      "predictionviaaedimagesplittingprocedureandacon-\n",
      "textaggregationnetwork.Weshowthatthiscomplexitycan\n",
      "beavoidedbyperformingsimple\n",
      "randomcropping\n",
      "(with\n",
      "resizing)oftargetimages,whichcreatesafamilyofpredic-\n",
      "tivetaskssubsumingtheabovementionedtwo,asshownin\n",
      "Figure\n",
      "3\n",
      ".Thissimpledesignchoiceconvenientlydecouples\n",
      "thepredictivetaskfromothercomponentssuchastheneural\n",
      "networkarchitecture.Broadercontrastivepredictiontasks\n",
      "canbebyextendingthefamilyofaugmentations\n",
      "andcomposingthemstochastically.\n",
      "3.1.Compositionofdataaugmentationoperationsis\n",
      "crucialforlearninggoodrepresentations\n",
      "Tosystematicallystudytheimpactofdataaugmentation,\n",
      "weconsiderseveralcommonaugmentationshere.Onetype\n",
      "ofaugmentationinvolvesspatial/geometrictransformation\n",
      "ofdata,suchascroppingandresizing(withhorizontal\n",
      "rotation(\n",
      "Gidarisetal.\n",
      ",\n",
      "2018\n",
      ")andcutout(\n",
      "De-\n",
      "Vries&Taylor\n",
      ",\n",
      "2017\n",
      ").Theothertypeofaugmentation\n",
      "involvesappearancetransformation,suchascolordistortion\n",
      "(includingcolordropping,brightness,contrast,saturation,\n",
      "hue)(\n",
      "Howard\n",
      ",\n",
      "2013\n",
      ";\n",
      "Szegedyetal.\n",
      ",\n",
      "2015\n",
      "),Gaussianblur,\n",
      "andSobelFigure\n",
      "4\n",
      "visualizestheaugmentations\n",
      "thatwestudyinthiswork.\n",
      "Figure5.\n",
      "Linearevaluation(ImageNettop-1accuracy)underin-\n",
      "dividualorcompositionofdataaugmentations,appliedonlyto\n",
      "onebranch.Forallcolumnsbutthelast,diagonalentriescorre-\n",
      "spondtosingletransformation,andoff-diagonalscorrespondto\n",
      "compositionoftwotransformations(appliedsequentially).The\n",
      "lastcolumntheaverageovertherow.\n",
      "Tounderstandtheeffectsofindividualdataaugmentations\n",
      "andtheimportanceofaugmentationcomposition,wein-\n",
      "vestigatetheperformanceofourframeworkwhenapplying\n",
      "augmentationsindividuallyorinpairs.SinceImageNet\n",
      "imagesareofdifferentsizes,wealwaysapplycropandre-\n",
      "sizeimages(\n",
      "Krizhevskyetal.\n",
      ",\n",
      "2012\n",
      ";\n",
      "Szegedyetal.\n",
      ",\n",
      "2015\n",
      "),\n",
      "whichmakesitdiftostudyotheraugmentationsin\n",
      "theabsenceofcropping.Toeliminatethisconfound,we\n",
      "consideranasymmetricdatatransformationsettingforthis\n",
      "ablation.,wealwaysrandomlycropim-\n",
      "agesandresizethemtothesameresolution,andwethen\n",
      "applythetargetedtransformation(s)\n",
      "only\n",
      "toonebranchof\n",
      "theframeworkinFigure\n",
      "2\n",
      ",whileleavingtheotherbranch\n",
      "astheidentity(i.e.\n",
      "t\n",
      "(\n",
      "x\n",
      "i\n",
      ")=\n",
      "x\n",
      "i\n",
      ").Notethatthisasymmet-\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "(a)Withoutcolordistortion.\n",
      "(b)Withcolordistortion.\n",
      "Figure6.\n",
      "Histogramsofpixelintensities(overallchannels)for\n",
      "differentcropsoftwodifferentimages(i.e.tworows).Theimage\n",
      "fortherowisfromFigure\n",
      "4\n",
      ".Allaxeshavethesamerange.\n",
      "Colordistortionstrength\n",
      "Methods\n",
      "1/81/41/211(+Blur)\n",
      "AutoAug\n",
      "SimCLR\n",
      "59.661.062.663.264.5\n",
      "61.1\n",
      "Supervised\n",
      "77.076.776.575.775.4\n",
      "77.1\n",
      "Table1.\n",
      "Top-1accuracyofunsupervisedResNet-50usinglinear\n",
      "evaluationandsupervisedResNet-50\n",
      "5\n",
      ",undervariedcolordistor-\n",
      "tionstrength(seeAppendix\n",
      "A\n",
      ")andotherdatatransformations.\n",
      "Strength1(+Blur)isourdefaultdataaugmentationpolicy.\n",
      "ricdataaugmentationhurtstheperformance.Nonetheless,\n",
      "thissetupshouldnotsubstantivelychangetheimpactof\n",
      "individualdataaugmentationsortheircompositions.\n",
      "Figure\n",
      "5\n",
      "showslinearevaluationresultsunderindividual\n",
      "andcompositionoftransformations.Weobservethat\n",
      "no\n",
      "singletransformationsufcestolearngoodrepresentations\n",
      ",\n",
      "eventhoughthemodelcanalmostperfectlyidentifythe\n",
      "positivepairsinthecontrastivetask.Whencomposingaug-\n",
      "mentations,thecontrastivepredictiontaskbecomesharder,\n",
      "butthequalityofrepresentationimprovesdramatically.Ap-\n",
      "pendix\n",
      "B.2\n",
      "providesafurtherstudyoncomposingbroader\n",
      "setofaugmentations.\n",
      "Onecompositionofaugmentationsstandsout:randomcrop-\n",
      "pingandrandomcolordistortion.Weconjecturethatone\n",
      "seriousissuewhenusingonlyrandomcroppingasdata\n",
      "augmentationisthatmostpatchesfromanimagesharea\n",
      "similarcolordistribution.Figure\n",
      "6\n",
      "showsthatcolorhis-\n",
      "togramsalonesuftodistinguishimages.Neuralnets\n",
      "mayexploitthisshortcuttosolvethepredictivetask.There-\n",
      "fore,itiscriticaltocomposecroppingwithcolordistortion\n",
      "inordertolearngeneralizablefeatures.\n",
      "3.2.Contrastivelearningneedsstrongerdata\n",
      "augmentationthansupervisedlearning\n",
      "Tofurtherdemonstratetheimportanceofthecoloraug-\n",
      "mentation,weadjustthestrengthofcoloraugmentationas\n",
      "5\n",
      "Supervisedmodelsaretrainedfor90epochs;longertraining\n",
      "improvesperformanceofstrongeraugmentationby\n",
      "˘\n",
      "0\n",
      ":\n",
      "5%\n",
      ".\n",
      "Figure7.\n",
      "Linearevaluationofmodelswithvarieddepthandwidth.\n",
      "Modelsinbluedotsareourstrainedfor100epochs,modelsinred\n",
      "starsareourstrainedfor1000epochs,andmodelsingreencrosses\n",
      "aresupervisedResNetstrainedfor90epochs\n",
      "7\n",
      "(\n",
      "Heetal.\n",
      ",\n",
      "2016\n",
      ").\n",
      "showninTable\n",
      "1\n",
      ".Strongercoloraugmentationsubstan-\n",
      "tiallyimprovesthelinearevaluationofthelearnedunsuper-\n",
      "visedmodels.Inthiscontext,AutoAugment(\n",
      "Cubuketal.\n",
      ",\n",
      "2019\n",
      "),asophisticatedaugmentationpolicyfoundusingsu-\n",
      "pervisedlearning,doesnotworkbetterthansimplecropping\n",
      "+(stronger)colordistortion.Whentrainingsupervisedmod-\n",
      "elswiththesamesetofaugmentations,weobservethat\n",
      "strongercoloraugmentationdoesnotimproveorevenhurts\n",
      "theirperformance.Thus,ourexperimentsshowthatunsu-\n",
      "pervisedcontrastivelearningfromstronger(color)\n",
      "dataaugmentationthansupervisedlearning.Althoughpre-\n",
      "viousworkhasreportedthatdataaugmentationisuseful\n",
      "forself-supervisedlearning(\n",
      "Doerschetal.\n",
      ",\n",
      "2015\n",
      ";\n",
      "Bachman\n",
      "etal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Hénaffetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Asanoetal.\n",
      ",\n",
      "2019\n",
      "),we\n",
      "showthatdataaugmentationthatdoesnotyieldaccuracy\n",
      "forsupervisedlearningcanstillhelpconsiderably\n",
      "withcontrastivelearning.\n",
      "4.ArchitecturesforEncoderandHead\n",
      "4.1.Unsupervisedcontrastivelearning(more)\n",
      "frombiggermodels\n",
      "Figure\n",
      "7\n",
      "shows,perhapsunsurprisingly,thatincreasing\n",
      "depthandwidthbothimproveperformance.Whilesimilar\n",
      "holdforsupervisedlearning(\n",
      "Heetal.\n",
      ",\n",
      "2016\n",
      "),we\n",
      "thegapbetweensupervisedmodelsandlinear\n",
      "trainedonunsupervisedmodelsshrinksasthemodelsize\n",
      "increases,suggestingthat\n",
      "unsupervisedlearning\n",
      "morefrombiggermodelsthanitssupervisedcounterpart\n",
      ".\n",
      "7\n",
      "TraininglongerdoesnotimprovesupervisedResNets(see\n",
      "Appendix\n",
      "B.3\n",
      ").\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "Name\n",
      "Negativelossfunction\n",
      "Gradientw.r.t.\n",
      "u\n",
      "NT-Xent\n",
      "u\n",
      "T\n",
      "v\n",
      "+\n",
      "=˝\n",
      "\n",
      "log\n",
      "P\n",
      "v\n",
      "2f\n",
      "v\n",
      "+\n",
      ";\n",
      "v\n",
      "\n",
      "g\n",
      "exp(\n",
      "u\n",
      "T\n",
      "v\n",
      "=˝\n",
      ")\n",
      "(1\n",
      "\n",
      "exp(\n",
      "u\n",
      "T\n",
      "v\n",
      "+\n",
      "=˝\n",
      ")\n",
      "Z\n",
      "(\n",
      "u\n",
      ")\n",
      ")\n",
      "=˝\n",
      "v\n",
      "+\n",
      "\n",
      "P\n",
      "v\n",
      "\n",
      "exp(\n",
      "u\n",
      "T\n",
      "v\n",
      "\n",
      "=˝\n",
      ")\n",
      "Z\n",
      "(\n",
      "u\n",
      ")\n",
      "=˝\n",
      "v\n",
      "\n",
      "NT-Logistic\n",
      "log\n",
      "˙\n",
      "(\n",
      "u\n",
      "T\n",
      "v\n",
      "+\n",
      "=˝\n",
      ")+log\n",
      "˙\n",
      "(\n",
      "\n",
      "u\n",
      "T\n",
      "v\n",
      "\n",
      "=˝\n",
      ")\n",
      "(\n",
      "˙\n",
      "(\n",
      "\n",
      "u\n",
      "T\n",
      "v\n",
      "+\n",
      "=˝\n",
      "))\n",
      "=˝\n",
      "v\n",
      "+\n",
      "\n",
      "˙\n",
      "(\n",
      "u\n",
      "T\n",
      "v\n",
      "\n",
      "=˝\n",
      ")\n",
      "=˝\n",
      "v\n",
      "\n",
      "MarginTriplet\n",
      "\n",
      "max(\n",
      "u\n",
      "T\n",
      "v\n",
      "\n",
      "\n",
      "u\n",
      "T\n",
      "v\n",
      "+\n",
      "+\n",
      "m;\n",
      "0)\n",
      "v\n",
      "+\n",
      "\n",
      "v\n",
      "\n",
      "if\n",
      "u\n",
      "T\n",
      "v\n",
      "+\n",
      "\n",
      "u\n",
      "T\n",
      "v\n",
      "\n",
      "<m\n",
      "else\n",
      "0\n",
      "Table2.\n",
      "Negativelossfunctionsandtheirgradients.Allinputvectors,i.e.\n",
      "u\n",
      ";\n",
      "v\n",
      "+\n",
      ";\n",
      "v\n",
      "\n",
      ",are\n",
      "`\n",
      "2\n",
      "normalized.NT-Xentisanabbreviationfor\n",
      "ﬁNormalizedTemperature-scaledCrossEntropyﬂ.Differentlossfunctionsimposedifferentweightingsofpositiveandnegativeexamples.\n",
      "Figure8.\n",
      "Linearevaluationofrepresentationswithdifferentpro-\n",
      "jectionheads\n",
      "g\n",
      "(\n",
      "\n",
      ")\n",
      "andvariousdimensionsof\n",
      "z\n",
      "=\n",
      "g\n",
      "(\n",
      "h\n",
      ")\n",
      ".The\n",
      "representation\n",
      "h\n",
      "(beforeprojection)is2048-dimensionalhere.\n",
      "4.2.Anonlinearprojectionheadimprovesthe\n",
      "representationqualityofthelayerbeforeit\n",
      "Wethenstudytheimportanceofincludingaprojection\n",
      "head,i.e.\n",
      "g\n",
      "(\n",
      "h\n",
      ")\n",
      ".Figure\n",
      "8\n",
      "showslinearevaluationresults\n",
      "usingthreedifferentarchitectureforthehead:(1)identity\n",
      "mapping;(2)linearprojection,asusedbyseveralprevious\n",
      "approaches(\n",
      "Wuetal.\n",
      ",\n",
      "2018\n",
      ");and(3)thedefaultnonlinear\n",
      "projectionwithoneadditionalhiddenlayer(andReLUacti-\n",
      "vation),similarto\n",
      "Bachmanetal.\n",
      "(\n",
      "2019\n",
      ").Weobservethata\n",
      "nonlinearprojectionisbetterthanalinearprojection(+3%),\n",
      "andmuchbetterthannoprojection(>10%).Whenapro-\n",
      "jectionheadisused,similarresultsareobservedregardless\n",
      "ofoutputdimension.Furthermore,evenwhennonlinear\n",
      "projectionisused,thelayerbeforetheprojectionhead,\n",
      "h\n",
      ",\n",
      "isstillmuchbetter(>10%)thanthelayerafter,\n",
      "z\n",
      "=\n",
      "g\n",
      "(\n",
      "h\n",
      ")\n",
      ",\n",
      "whichshowsthat\n",
      "thehiddenlayerbeforetheprojection\n",
      "headisabetterrepresentationthanthelayerafter\n",
      ".\n",
      "Weconjecturethattheimportanceofusingtherepresenta-\n",
      "tionbeforethenonlinearprojectionisduetolossofinforma-\n",
      "tioninducedbythecontrastiveloss.Inparticular,\n",
      "z\n",
      "=\n",
      "g\n",
      "(\n",
      "h\n",
      ")\n",
      "istrainedtobeinvarianttodatatransformation.Thus,\n",
      "g\n",
      "can\n",
      "removeinformationthatmaybeusefulforthedownstream\n",
      "task,suchasthecolorororientationofobjects.Byleverag-\n",
      "ingthenonlineartransformation\n",
      "g\n",
      "(\n",
      "\n",
      ")\n",
      ",moreinformationcan\n",
      "beformedandmaintainedin\n",
      "h\n",
      ".Toverifythishypothesis,\n",
      "weconductexperimentsthatuseeither\n",
      "h\n",
      "or\n",
      "g\n",
      "(\n",
      "h\n",
      ")\n",
      "tolearn\n",
      "topredictthetransformationappliedduringthepretraining.\n",
      "Hereweset\n",
      "g\n",
      "(\n",
      "h\n",
      ")=\n",
      "W\n",
      "(2)\n",
      "˙\n",
      "(\n",
      "W\n",
      "(1)\n",
      "h\n",
      ")\n",
      ",withthesameinput\n",
      "andoutputdimensionality(i.e.2048).Table\n",
      "3\n",
      "shows\n",
      "h\n",
      "containsmuchmoreinformationaboutthetransformation\n",
      "applied,while\n",
      "g\n",
      "(\n",
      "h\n",
      ")\n",
      "losesinformation.Furtheranalysiscan\n",
      "Whattopredict?Randomguess\n",
      "Representation\n",
      "h\n",
      "g\n",
      "(\n",
      "h\n",
      ")\n",
      "Colorvsgrayscale8099.397.4\n",
      "Rotation2567.625.6\n",
      "Orig.vscorrupted5099.559.6\n",
      "Orig.vsSobel5096.656.3\n",
      "Table3.\n",
      "AccuracyoftrainingadditionalMLPsondifferentrepre-\n",
      "sentationstopredictthetransformationapplied.Otherthancrop\n",
      "andcoloraugmentation,weadditionallyandindependentlyadd\n",
      "rotation(oneof\n",
      "f\n",
      "0\n",
      "\n",
      ";\n",
      "90\n",
      "\n",
      ";\n",
      "180\n",
      "\n",
      ";\n",
      "270\n",
      "\n",
      "g\n",
      "),Gaussiannoise,andSo-\n",
      "beltransformationduringthepretrainingforthelastthree\n",
      "rows.Both\n",
      "h\n",
      "and\n",
      "g\n",
      "(\n",
      "h\n",
      ")\n",
      "areofthesamedimensionality,i.e.2048.\n",
      "befoundinAppendix\n",
      "B.4\n",
      ".\n",
      "5.LossFunctionsandBatchSize\n",
      "5.1.Normalizedcrossentropylosswithadjustable\n",
      "temperatureworksbetterthanalternatives\n",
      "WecomparetheNT-Xentlossagainstothercommonlyused\n",
      "contrastivelossfunctions,suchaslogisticloss(\n",
      "Mikolov\n",
      "etal.\n",
      ",\n",
      "2013\n",
      "),andmarginloss(\n",
      "Schroffetal.\n",
      ",\n",
      "2015\n",
      ").Table\n",
      "2\n",
      "showstheobjectivefunctionaswellasthegradientto\n",
      "theinputofthelossfunction.Lookingatthegradient,we\n",
      "observe1)\n",
      "`\n",
      "2\n",
      "normalization(i.e.cosinesimilarity)along\n",
      "withtemperatureeffectivelyweightsdifferentexamples,and\n",
      "anappropriatetemperaturecanhelpthemodellearnfrom\n",
      "hardnegatives;and2)unlikecross-entropy,otherobjec-\n",
      "tivefunctionsdonotweighthenegativesbytheirrelative\n",
      "hardness.Asaresult,onemustapplysemi-hardnegative\n",
      "mining(\n",
      "Schroffetal.\n",
      ",\n",
      "2015\n",
      ")fortheselossfunctions:in-\n",
      "steadofcomputingthegradientoveralllossterms,onecan\n",
      "computethegradientusingsemi-hardnegativeterms(\n",
      "i.e.\n",
      ",\n",
      "thosethatarewithinthelossmarginandclosestindistance,\n",
      "butfartherthanpositiveexamples).\n",
      "Tomakethecomparisonsfair,weusethesame\n",
      "`\n",
      "2\n",
      "normaliza-\n",
      "tionforalllossfunctions,andwetunethehyperparameters,\n",
      "andreporttheirbestresults.\n",
      "8\n",
      "Table\n",
      "4\n",
      "showsthat,while\n",
      "(semi-hard)negativemininghelps,thebestresultisstill\n",
      "muchworsethanourdefaultNT-Xentloss.\n",
      "8\n",
      "DetailscanbefoundinAppendix\n",
      "B.10\n",
      ".Forsimplicity,we\n",
      "onlyconsiderthenegativesfromoneaugmentationview.\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "MarginNT-Logi.Margin(sh)NT-Logi.(sh)NT-Xent\n",
      "50.951.657.557.963.9\n",
      "Table4.\n",
      "Linearevaluation(top-1)formodelstrainedwithdifferent\n",
      "lossfunctions.ﬁshﬂmeansusingsemi-hardnegativemining.\n",
      "`\n",
      "2\n",
      "norm?\n",
      "˝\n",
      "EntropyContrastiveacc.\n",
      "Top1\n",
      "Yes\n",
      "0.05\n",
      "1.090.5\n",
      "59.7\n",
      "0.1\n",
      "4.587.8\n",
      "64.4\n",
      "0.5\n",
      "8.268.2\n",
      "60.7\n",
      "1\n",
      "8.359.1\n",
      "58.0\n",
      "No\n",
      "10\n",
      "0.591.7\n",
      "57.2\n",
      "100\n",
      "0.592.1\n",
      "57.0\n",
      "Table5.\n",
      "Linearevaluationformodelstrainedwithdifferentchoices\n",
      "of\n",
      "`\n",
      "2\n",
      "normandtemperature\n",
      "˝\n",
      "forNT-Xentloss.Thecontrastive\n",
      "distributionisover4096examples.\n",
      "Figure9.\n",
      "Linearevaluationmodels(ResNet-50)trainedwithdiffer-\n",
      "entbatchsizeandepochs.Eachbarisasinglerunfromscratch.\n",
      "10\n",
      "Wenexttesttheimportanceofthe\n",
      "`\n",
      "2\n",
      "normalization(i.e.\n",
      "cosinesimilarityvsdotproduct)andtemperature\n",
      "˝\n",
      "inour\n",
      "defaultNT-Xentloss.Table\n",
      "5\n",
      "showsthatwithoutnormal-\n",
      "izationandpropertemperaturescaling,performanceissig-\n",
      "worse.Without\n",
      "`\n",
      "2\n",
      "normalization,thecontrastive\n",
      "taskaccuracyishigher,buttheresultingrepresentationis\n",
      "worseunderlinearevaluation.\n",
      "5.2.Contrastivelearning(more)fromlarger\n",
      "batchsizesandlongertraining\n",
      "Figure\n",
      "9\n",
      "showstheimpactofbatchsizewhenmodelsare\n",
      "trainedfordifferentnumbersofepochs.Wethat,when\n",
      "thenumberoftrainingepochsissmall(e.g.100epochs),\n",
      "largerbatchsizeshaveaadvantageoverthe\n",
      "smallerones.Withmoretrainingsteps/epochs,thegaps\n",
      "betweendifferentbatchsizesdecreaseordisappear,pro-\n",
      "videdthebatchesarerandomlyresampled.Incontrastto\n",
      "10\n",
      "Alinearlearningratescalingisusedhere.Figure\n",
      "B.1\n",
      "shows\n",
      "usingasquarerootlearningratescalingcanimproveperformance\n",
      "ofoneswithsmallbatchsizes.\n",
      "MethodArchitectureParam(M)Top1Top5\n",
      "MethodsusingResNet-50:\n",
      "LocalAgg.ResNet-502460.2-\n",
      "MoCoResNet-502460.6-\n",
      "PIRLResNet-502463.6-\n",
      "CPCv2ResNet-502463.885.3\n",
      "SimCLR(ours)ResNet-5024\n",
      "69.389.0\n",
      "Methodsusingotherarchitectures:\n",
      "RotationRevNet-50(\n",
      "4\n",
      "\n",
      ")8655.4-\n",
      "BigBiGANRevNet-50(\n",
      "4\n",
      "\n",
      ")8661.381.9\n",
      "AMDIMCustom-ResNet62668.1-\n",
      "CMCResNet-50(\n",
      "2\n",
      "\n",
      ")18868.488.2\n",
      "MoCoResNet-50(\n",
      "4\n",
      "\n",
      ")37568.6-\n",
      "CPCv2ResNet-161(\n",
      "\n",
      ")30571.590.1\n",
      "SimCLR(ours)ResNet-50(\n",
      "2\n",
      "\n",
      ")9474.292.0\n",
      "SimCLR(ours)ResNet-50(\n",
      "4\n",
      "\n",
      ")375\n",
      "76.593.2\n",
      "Table6.\n",
      "ImageNetaccuraciesoflineartrainedonrepre-\n",
      "sentationslearnedwithdifferentself-supervisedmethods.\n",
      "MethodArchitecture\n",
      "Labelfraction\n",
      "1%10%\n",
      "Top5\n",
      "SupervisedbaselineResNet-5048.480.4\n",
      "Methodsusingotherlabel-propagation:\n",
      "Pseudo-labelResNet-5051.682.4\n",
      "VAT+EntropyMin.ResNet-5047.083.4\n",
      "UDA(w.RandAug)ResNet-50-88.5\n",
      "FixMatch(w.RandAug)ResNet-50-89.1\n",
      "S4L(Rot+VAT+En.M.)ResNet-50(4\n",
      "\n",
      ")-91.2\n",
      "Methodsusingrepresentationlearningonly:\n",
      "InstDiscResNet-5039.277.4\n",
      "BigBiGANRevNet-50(\n",
      "4\n",
      "\n",
      ")55.278.8\n",
      "PIRLResNet-5057.283.8\n",
      "CPCv2ResNet-161(\n",
      "\n",
      ")77.991.2\n",
      "SimCLR(ours)ResNet-5075.587.8\n",
      "SimCLR(ours)ResNet-50(\n",
      "2\n",
      "\n",
      ")83.091.2\n",
      "SimCLR(ours)ResNet-50(\n",
      "4\n",
      "\n",
      ")\n",
      "85.892.6\n",
      "Table7.\n",
      "ImageNetaccuracyofmodelstrainedwithfewlabels.\n",
      "supervisedlearning(\n",
      "Goyaletal.\n",
      ",\n",
      "2017\n",
      "),incontrastivelearn-\n",
      "ing,largerbatchsizesprovidemorenegativeexamples,\n",
      "facilitatingconvergence(i.e.takingfewerepochsandsteps\n",
      "foragivenaccuracy).Traininglongeralsoprovidesmore\n",
      "negativeexamples,improvingtheresults.InAppendix\n",
      "B.1\n",
      ",\n",
      "resultswithevenlongertrainingstepsareprovided.\n",
      "6.ComparisonwithState-of-the-art\n",
      "Inthissubsection,similarto\n",
      "Kolesnikovetal.\n",
      "(\n",
      "2019\n",
      ");\n",
      "He\n",
      "etal.\n",
      "(\n",
      "2019\n",
      "),weuseResNet-50in3differenthiddenlayer\n",
      "widths(widthmultipliersof\n",
      "1\n",
      "\n",
      ",\n",
      "2\n",
      "\n",
      ",and\n",
      "4\n",
      "\n",
      ").Forbetter\n",
      "convergence,ourmodelsherearetrainedfor1000epochs.\n",
      "Linearevaluation.\n",
      "Table\n",
      "6\n",
      "comparesourresultswithprevi-\n",
      "ousapproaches(\n",
      "Zhuangetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Heetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Misra\n",
      "&vanderMaaten\n",
      ",\n",
      "2019\n",
      ";\n",
      "Hénaffetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Kolesnikov\n",
      "etal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Donahue&Simonyan\n",
      ",\n",
      "2019\n",
      ";\n",
      "Bachmanetal.\n",
      ",\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "FoodCIFAR10CIFAR100BirdsnapSUN397CarsAircraftVOC2007DTDPetsCaltech-101Flowers\n",
      "Linearevaluation:\n",
      "SimCLR(ours)\n",
      "76.995.3\n",
      "80.248.4\n",
      "65.9\n",
      "60.061.2\n",
      "84.278.9\n",
      "89.2\n",
      "93.995.0\n",
      "Supervised75.2\n",
      "95.781.256.4\n",
      "64.9\n",
      "68.863.8\n",
      "83.8\n",
      "78.792.394.1\n",
      "94.2\n",
      "Fine-tuned:\n",
      "SimCLR(ours)\n",
      "89.498.689.078.268.192.187.086.677.8\n",
      "92.1\n",
      "94.1\n",
      "97.6\n",
      "Supervised88.798.3\n",
      "88.777.8\n",
      "67.091.4\n",
      "88.0\n",
      "86.5\n",
      "78.893.294.298.0\n",
      "Randominit88.396.081.9\n",
      "77.0\n",
      "53.791.384.869.464.182.772.592.5\n",
      "Table8.\n",
      "Comparisonoftransferlearningperformanceofourself-supervisedapproachwithsupervisedbaselinesacross12naturalimage\n",
      "datasets,forResNet-50\n",
      "(4\n",
      "\n",
      ")\n",
      "modelspretrainedonImageNet.Resultsnotworsethanthebest(\n",
      "p>\n",
      "0\n",
      ":\n",
      "05\n",
      ",\n",
      "permutationtest)areshowninbold.SeeAppendix\n",
      "B.8\n",
      "forexperimentaldetailsandresultswithstandardResNet-50.\n",
      "2019\n",
      ";\n",
      "Tianetal.\n",
      ",\n",
      "2019\n",
      ")inthelinearevaluationsetting(see\n",
      "Appendix\n",
      "B.6\n",
      ").Table\n",
      "1\n",
      "showsmorenumericalcompar-\n",
      "isonsamongdifferentmethods.Weareabletousestandard\n",
      "networkstoobtainsubstantiallybetterresultscomparedto\n",
      "previousmethodsthatrequiredesignedarchi-\n",
      "tectures.ThebestresultobtainedwithourResNet-50(\n",
      "4\n",
      "\n",
      ")\n",
      "canmatchthesupervisedpretrainedResNet-50.\n",
      "Semi-supervisedlearning.\n",
      "Wefollow\n",
      "Zhaietal.\n",
      "(\n",
      "2019\n",
      ")\n",
      "andsample1%or10%ofthelabeledILSVRC-12training\n",
      "datasetsinaclass-balancedway(\n",
      "˘\n",
      "12.8and\n",
      "˘\n",
      "128images\n",
      "perclassrespectively).\n",
      "11\n",
      "Wesimplythewhole\n",
      "basenetworkonthelabeleddatawithoutregularization\n",
      "(seeAppendix\n",
      "B.5\n",
      ").Table\n",
      "7\n",
      "showsthecomparisonsof\n",
      "ourresultsagainstrecentmethods(\n",
      "Zhaietal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Xie\n",
      "etal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Sohnetal.\n",
      ",\n",
      "2020\n",
      ";\n",
      "Wuetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Donahue&\n",
      "Simonyan\n",
      ",\n",
      "2019\n",
      ";\n",
      "Misra&vanderMaaten\n",
      ",\n",
      "2019\n",
      ";\n",
      "Hénaff\n",
      "etal.\n",
      ",\n",
      "2019\n",
      ").Thesupervisedbaselinefrom(\n",
      "Zhaietal.\n",
      ",\n",
      "2019\n",
      ")isstrongduetointensivesearchofhyper-parameters\n",
      "(includingaugmentation).Again,ourapproach\n",
      "improvesoverstate-of-the-artwithboth1%and10%ofthe\n",
      "labels.Interestingly,ourpretrainedResNet-50\n",
      "(2\n",
      "\n",
      ";\n",
      "4\n",
      "\n",
      ")on\n",
      "full\n",
      "ImageNetarealsobetterthen\n",
      "trainingfromscratch(upto2%,seeAppendix\n",
      "B.2\n",
      ").\n",
      "Transferlearning.\n",
      "Weevaluatetransferlearningperfor-\n",
      "manceacross12naturalimagedatasetsinbothlinearevalu-\n",
      "ationedfeatureextractor)andsettings.Fol-\n",
      "lowing\n",
      "Kornblithetal.\n",
      "(\n",
      "2019\n",
      "),weperformhyperparameter\n",
      "tuningforeachmodel-datasetcombinationandselectthe\n",
      "besthyperparametersonavalidationset.Table\n",
      "8\n",
      "shows\n",
      "resultswiththeResNet-50(\n",
      "4\n",
      "\n",
      ")model.When\n",
      "ourself-supervisedmodeloutperformsthesu-\n",
      "pervisedbaselineon5datasets,whereasthesupervised\n",
      "baselineissuperiorononly2(i.e.PetsandFlowers).On\n",
      "theremaining5datasets,themodelsarestatisticallytied.\n",
      "Fullexperimentaldetailsaswellasresultswiththestandard\n",
      "ResNet-50architectureareprovidedinAppendix\n",
      "B.8\n",
      ".\n",
      "11\n",
      "Thedetailsofsamplingandexactsubsetscanbefoundin\n",
      "https://wwww.org/datasets/catalog/imagenet2012_subset\n",
      ".\n",
      "7.RelatedWork\n",
      "Theideaofmakingrepresentationsofanimageagreewith\n",
      "eachotherundersmalltransformationsdatesbackto\n",
      "Becker\n",
      "&Hinton\n",
      "(\n",
      "1992\n",
      ").Weextenditbyleveragingrecentad-\n",
      "vancesindataaugmentation,networkarchitectureandcon-\n",
      "trastiveloss.Asimilarconsistencyidea,butfor\n",
      "classlabel\n",
      "prediction\n",
      ",hasbeenexploredinothercontextssuchassemi-\n",
      "supervisedlearning(\n",
      "Xieetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Berthelotetal.\n",
      ",\n",
      "2019\n",
      ").\n",
      "Handcraftedpretexttasks.\n",
      "Therecentrenaissanceofself-\n",
      "supervisedlearningbeganwithdesignedpretext\n",
      "tasks,suchasrelativepatchprediction(\n",
      "Doerschetal.\n",
      ",\n",
      "2015\n",
      "),\n",
      "solvingjigsawpuzzles(\n",
      "Noroozi&Favaro\n",
      ",\n",
      "2016\n",
      "),coloriza-\n",
      "tion(\n",
      "Zhangetal.\n",
      ",\n",
      "2016\n",
      ")androtationprediction(\n",
      "Gidaris\n",
      "etal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Chenetal.\n",
      ",\n",
      "2019\n",
      ").Althoughgoodresults\n",
      "canbeobtainedwithbiggernetworksandlongertrain-\n",
      "ing(\n",
      "Kolesnikovetal.\n",
      ",\n",
      "2019\n",
      "),thesepretexttasksrelyon\n",
      "somewhatad-hocheuristics,whichlimitsthegeneralityof\n",
      "learnedrepresentations.\n",
      "Contrastivevisualrepresentationlearning.\n",
      "Datingback\n",
      "to\n",
      "Hadselletal.\n",
      "(\n",
      "2006\n",
      "),theseapproacheslearnrepresen-\n",
      "tationsbycontrastingpositivepairsagainstnegativepairs.\n",
      "Alongtheselines,\n",
      "Dosovitskiyetal.\n",
      "(\n",
      "2014\n",
      ")proposesto\n",
      "treateachinstanceasaclassrepresentedbyafeaturevector\n",
      "(inaparametricform).\n",
      "Wuetal.\n",
      "(\n",
      "2018\n",
      ")proposestouse\n",
      "amemorybanktostoretheinstanceclassrepresentation\n",
      "vector,anapproachadoptedandextendedinseveralrecent\n",
      "papers(\n",
      "Zhuangetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Tianetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Heetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Misra&vanderMaaten\n",
      ",\n",
      "2019\n",
      ").Otherworkexplores\n",
      "theuseofin-batchsamplesfornegativesamplinginstead\n",
      "ofamemorybank(\n",
      "Doersch&Zisserman\n",
      ",\n",
      "2017\n",
      ";\n",
      "Yeetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Jietal.\n",
      ",\n",
      "2019\n",
      ").\n",
      "Recentliteraturehasattemptedtorelatethesuccessoftheir\n",
      "methodstomaximizationofmutualinformationbetween\n",
      "latentrepresentations(\n",
      "Oordetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Hénaffetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Hjelmetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Bachmanetal.\n",
      ",\n",
      "2019\n",
      ").However,itisnot\n",
      "clearifthesuccessofcontrastiveapproachesisdetermined\n",
      "bythemutualinformation,orbytheformofthe\n",
      "contrastiveloss(\n",
      "Tschannenetal.\n",
      ",\n",
      "2019\n",
      ").\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "Wenotethatalmostallindividualcomponentsofourframe-\n",
      "workhaveappearedinpreviouswork,althoughthe\n",
      "instantiationsmaybedifferent.Thesuperiorityofourframe-\n",
      "workrelativetopreviousworkisnotexplainedbyanysingle\n",
      "designchoice,butbytheircomposition.Weprovideacom-\n",
      "prehensivecomparisonofourdesignchoiceswiththoseof\n",
      "previousworkinAppendix\n",
      "C\n",
      ".\n",
      "8.Conclusion\n",
      "Inthiswork,wepresentasimpleframeworkanditsin-\n",
      "stantiationforcontrastivevisualrepresentationlearning.\n",
      "Wecarefullystudyitscomponents,andshowtheeffects\n",
      "ofdifferentdesignchoices.Bycombiningour\n",
      "weimproveconsiderablyoverpreviousmethodsforself-\n",
      "supervised,semi-supervised,andtransferlearning.\n",
      "Ourapproachdiffersfromstandardsupervisedlearningon\n",
      "ImageNetonlyinthechoiceofdataaugmentation,theuseof\n",
      "anonlinearheadattheendofthenetwork,andthelossfunc-\n",
      "tion.Thestrengthofthissimpleframeworksuggeststhat,\n",
      "despitearecentsurgeininterest,self-supervisedlearning\n",
      "remainsundervalued.\n",
      "Acknowledgements\n",
      "WewouldliketothankXiaohuaZhai,RafaelMüllerand\n",
      "YaniIoannoufortheirfeedbackonthedraft.Wearealso\n",
      "gratefulforgeneralsupportfromGoogleResearchteamsin\n",
      "Torontoandelsewhere.\n",
      "References\n",
      "Asano,Y.M.,Rupprecht,C.,andVedaldi,A.Acriticalanalysis\n",
      "ofself-supervision,orwhatwecanlearnfromasingleimage.\n",
      "arXivpreprintarXiv:1904.13132\n",
      ",2019.\n",
      "Bachman,P.,Hjelm,R.D.,andBuchwalter,W.Learningrep-\n",
      "resentationsbymaximizingmutualinformationacrossviews.\n",
      "In\n",
      "AdvancesinNeuralInformationProcessingSystems\n",
      ",pp.\n",
      "15509Œ15519,2019.\n",
      "Becker,S.andHinton,G.E.Self-organizingneuralnetworkthat\n",
      "discoverssurfacesinrandom-dotstereograms.\n",
      "Nature\n",
      ",355\n",
      "(6356):161Œ163,1992.\n",
      "Berg,T.,Liu,J.,Lee,S.W.,Alexander,M.L.,Jacobs,D.W.,\n",
      "andBelhumeur,P.N.Birdsnap:Large-scalevisual\n",
      "categorizationofbirds.In\n",
      "IEEEConferenceonComputerVision\n",
      "andPatternRecognition(CVPR)\n",
      ",pp.2019Œ2026.IEEE,2014.\n",
      "Berthelot,D.,Carlini,N.,Goodfellow,I.,Papernot,N.,Oliver,\n",
      "A.,andRaffel,C.A.Mixmatch:Aholisticapproachtosemi-\n",
      "supervisedlearning.In\n",
      "AdvancesinNeuralInformationPro-\n",
      "cessingSystems\n",
      ",pp.5050Œ5060,2019.\n",
      "Bossard,L.,Guillaumin,M.,andVanGool,L.Food-101Œmining\n",
      "discriminativecomponentswithrandomforests.In\n",
      "European\n",
      "conferenceoncomputervision\n",
      ",pp.446Œ461.Springer,2014.\n",
      "Chen,T.,Sun,Y.,Shi,Y.,andHong,L.Onsamplingstrategies\n",
      "forneuralnetwork-basedcollaborativeIn\n",
      "Proceed-\n",
      "ingsofthe23rdACMSIGKDDInternationalConferenceon\n",
      "KnowledgeDiscoveryandDataMining\n",
      ",pp.767Œ776,2017.\n",
      "Chen,T.,Zhai,X.,Ritter,M.,Lucic,M.,andHoulsby,N.Self-\n",
      "supervisedgansviaauxiliaryrotationloss.In\n",
      "Proceedingsofthe\n",
      "IEEEConferenceonComputerVisionandPatternRecognition\n",
      ",\n",
      "pp.12154Œ12163,2019.\n",
      "Cimpoi,M.,Maji,S.,Kokkinos,I.,Mohamed,S.,andVedaldi,\n",
      "A.Describingtexturesinthewild.In\n",
      "IEEEConferenceon\n",
      "ComputerVisionandPatternRecognition(CVPR)\n",
      ",pp.3606Œ\n",
      "3613.IEEE,2014.\n",
      "Cubuk,E.D.,Zoph,B.,Mane,D.,Vasudevan,V.,andLe,Q.V.\n",
      "Autoaugment:Learningaugmentationstrategiesfromdata.In\n",
      "ProceedingsoftheIEEEconferenceoncomputervisionand\n",
      "patternrecognition\n",
      ",pp.113Œ123,2019.\n",
      "DeVries,T.andTaylor,G.W.Improvedregularizationof\n",
      "convolutionalneuralnetworkswithcutout.\n",
      "arXivpreprint\n",
      "arXiv:1708.04552\n",
      ",2017.\n",
      "Doersch,C.andZisserman,A.Multi-taskself-supervisedvisual\n",
      "learning.In\n",
      "ProceedingsoftheIEEEInternationalConference\n",
      "onComputerVision\n",
      ",pp.2051Œ2060,2017.\n",
      "Doersch,C.,Gupta,A.,andEfros,A.A.Unsupervisedvisual\n",
      "representationlearningbycontextprediction.In\n",
      "Proceedings\n",
      "oftheIEEEInternationalConferenceonComputerVision\n",
      ",pp.\n",
      "1422Œ1430,2015.\n",
      "Donahue,J.andSimonyan,K.Largescaleadversarialrepresenta-\n",
      "tionlearning.In\n",
      "AdvancesinNeuralInformationProcessing\n",
      "Systems\n",
      ",pp.10541Œ10551,2019.\n",
      "Donahue,J.,Jia,Y.,Vinyals,O.,Hoffman,J.,Zhang,N.,Tzeng,E.,\n",
      "andDarrell,T.Decaf:Adeepconvolutionalactivationfeature\n",
      "forgenericvisualrecognition.In\n",
      "InternationalConferenceon\n",
      "MachineLearning\n",
      ",pp.647Œ655,2014.\n",
      "Dosovitskiy,A.,Springenberg,J.T.,Riedmiller,M.,andBrox,T.\n",
      "Discriminativeunsupervisedfeaturelearningwithconvolutional\n",
      "neuralnetworks.In\n",
      "Advancesinneuralinformationprocessing\n",
      "systems\n",
      ",pp.766Œ774,2014.\n",
      "Everingham,M.,VanGool,L.,Williams,C.K.,Winn,J.,and\n",
      "Zisserman,A.Thepascalvisualobjectclasses(voc)challenge.\n",
      "InternationalJournalofComputerVision\n",
      ",88(2):303Œ338,2010.\n",
      "Fei-Fei,L.,Fergus,R.,andPerona,P.Learninggenerativevisual\n",
      "modelsfromfewtrainingexamples:Anincrementalbayesian\n",
      "approachtestedon101objectcategories.In\n",
      "IEEEConference\n",
      "onComputerVisionandPatternRecognition(CVPR)Workshop\n",
      "onGenerative-ModelBasedVision\n",
      ",2004.\n",
      "Gidaris,S.,Singh,P.,andKomodakis,N.Unsupervisedrepresen-\n",
      "tationlearningbypredictingimagerotations.\n",
      "arXivpreprint\n",
      "arXiv:1803.07728\n",
      ",2018.\n",
      "Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-\n",
      "Farley,D.,Ozair,S.,Courville,A.,andBengio,Y.Generative\n",
      "adversarialnets.In\n",
      "Advancesinneuralinformationprocessing\n",
      "systems\n",
      ",pp.2672Œ2680,2014.\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "Goyal,P.,Dollár,P.,Girshick,R.,Noordhuis,P.,Wesolowski,L.,\n",
      "Kyrola,A.,Tulloch,A.,Jia,Y.,andHe,K.Accurate,large\n",
      "minibatchsgd:Trainingimagenetin1hour.\n",
      "arXivpreprint\n",
      "arXiv:1706.02677\n",
      ",2017.\n",
      "Hadsell,R.,Chopra,S.,andLeCun,Y.Dimensionalityreduction\n",
      "bylearninganinvariantmapping.In\n",
      "2006IEEEComputerSo-\n",
      "cietyConferenceonComputerVisionandPatternRecognition\n",
      "(CVPR'06)\n",
      ",volume2,pp.1735Œ1742.IEEE,2006.\n",
      "He,K.,Zhang,X.,Ren,S.,andSun,J.Deepresiduallearningfor\n",
      "imagerecognition.In\n",
      "ProceedingsoftheIEEEconferenceon\n",
      "computervisionandpatternrecognition\n",
      ",pp.770Œ778,2016.\n",
      "He,K.,Fan,H.,Wu,Y.,Xie,S.,andGirshick,R.Momentum\n",
      "contrastforunsupervisedvisualrepresentationlearning.\n",
      "arXiv\n",
      "preprintarXiv:1911.05722\n",
      ",2019.\n",
      "Hénaff,O.J.,Razavi,A.,Doersch,C.,Eslami,S.,andOord,A.\n",
      "v.d.Data-efcientimagerecognitionwithcontrastivepredictive\n",
      "coding.\n",
      "arXivpreprintarXiv:1905.09272\n",
      ",2019.\n",
      "Hinton,G.E.,Osindero,S.,andTeh,Y.-W.Afastlearningal-\n",
      "gorithmfordeepbeliefnets.\n",
      "Neuralcomputation\n",
      ",18(7):1527Œ\n",
      "1554,2006.\n",
      "Hjelm,R.D.,Fedorov,A.,Lavoie-Marchildon,S.,Grewal,K.,\n",
      "Bachman,P.,Trischler,A.,andBengio,Y.Learningdeeprepre-\n",
      "sentationsbymutualinformationestimationandmaximization.\n",
      "arXivpreprintarXiv:1808.06670\n",
      ",2018.\n",
      "Howard,A.G.Someimprovementsondeepconvolutional\n",
      "neuralnetworkbasedimage\n",
      "arXivpreprint\n",
      "arXiv:1312.5402\n",
      ",2013.\n",
      "Ioffe,S.andSzegedy,C.Batchnormalization:Acceleratingdeep\n",
      "networktrainingbyreducinginternalcovariateshift.\n",
      "arXiv\n",
      "preprintarXiv:1502.03167\n",
      ",2015.\n",
      "Ji,X.,Henriques,J.F.,andVedaldi,A.Invariantinformation\n",
      "clusteringforunsupervisedimageandsegmenta-\n",
      "tion.In\n",
      "ProceedingsoftheIEEEInternationalConferenceon\n",
      "ComputerVision\n",
      ",pp.9865Œ9874,2019.\n",
      "Kingma,D.P.andWelling,M.Auto-encodingvariationalbayes.\n",
      "arXivpreprintarXiv:1312.6114\n",
      ",2013.\n",
      "Kolesnikov,A.,Zhai,X.,andBeyer,L.Revisitingself-supervised\n",
      "visualrepresentationlearning.In\n",
      "ProceedingsoftheIEEE\n",
      "conferenceonComputerVisionandPatternRecognition\n",
      ",pp.\n",
      "1920Œ1929,2019.\n",
      "Kornblith,S.,Shlens,J.,andLe,Q.V.DobetterImageNetmodels\n",
      "transferbetter?In\n",
      "ProceedingsoftheIEEEconferenceon\n",
      "computervisionandpatternrecognition\n",
      ",pp.2661Œ2671,2019.\n",
      "Krause,J.,Deng,J.,Stark,M.,andFei-Fei,L.Collectinga\n",
      "large-scaledatasetofcars.In\n",
      "SecondWorkshopon\n",
      "Fine-GrainedVisualCategorization\n",
      ",2013.\n",
      "Krizhevsky,A.andHinton,G.Learningmultiplelayersoffeatures\n",
      "fromtinyimages.Technicalreport,UniversityofToronto,\n",
      "2009.URL\n",
      "https://www.cs.toronto.edu/~kriz/\n",
      "learning-features-2009-TR.pdf\n",
      ".\n",
      "Krizhevsky,A.,Sutskever,I.,andHinton,G.E.Imagenet\n",
      "cationwithdeepconvolutionalneuralnetworks.In\n",
      "Advancesin\n",
      "neuralinformationprocessingsystems\n",
      ",pp.1097Œ1105,2012.\n",
      "Loshchilov,I.andHutter,F.Sgdr:Stochasticgradientdescent\n",
      "withwarmrestarts.\n",
      "arXivpreprintarXiv:1608.03983\n",
      ",2016.\n",
      "Maaten,L.v.d.andHinton,G.Visualizingdatausingt-sne.\n",
      "Jour-\n",
      "nalofmachinelearningresearch\n",
      ",9(Nov):2579Œ2605,2008.\n",
      "Maji,S.,Kannala,J.,Rahtu,E.,Blaschko,M.,andVedaldi,A.\n",
      "Fine-grainedvisualofaircraft.Technicalreport,\n",
      "2013.\n",
      "Mikolov,T.,Chen,K.,Corrado,G.,andDean,J.Efcientesti-\n",
      "mationofwordrepresentationsinvectorspace.\n",
      "arXivpreprint\n",
      "arXiv:1301.3781\n",
      ",2013.\n",
      "Misra,I.andvanderMaaten,L.Self-supervisedlearn-\n",
      "ingofpretext-invariantrepresentations.\n",
      "arXivpreprint\n",
      "arXiv:1912.01991\n",
      ",2019.\n",
      "Nilsback,M.-E.andZisserman,A.Automatedwer\n",
      "overalargenumberofclasses.In\n",
      "ComputerVision,Graphics&\n",
      "ImageProcessing,2008.ICVGIP'08.SixthIndianConference\n",
      "on\n",
      ",pp.722Œ729.IEEE,2008.\n",
      "Noroozi,M.andFavaro,P.Unsupervisedlearningofvisualrepre-\n",
      "sentationsbysolvingjigsawpuzzles.In\n",
      "EuropeanConference\n",
      "onComputerVision\n",
      ",pp.69Œ84.Springer,2016.\n",
      "Oord,A.v.d.,Li,Y.,andVinyals,O.Representationlearningwith\n",
      "contrastivepredictivecoding.\n",
      "arXivpreprintarXiv:1807.03748\n",
      ",\n",
      "2018.\n",
      "Parkhi,O.M.,Vedaldi,A.,Zisserman,A.,andJawahar,C.Cats\n",
      "anddogs.In\n",
      "IEEEConferenceonComputerVisionandPattern\n",
      "Recognition(CVPR)\n",
      ",pp.3498Œ3505.IEEE,2012.\n",
      "Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,\n",
      "S.,Huang,Z.,Karpathy,A.,Khosla,A.,Bernstein,M.,etal.\n",
      "Imagenetlargescalevisualrecognitionchallenge.\n",
      "International\n",
      "journalofcomputervision\n",
      ",115(3):211Œ252,2015.\n",
      "Schroff,F.,Kalenichenko,D.,andPhilbin,J.Facenet:Aed\n",
      "embeddingforfacerecognitionandclustering.In\n",
      "Proceed-\n",
      "ingsoftheIEEEconferenceoncomputervisionandpattern\n",
      "recognition\n",
      ",pp.815Œ823,2015.\n",
      "Simonyan,K.andZisserman,A.Verydeepconvolutional\n",
      "networksforlarge-scaleimagerecognition.\n",
      "arXivpreprint\n",
      "arXiv:1409.1556\n",
      ",2014.\n",
      "Sohn,K.Improveddeepmetriclearningwithmulti-classn-pair\n",
      "lossobjective.In\n",
      "Advancesinneuralinformationprocessing\n",
      "systems\n",
      ",pp.1857Œ1865,2016.\n",
      "Sohn,K.,Berthelot,D.,Li,C.-L.,Zhang,Z.,Carlini,N.,Cubuk,\n",
      "E.D.,Kurakin,A.,Zhang,H.,andRaffel,C.Fixmatch:Simpli-\n",
      "fyingsemi-supervisedlearningwithconsistencyand\n",
      "arXivpreprintarXiv:2001.07685\n",
      ",2020.\n",
      "Szegedy,C.,Liu,W.,Jia,Y.,Sermanet,P.,Reed,S.,Anguelov,D.,\n",
      "Erhan,D.,Vanhoucke,V.,andRabinovich,A.Goingdeeper\n",
      "withconvolutions.In\n",
      "ProceedingsoftheIEEEconferenceon\n",
      "computervisionandpatternrecognition\n",
      ",pp.1Œ9,2015.\n",
      "Tian,Y.,Krishnan,D.,andIsola,P.Contrastivemultiviewcoding.\n",
      "arXivpreprintarXiv:1906.05849\n",
      ",2019.\n",
      "Tschannen,M.,Djolonga,J.,Rubenstein,P.K.,Gelly,S.,andLu-\n",
      "cic,M.Onmutualinformationmaximizationforrepresentation\n",
      "learning.\n",
      "arXivpreprintarXiv:1907.13625\n",
      ",2019.\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "Wu,Z.,Xiong,Y.,Yu,S.X.,andLin,D.Unsupervisedfeature\n",
      "learningvianon-parametricinstancediscrimination.In\n",
      "Proceed-\n",
      "ingsoftheIEEEConferenceonComputerVisionandPattern\n",
      "Recognition\n",
      ",pp.3733Œ3742,2018.\n",
      "Xiao,J.,Hays,J.,Ehinger,K.A.,Oliva,A.,andTorralba,A.Sun\n",
      "database:Large-scalescenerecognitionfromabbeytozoo.In\n",
      "IEEEConferenceonComputerVisionandPatternRecognition\n",
      "(CVPR)\n",
      ",pp.3485Œ3492.IEEE,2010.\n",
      "Xie,Q.,Dai,Z.,Hovy,E.,Luong,M.-T.,andLe,Q.V.Unsu-\n",
      "perviseddataaugmentation.\n",
      "arXivpreprintarXiv:1904.12848\n",
      ",\n",
      "2019.\n",
      "Ye,M.,Zhang,X.,Yuen,P.C.,andChang,S.-F.Unsupervised\n",
      "embeddinglearningviainvariantandspreadinginstancefeature.\n",
      "In\n",
      "ProceedingsoftheIEEEConferenceonComputerVision\n",
      "andPatternRecognition\n",
      ",pp.6210Œ6219,2019.\n",
      "You,Y.,Gitman,I.,andGinsburg,B.Largebatchtrainingofcon-\n",
      "volutionalnetworks.\n",
      "arXivpreprintarXiv:1708.03888\n",
      ",2017.\n",
      "Zhai,X.,Oliver,A.,Kolesnikov,A.,andBeyer,L.S4l:Self-\n",
      "supervisedsemi-supervisedlearning.In\n",
      "TheIEEEInternational\n",
      "ConferenceonComputerVision(ICCV)\n",
      ",October2019.\n",
      "Zhang,R.,Isola,P.,andEfros,A.A.Colorfulimagecoloriza-\n",
      "tion.In\n",
      "Europeanconferenceoncomputervision\n",
      ",pp.649Œ666.\n",
      "Springer,2016.\n",
      "Zhuang,C.,Zhai,A.L.,andYamins,D.Localaggregationfor\n",
      "unsupervisedlearningofvisualembeddings.In\n",
      "Proceedings\n",
      "oftheIEEEInternationalConferenceonComputerVision\n",
      ",pp.\n",
      "6002Œ6012,2019.\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "A.DataAugmentationDetails\n",
      "Inourdefaultpretrainingsetting(whichisusedtotrainourbestmodels),weutilizerandomcrop(withresizeandrandom\n",
      "randomcolordistortion,andrandomGaussianblurasthedataaugmentations.Thedetailsofthesethreeaugmentations\n",
      "areprovidedbelow.\n",
      "Randomcropandresizeto224x224\n",
      "WeusestandardInception-stylerandomcropping(\n",
      "Szegedyetal.\n",
      ",\n",
      "2015\n",
      ").The\n",
      "cropofrandomsize(uniformfrom0.08to1.0inarea)oftheoriginalsizeandarandomaspectratio(default:of\n",
      "3/4to4/3)oftheoriginalaspectratioismade.Thiscropisresizedtotheoriginalsize.Thishasbeenimple-\n",
      "mentedinTwasﬁ\n",
      "slim\n",
      ":\n",
      "preprocessing\n",
      ":\n",
      "inception\n",
      "_\n",
      "preprocessing\n",
      ":\n",
      "distorted\n",
      "_\n",
      "bounding\n",
      "_\n",
      "box\n",
      "_\n",
      "crop\n",
      "ﬂ,orinPytorch\n",
      "asﬁ\n",
      "torchvision\n",
      ":\n",
      "transforms\n",
      ":\n",
      "RandomResizedCrop\n",
      "ﬂ.Additionally,therandomcrop(withresize)isalwaysfollowedbya\n",
      "randomhorizontal/left-to-rightwith\n",
      "50%\n",
      "probability.Thisishelpfulbutnotessential.Byremovingthisfromourdefault\n",
      "augmentationpolicy,thetop-1linearevaluationdropsfrom64.5%to63.4%forourResNet-50modeltrainedin100epochs.\n",
      "Colordistortion\n",
      "Colordistortioniscomposedbycolorjitteringandcolordropping.Westrongercolorjittering\n",
      "usuallyhelps,sowesetastrengthparameter.\n",
      "Apseudo-codeforcolordistortionusingTensorFlowisasfollows.\n",
      "importtensorflowastf\n",
      "defcolor_distortion(image,s=1.0):\n",
      "#imageisatensorwithvaluerangein[0,1].\n",
      "#sisthestrengthofcolordistortion.\n",
      "defcolor_jitter(x):\n",
      "#onecanalsoshuffletheorderoffollowingaugmentations\n",
      "#eachtimetheyareapplied.\n",
      "x=tf.image.random_brightness(x,max_delta=0.8\n",
      "*\n",
      "s)\n",
      "x=tf.image.random_contrast(x,lower=1-0.8\n",
      "*\n",
      "s,upper=1+0.8\n",
      "*\n",
      "s)\n",
      "x=tf.image.random_saturation(x,lower=1-0.8\n",
      "*\n",
      "s,upper=1+0.8\n",
      "*\n",
      "s)\n",
      "x=tf.image.random_hue(x,max_delta=0.2\n",
      "*\n",
      "s)\n",
      "x=tf.clip_by_value(x,0,1)\n",
      "returnx\n",
      "defcolor_drop(x):\n",
      "image=tf.image.rgb_to_grayscale(image)\n",
      "image=tf.tile(image,[1,1,3])\n",
      "#randomlyapplytransformationwithprobabilityp.\n",
      "image=random_apply(color_jitter,image,p=0.8)\n",
      "image=random_apply(color_drop,image,p=0.2)\n",
      "returnimage\n",
      "Apseudo-codeforcolordistortionusingPytorchisasfollows\n",
      "12\n",
      ".\n",
      "fromtorchvisionimporttransforms\n",
      "defget_color_distortion(s=1.0):\n",
      "#sisthestrengthofcolordistortion.\n",
      "color_jitter=transforms.ColorJitter(0.8\n",
      "*\n",
      "s,0.8\n",
      "*\n",
      "s,0.8\n",
      "*\n",
      "s,0.2\n",
      "*\n",
      "s)\n",
      "rnd_color_jitter=transforms.RandomApply([color_jitter],p=0.8)\n",
      "rnd_gray=transforms.RandomGrayscale(p=0.2)\n",
      "color_distort=transforms.Compose([\n",
      "rnd_color_jitter,\n",
      "rnd_gray])\n",
      "12\n",
      "OurcodeandresultsarebasedonTw,thePytorchcodehereisareference.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "returncolor_distort\n",
      "Gaussianblur\n",
      "Thisaugmentationisinourdefaultpolicy.Weithelpful,asitimprovesourResNet-50trainedfor\n",
      "100epochsfrom63.2%to64.5%.Weblurtheimage50%ofthetimeusingaGaussiankernel.Werandomlysample\n",
      "˙\n",
      "2\n",
      "[0\n",
      ":\n",
      "1\n",
      ";\n",
      "2\n",
      ":\n",
      "0]\n",
      ",andthekernelsizeissettobe10%oftheimageheight/width.\n",
      "B.AdditionalExperimentalResults\n",
      "B.1.BatchSizeandTrainingSteps\n",
      "Figure\n",
      "B.1\n",
      "showsthetop-5accuracyonlinearevaluationwhentrainedwithdifferentbatchsizesandtrainingepochs.The\n",
      "conclusionisverysimilartotop-1accuracyshownbefore,exceptthatthedifferencesbetweendifferentbatchsizesand\n",
      "trainingstepsseemsslightlysmallerhere.\n",
      "InbothFigure\n",
      "9\n",
      "andFigure\n",
      "B.1\n",
      ",weusealinearscalingoflearningratesimilarto(\n",
      "Goyaletal.\n",
      ",\n",
      "2017\n",
      ")whentraining\n",
      "withdifferentbatchsizes.AlthoughlinearlearningratescalingispopularwithSGD/Momentumoptimizer,wea\n",
      "squarerootlearningratescalingismoredesirablewithLARSoptimizer.Withsquarerootlearningratescaling,wehave\n",
      "LearningRate=0\n",
      ":\n",
      "075\n",
      "\n",
      "p\n",
      "BatchSize\n",
      ",insteadof\n",
      "LearningRate=0\n",
      ":\n",
      "3\n",
      "\n",
      "BatchSize\n",
      "=\n",
      "256\n",
      "inthelinearscalingcase,but\n",
      "thelearningrateisthesameunderbothscalingmethodswhenbatchsizeof4096(ourdefaultbatchsize).Acomparisonis\n",
      "presentedinTable\n",
      "B.1\n",
      ",whereweobservethatsquarerootlearningratescalingimprovestheperformanceformodelstrained\n",
      "withsmallbatchsizesandinsmallernumberofepochs.\n",
      "Batchsize\\Epochs\n",
      "100200400800\n",
      "256\n",
      "57.5/\n",
      "62.8\n",
      "61.9/\n",
      "64.3\n",
      "64.7/\n",
      "65.7\n",
      "66.6/66.5\n",
      "512\n",
      "60.7/\n",
      "63.8\n",
      "64.0/\n",
      "65.6\n",
      "66.2/66.767.8/67.4\n",
      "1024\n",
      "62.8/\n",
      "64.3\n",
      "65.3/\n",
      "66.1\n",
      "67.2/67.268.5/68.3\n",
      "2048\n",
      "64.0/\n",
      "64.7\n",
      "66.1/\n",
      "66.8\n",
      "68.1/67.968.9/68.8\n",
      "4096\n",
      "64.6/64.566.5/66.868.2/68.068.9/69.1\n",
      "8192\n",
      "64.8/64.866.6/67.067.8/68.369.0/69.1\n",
      "TableB.1.\n",
      "Linearevaluation(top-1)underdifferentbatchsizesandtrainingepochs.Ontheleftsideofslashsignaremodelstrainedwith\n",
      "linearLRscaling,andontherightaremodelstrainedwithsquarerootLRscaling.Theresultisboldedifitismorethan0.5%better.\n",
      "SquarerootLRscalingworksbetterforsmallerbatchsizetrainedinfewerepochs(withLARSoptimizer).\n",
      "Wealsotrainwithlargerbatchsize(upto32K)andlonger(upto3200epochs),withthesquarerootlearningratescaling.A\n",
      "showninFigure\n",
      "B.2\n",
      ",\n",
      "theperformanceseemstosaturatewithabatchsizeof8192,whiletraininglongercanstill\n",
      "improvetheperformance\n",
      ".\n",
      "FigureB.1.\n",
      "Linearevaluation(top-5)ofResNet-50trainedwith\n",
      "differentbatchsizesandepochs.Eachbarisasinglerunfrom\n",
      "scratch.SeeFigure\n",
      "9\n",
      "fortop-1accuracy.\n",
      "FigureB.2.\n",
      "Linearevaluation(top-1)ofResNet-50trainedwith\n",
      "differentbatchsizesand\n",
      "longer\n",
      "epochs.Herea\n",
      "squareroot\n",
      "learn-\n",
      "ingrate,insteadofalinearone,isutilized.\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "B.2.Broadercompositionofdataaugmentationsfurtherimprovesperformance\n",
      "Ourbestresultsinthemaintext(Table\n",
      "6\n",
      "and\n",
      "7\n",
      ")canbefurtherimprovedwhenexpandingthedefaultaugmentationpolicyto\n",
      "includethefollowing:(1)Sobel(2)additionalcolordistortion(equalize,solarize),and(3)motionblur.Forlinear\n",
      "evaluationprotocol,theResNet-50models(\n",
      "1\n",
      "\n",
      ";\n",
      "2\n",
      "\n",
      ";\n",
      "4\n",
      "\n",
      ")trainedwithbroaderdataaugmentationsachieve70.0(+0.7),74.4\n",
      "(+0.2),76.8(+0.3),respectively.\n",
      "Table\n",
      "B.2\n",
      "showsImageNetaccuracyobtainedbytheSimCLRmodel(seeAppendix\n",
      "B.5\n",
      "forthedetailsof\n",
      "procedure).Interestingly,whenonfull(100%)ImageNettrainingset,ourResNet(4\n",
      "\n",
      ")model\n",
      "achieves80.4%top-1/95.4%top-5\n",
      "13\n",
      ",whichisbetterthanthat(78.4%top-1/94.2%top-5)oftrainingfrom\n",
      "scratchusingthesamesetofaugmentations(i.e.randomcropandhorizontalForResNet-50(2\n",
      "\n",
      "),our\n",
      "pre-trainedResNet-50(2\n",
      "\n",
      ")isalsobetterthantrainingfromscratch(77.8%top-1/93.9%top-5).Thereisnoimprovement\n",
      "fromforResNet-50.\n",
      "Architecture\n",
      "Labelfraction\n",
      "1%10%100%\n",
      "Top1Top5Top1Top5Top1Top5\n",
      "ResNet-5049.476.666.188.176.093.1\n",
      "ResNet-50(2\n",
      "\n",
      ")59.483.771.891.279.194.8\n",
      "ResNet-50(4\n",
      "\n",
      ")64.186.674.892.880.495.4\n",
      "TableB.2.\n",
      "accuracyobtainedbytheSimCLR(whichispretrainedwithbroaderdataaugmentations)on1%,\n",
      "10%andfullofImageNet.Asareference,ourResNet-50(4\n",
      "\n",
      ")trainedfromscratchon100%labelsachieves78.4%top-1/94.2%top-5.\n",
      "B.3.EffectsofLongerTrainingforSupervisedModels\n",
      "Hereweperformexperimentstoseehowtrainingstepsandstrongerdataaugmentationaffectsupervisedtraining.Wetest\n",
      "ResNet-50andResNet-50(4\n",
      "\n",
      ")underthesamesetofdataaugmentations(randomcrops,colordistortion,50%Gaussian\n",
      "blur)asusedinourunsupervisedmodels.Figure\n",
      "B.3\n",
      "showsthetop-1accuracy.Weobservethatthereisno\n",
      "fromtrainingsupervisedmodelslongeronImageNet.Strongerdataaugmentationslightlyimprovestheaccuracyof\n",
      "ResNet-50(4\n",
      "\n",
      ")butdoesnothelponResNet-50.Whenstrongerdataaugmentationisapplied,ResNet-50generallyrequires\n",
      "longertraining(e.g.500epochs\n",
      "14\n",
      ")toobtaintheoptimalresult,whileResNet-50(4\n",
      "\n",
      ")doesnotfromlongertraining.\n",
      "ModelTrainingepochs\n",
      "Top1\n",
      "Crop+Color+Color+Blur\n",
      "ResNet-50\n",
      "9076.575.675.3\n",
      "50076.276.576.7\n",
      "100075.875.276.4\n",
      "ResNet-50(4\n",
      "\n",
      ")\n",
      "9078.478.978.7\n",
      "50078.378.478.5\n",
      "100077.978.278.3\n",
      "TableB.3.\n",
      "Top-1accuracyofsupervisedmodelstrainedlongerundervariousdataaugmentationprocedures(fromthesamesetofdata\n",
      "augmentationsforcontrastivelearning).\n",
      "B.4.UnderstandingTheNon-LinearProjectionHead\n",
      "Figure\n",
      "B.3\n",
      "showstheeigenvaluedistributionoflinearprojectionmatrix\n",
      "W\n",
      "2\n",
      "R\n",
      "2048\n",
      "\n",
      "2048\n",
      "usedtocompute\n",
      "z\n",
      "=\n",
      "W\n",
      "h\n",
      ".This\n",
      "matrixhasrelativelyfewlargeeigenvalues,indicatingthatitisapproximatelylow-rank.\n",
      "Figure\n",
      "B.4\n",
      "showst-SNE(\n",
      "Maaten&Hinton\n",
      ",\n",
      "2008\n",
      ")visualizationsof\n",
      "h\n",
      "and\n",
      "z\n",
      "=\n",
      "g\n",
      "(\n",
      "h\n",
      ")\n",
      "forrandomlyselected10classesby\n",
      "ourbestResNet-50(top-1linearevaluation69.3%).Classesrepresentedby\n",
      "h\n",
      "arebetterseparatedcomparedto\n",
      "z\n",
      ".\n",
      "13\n",
      "Itis80.1%top-1/95.2%top-5withoutbroaderaugmentationsforpretrainingSimCLR.\n",
      "14\n",
      "WithAutoAugment(\n",
      "Cubuketal.\n",
      ",\n",
      "2019\n",
      "),optimaltestaccuracycanbeachievedbetween900and500epochs.\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "(a)Y-axisinuniformscale.\n",
      "(b)Y-axisinlogscale.\n",
      "FigureB.3.\n",
      "Squaredrealeigenvaluedistributionoflinearprojection\n",
      "matrix\n",
      "W\n",
      "2\n",
      "R\n",
      "2048\n",
      "\n",
      "2048\n",
      "usedtocompute\n",
      "g\n",
      "(\n",
      "h\n",
      ")=\n",
      "W\n",
      "h\n",
      ".\n",
      "(a)\n",
      "h\n",
      "(b)\n",
      "z\n",
      "=\n",
      "g\n",
      "(\n",
      "h\n",
      ")\n",
      "FigureB.4.\n",
      "t-SNEvisualizationsofhiddenvectorsofimagesfrom\n",
      "arandomlyselected10classesinthevalidationset.\n",
      "B.5.Semi-supervisedLearningviaFine-Tuning\n",
      "Fine-tuningProcedure\n",
      "WeusingtheNesterovmomentumoptimizerwithabatchsizeof4096,momentumof\n",
      "0.9,andalearningrateof0.8(following\n",
      "LearningRate=0\n",
      ":\n",
      "05\n",
      "\n",
      "BatchSize\n",
      "=\n",
      "256\n",
      ")withoutwarmup.Onlyrandomcropping\n",
      "(withrandomleft-to-rightandresizingto224x224)isusedforpreprocessing.Wedonotuseanyregularization\n",
      "(includingweightdecay).For1%labeleddatawefor60epochs,andfor10%labeleddatawefor30\n",
      "epochs.Fortheinference,weresizethegivenimageto256x256,andtakeasinglecentercropof224x224.\n",
      "Table\n",
      "B.4\n",
      "showsthecomparisonsoftop-1accuracyfordifferentmethodsforsemi-supervisedlearning.Ourmodels\n",
      "improvestate-of-the-art.\n",
      "MethodArchitecture\n",
      "Labelfraction\n",
      "1%10%\n",
      "Top1\n",
      "SupervisedbaselineResNet-5025.456.4\n",
      "Methodsusinglabel-propagation:\n",
      "UDA(w.RandAug)ResNet-50-68.8\n",
      "FixMatch(w.RandAug)ResNet-50-71.5\n",
      "S4L(Rot+VAT+Ent.Min.)ResNet-50(4\n",
      "\n",
      ")-73.2\n",
      "Methodsusingself-supervisedrepresentationlearningonly:\n",
      "CPCv2ResNet-161(\n",
      "\n",
      ")52.773.1\n",
      "SimCLR(ours)ResNet-5048.365.6\n",
      "SimCLR(ours)ResNet-50(\n",
      "2\n",
      "\n",
      ")58.571.7\n",
      "SimCLR(ours)ResNet-50(\n",
      "4\n",
      "\n",
      ")\n",
      "63.074.4\n",
      "TableB.4.\n",
      "ImageNettop-1accuracyofmodelstrainedwithfewlabels.SeeTable\n",
      "7\n",
      "fortop-5accuracy.\n",
      "B.6.LinearEvaluation\n",
      "Forlinearevaluation,wefollowsimilarprocedureas(describedinAppendix\n",
      "B.5\n",
      "),exceptthatalargerlearning\n",
      "rateof1.6(following\n",
      "LearningRate=0\n",
      ":\n",
      "1\n",
      "\n",
      "BatchSize\n",
      "=\n",
      "256\n",
      ")andlongertrainingof90epochs.Alternatively,usingLARS\n",
      "optimizerwiththepretraininghyper-parametersalsoyieldsimilarresults.Furthermore,wethatattachingthelinear\n",
      "ontopofthebaseencoder(witha\n",
      "stop\n",
      "_\n",
      "gradient\n",
      "ontheinputtolineartopreventthelabelinformation\n",
      "fromtheencoder)andtrainthemsimultaneouslyduringthepretrainingachievessimilarperformance.\n",
      "B.7.CorrelationBetweenLinearEvaluationandFine-Tuning\n",
      "Herewestudythecorrelationbetweenlinearevaluationandunderdifferentsettingsoftrainingstepandnetwork\n",
      "architecture.\n",
      "Figure\n",
      "B.5\n",
      "showslinearevaluationversuswhentrainingepochsofaResNet-50(usingbatchsizeof4096)are\n",
      "variedfrom50to3200asinFigure\n",
      "B.2\n",
      ".Whiletheyarealmostlinearlycorrelated,itseemsonasmallfraction\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "oflabelsmorefromtraininglonger.\n",
      "FigureB.5.\n",
      "Top-1accuracyofmodelstrainedindifferentepochs(fromFigure\n",
      "B.2\n",
      "),underlinearevaluationand\n",
      "Figure\n",
      "B.6\n",
      "showsshowslinearevaluationversusfordifferentarchitecturesofchoice.\n",
      "FigureB.6.\n",
      "Top-1accuracyofdifferentarchitecturesunderlinearevaluationand\n",
      "B.8.TransferLearning\n",
      "Weevaluatedtheperformanceofourself-supervisedrepresentationfortransferlearningintwosettings:linearevaluation,\n",
      "wherealogisticregressionistrainedtoclassifyanewdatasetbasedontheself-supervisedrepresentationlearned\n",
      "onImageNet,andwhereweallowallweightstovaryduringtraining.Inbothcases,wefollowtheapproach\n",
      "describedby\n",
      "Kornblithetal.\n",
      "(\n",
      "2019\n",
      "),althoughourpreprocessingdiffersslightly.\n",
      "B.8.1.M\n",
      "ETHODS\n",
      "Datasets\n",
      "WeinvestigatedtransferlearningperformanceontheFood-101dataset(\n",
      "Bossardetal.\n",
      ",\n",
      "2014\n",
      "),CIFAR-10\n",
      "andCIFAR-100(\n",
      "Krizhevsky&Hinton\n",
      ",\n",
      "2009\n",
      "),Birdsnap(\n",
      "Bergetal.\n",
      ",\n",
      "2014\n",
      "),theSUN397scenedataset(\n",
      "Xiaoetal.\n",
      ",\n",
      "2010\n",
      "),StanfordCars(\n",
      "Krauseetal.\n",
      ",\n",
      "2013\n",
      "),FGVCAircraft(\n",
      "Majietal.\n",
      ",\n",
      "2013\n",
      "),thePASCALVOC2007\n",
      "task(\n",
      "Everinghametal.\n",
      ",\n",
      "2010\n",
      "),theDescribableTexturesDataset(DTD)(\n",
      "Cimpoietal.\n",
      ",\n",
      "2014\n",
      "),Oxford-IIITPets(\n",
      "Parkhietal.\n",
      ",\n",
      "2012\n",
      "),Caltech-101(\n",
      "Fei-Feietal.\n",
      ",\n",
      "2004\n",
      "),andOxford102Flowers(\n",
      "Nilsback&Zisserman\n",
      ",\n",
      "2008\n",
      ").Wefollowtheevaluation\n",
      "protocolsinthepapersintroducingthesedatasets,\n",
      "i.e.\n",
      ",wereporttop-1accuracyforFood-101,CIFAR-10,CIFAR-100,\n",
      "Birdsnap,SUN397,StanfordCars,andDTD;meanper-classaccuracyforFGVCAircraft,Oxford-IIITPets,Caltech-101,\n",
      "andOxford102Flowers;andthe11-pointmAPmetricasin\n",
      "Everinghametal.\n",
      "(\n",
      "2010\n",
      ")forPASCALVOC2007.For\n",
      "DTDandSUN397,thedatasetcreatorsmultipletrain/testsplits;wereportresultsonlyforthesplit.Caltech-101\n",
      "notrain/testsplit,sowerandomlychose30imagesperclassandtestontheremainder,forfaircomparisonwith\n",
      "previouswork(\n",
      "Donahueetal.\n",
      ",\n",
      "2014\n",
      ";\n",
      "Simonyan&Zisserman\n",
      ",\n",
      "2014\n",
      ").\n",
      "WeusedthevalidationsetsbythedatasetcreatorstoselecthyperparametersforFGVCAircraft,PASCALVOC\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "2007,DTD,andOxford102Flowers.Forotherdatasets,weheldoutasubsetofthetrainingsetforvalidationwhile\n",
      "performinghyperparametertuning.Afterselectingtheoptimalhyperparametersonthevalidationset,weretrainedthe\n",
      "modelusingtheselectedparametersusingalltrainingandvalidationimages.Wereportaccuracyonthetestset.\n",
      "TransferLearningviaaLinear\n",
      "Wetrainedan\n",
      "`\n",
      "2\n",
      "-regularizedmultinomiallogisticregressionon\n",
      "featuresextractedfromthefrozenpretrainednetwork.WeusedL-BFGStooptimizethesoftmaxcross-entropyobjective\n",
      "andwedidnotapplydataaugmentation.Aspreprocessing,allimageswereresizedto224pixelsalongtheshorterside\n",
      "usingbicubicresampling,afterwhichwetooka\n",
      "224\n",
      "\n",
      "224\n",
      "centercrop.Weselectedthe\n",
      "`\n",
      "2\n",
      "regularizationparameterfroma\n",
      "rangeof45logarithmicallyspacedvaluesbetween\n",
      "10\n",
      "\n",
      "6\n",
      "and\n",
      "10\n",
      "5\n",
      ".\n",
      "TransferLearningviaFine-Tuning\n",
      "Wetheentirenetworkusingtheweightsofthepretrainednetworkas\n",
      "initialization.Wetrainedfor20,000stepsatabatchsizeof256usingSGDwithNesterovmomentumwithamomentum\n",
      "parameterof0.9.Wesetthemomentumparameterforthebatchnormalizationstatisticsto\n",
      "max(1\n",
      "\n",
      "10\n",
      "=s;\n",
      "0\n",
      ":\n",
      "9)\n",
      "where\n",
      "s\n",
      "is\n",
      "thenumberofstepsperepoch.Asdataaugmentationduringweperformedonlyrandomcropswithresizeand\n",
      "incontrasttopretraining,wedidnotperformcoloraugmentationorblurring.Attesttime,weresizedimagesto256\n",
      "pixelsalongtheshortersideandtooka\n",
      "224\n",
      "\n",
      "224\n",
      "centercrop.(Additionalaccuracyimprovementsmaybepossiblewith\n",
      "furtheroptimizationofdataaugmentation,particularlyontheCIFAR-10andCIFAR-100datasets.)Weselectedthelearning\n",
      "rateandweightdecay,withagridof7logarithmicallyspacedlearningratesbetween0.0001and0.1and7logarithmically\n",
      "spacedvaluesofweightdecaybetween\n",
      "10\n",
      "\n",
      "6\n",
      "and\n",
      "10\n",
      "\n",
      "3\n",
      ",aswellasnoweightdecay.Wedividethesevaluesofweightdecay\n",
      "bythelearningrate.\n",
      "TrainingfromRandomInitialization\n",
      "Wetrainedthenetworkfromrandominitializationusingthesameprocedure\n",
      "asforbutforlonger,andwithanalteredhyperparametergrid.Wechosehyperparametersfromagridof7\n",
      "logarithmicallyspacedlearningratesbetween0.001and1.0and8logarithmicallyspacedvaluesofweightdecaybetween\n",
      "10\n",
      "\n",
      "5\n",
      "and\n",
      "10\n",
      "\n",
      "1\n",
      ":\n",
      "5\n",
      ".Importantly,ourrandominitializationbaselinesaretrainedfor40,000steps,whichissuflongto\n",
      "achievenear-maximalaccuracy,asdemonstratedinFigure8of\n",
      "Kornblithetal.\n",
      "(\n",
      "2019\n",
      ").\n",
      "OnBirdsnap,therearenostatisticallydifferencesamongmethods,andonFood-101,StanfordCars,andFGVC\n",
      "Aircraftdatasets,providesonlyasmalladvantageovertrainingfromrandominitialization.However,onthe\n",
      "remaining8datasets,pretraininghasclearadvantages.\n",
      "SupervisedBaselines\n",
      "WecompareagainstarchitecturallyidenticalResNetmodelstrainedonImageNetwithstandard\n",
      "cross-entropyloss.Thesemodelsaretrainedwiththesamedataaugmentationasourself-supervisedmodels(crops,strong\n",
      "coloraugmentation,andblur)andarealsotrainedfor1000epochs.Wefoundthat,althoughstrongerdataaugmentationand\n",
      "longertrainingtimedonotaccuracyonImageNet,thesemodelsperformedbetterthanasupervised\n",
      "baselinetrainedfor90epochsandordinarydataaugmentationforlinearevaluationonasubsetoftransferdatasets.The\n",
      "supervisedResNet-50baselineachieves76.3%top-1accuracyonImageNet,vs.69.3%fortheself-supervisedcounterpart,\n",
      "whiletheResNet-50(\n",
      "4\n",
      "\n",
      ")baselineachieves78.3%,vs.76.5%fortheself-supervisedmodel.\n",
      "StatisticalTesting\n",
      "Wetestfortheofdifferencesbetweenmodelwithapermutationtest.Given\n",
      "predictionsoftwomodels,wegenerate100,000samplesfromthenulldistributionbyrandomlyexchangingpredictions\n",
      "foreachexampleandcomputingthedifferenceinaccuracyafterperformingthisrandomization.Wethencomputethe\n",
      "percentageofsamplesfromthenulldistributionthataremoreextremethantheobserveddifferenceinpredictions.Fortop-1\n",
      "accuracy,thisprocedureyieldsthesameresultastheexactMcNemartest.Theassumptionofexchangeabilityunderthenull\n",
      "hypothesisisalsovalidformeanper-classaccuracy,butnotwhencomputingaverageprecisioncurves.Thus,weperform\n",
      "testingforadifferenceinaccuracyonVOC2007ratherthanadifferenceinmAP.Acaveatofthisprocedureis\n",
      "thatitdoesnotconsiderrun-to-runvariabilitywhentrainingthemodels,onlyvariabilityarisingfromusingasample\n",
      "ofimagesforevaluation.\n",
      "B.8.2.R\n",
      "ESULTSWITH\n",
      "S\n",
      "TANDARD\n",
      "R\n",
      "ES\n",
      "N\n",
      "ET\n",
      "TheResNet-50(\n",
      "4\n",
      "\n",
      ")resultsshowninTable\n",
      "8\n",
      "ofthetextshownoclearadvantagetothesupervisedorself-supervisedmodels.\n",
      "WiththenarrowerResNet-50architecture,however,supervisedlearningmaintainsaclearadvantageoverself-supervised\n",
      "learning.ThesupervisedResNet-50modeloutperformstheself-supervisedmodelonalldatasetswithlinearevaluation,\n",
      "andmost(10of12)datasetswithTheweakerperformanceoftheResNetmodelcomparedtotheResNet(\n",
      "4\n",
      "\n",
      ")\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "FoodCIFAR10CIFAR100BirdsnapSUN397CarsAircraftVOC2007DTDPetsCaltech-101Flowers\n",
      "Linearevaluation:\n",
      "SimCLR(ours)68.490.671.637.458.850.350.380.5\n",
      "74.5\n",
      "83.690.391.2\n",
      "Supervised\n",
      "72.393.678.353.761.966.761.082.874.991.594.594.7\n",
      "Fine-tuned:\n",
      "SimCLR(ours)\n",
      "88.297.785.975.9\n",
      "63.591.3\n",
      "88.1\n",
      "84.1\n",
      "73.2\n",
      "89.292.197.0\n",
      "Supervised\n",
      "88.397.586.475.864.392.1\n",
      "86.0\n",
      "85.074.692.193.397.6\n",
      "Randominit86.995.980.2\n",
      "76.1\n",
      "53.691.485.967.364.881.572.692.0\n",
      "TableB.5.\n",
      "Comparisonoftransferlearningperformanceofourself-supervisedapproachwithsupervisedbaselinesacross12natural\n",
      "imagedatasets,usingImageNet-pretrainedResNetmodels.SeealsoFigure\n",
      "8\n",
      "forresultswiththeResNet(\n",
      "4\n",
      "\n",
      ")architecture.\n",
      "modelmayrelatetotheaccuracygapbetweenthesupervisedandself-supervisedmodelsonImageNet.Theself-supervised\n",
      "ResNetgets69.3%top-1accuracy,6.8%worsethanthesupervisedmodelinabsoluteterms,whereastheself-supervised\n",
      "ResNet\n",
      "(4\n",
      "\n",
      ")\n",
      "modelgets76.5%,whichisonly1.8%worsethanthesupervisedmodel.\n",
      "B.9.CIFAR-10\n",
      "WhilewefocusonusingImageNetasthemaindatasetforpretrainingourunsupervisedmodel,ourmethodalsoworkswith\n",
      "otherdatasets.WedemonstrateitbytestingonCIFAR-10asfollows.\n",
      "Setup\n",
      "AsourgoalisnottooptimizeCIFAR-10performance,butrathertoprovidefurtherofourobservations\n",
      "onImageNet,weusethesamearchitecture(ResNet-50)forCIFAR-10experiments.BecauseCIFAR-10imagesaremuch\n",
      "smallerthanImageNetimages,wereplacethe7x7Convofstride2with3x3Convofstride1,andalsoremovethe\n",
      "maxpoolingoperation.Fordataaugmentation,weusethesameInceptioncropandresizeto32x32)asImageNet,\n",
      "15\n",
      "and\n",
      "colordistortion(strength=0.5),leavingoutGaussianblur.Wepretrainwithlearningratein\n",
      "f\n",
      "0\n",
      ":\n",
      "5\n",
      ";\n",
      "1\n",
      ":\n",
      "0\n",
      ";\n",
      "1\n",
      ":\n",
      "5\n",
      "g\n",
      ",temperaturein\n",
      "f\n",
      "0\n",
      ":\n",
      "1\n",
      ";\n",
      "0\n",
      ":\n",
      "5\n",
      ";\n",
      "1\n",
      ":\n",
      "0\n",
      "g\n",
      ",andbatchsizein\n",
      "f\n",
      "256\n",
      ";\n",
      "512\n",
      ";\n",
      "1024\n",
      ";\n",
      "2048\n",
      ";\n",
      "4096\n",
      "g\n",
      ".Therestofthesettings(includingoptimizer,weightdecay,\n",
      "etc.)arethesameasourImageNettraining.\n",
      "Ourbestmodeltrainedwithbatchsize1024canachievealinearevaluationaccuracyof94.0%,comparedto95.1%fromthe\n",
      "supervisedbaselineusingthesamearchitectureandbatchsize.Thebestself-supervisedmodelthatreportslinearevaluation\n",
      "resultonCIFAR-10isAMDIM(\n",
      "Bachmanetal.\n",
      ",\n",
      "2019\n",
      "),whichachieves91.2%withamodel\n",
      "25\n",
      "\n",
      "largerthanours.Wenote\n",
      "thatourmodelcanbeimprovedbyincorporatingextradataaugmentationsaswellasusingamoresuitablebasenetwork.\n",
      "Performanceunderdifferentbatchsizesandtrainingsteps\n",
      "Figure\n",
      "B.7\n",
      "showsthelinearevaluationperformanceunder\n",
      "differentbatchsizesandtrainingsteps.TheresultsareconsistentwithourobservationsonImageNet,althoughthelargest\n",
      "batchsizeof4096seemstocauseasmalldegradationinperformanceonCIFAR-10.\n",
      "FigureB.7.\n",
      "LinearevaluationofResNet-50(withad-\n",
      "justedstem)trainedwithdifferentbatchsizeand\n",
      "epochsonCIFAR-10dataset.Eachbarisaveraged\n",
      "over3runswithdifferentlearningrates(0.5,1.0,\n",
      "1.5)andtemperature\n",
      "˝\n",
      "=0\n",
      ":\n",
      "5\n",
      ".Errorbardenotes\n",
      "standarddeviation.\n",
      "15\n",
      "Itisworthnotingthat,althoughCIFAR-10imagesaremuchsmallerthanImageNetimagesandimagesizedoesnotdifferamong\n",
      "examples,croppingwithresizingisstillaveryeffectiveaugmentationforcontrastivelearning.\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "Optimaltemperatureunderdifferentbatchsizes\n",
      "Figure\n",
      "B.8\n",
      "showsthelinearevaluationofmodeltrainedwiththree\n",
      "differenttemperaturesundervariousbatchsizes.Wethatwhentrainingtoconvergence(e.g.trainingepochs>300),the\n",
      "optimaltemperaturein\n",
      "f\n",
      "0\n",
      ":\n",
      "1\n",
      ";\n",
      "0\n",
      ":\n",
      "5\n",
      ";\n",
      "1\n",
      ":\n",
      "0\n",
      "g\n",
      "is0.5andseemsconsistentregardlessofthebatchsizes.However,theperformance\n",
      "with\n",
      "˝\n",
      "=0\n",
      ":\n",
      "1\n",
      "improvesasbatchsizeincreases,whichmaysuggestasmallshiftofoptimaltemperaturetowards0.1.\n",
      "(a)Trainingepochs\n",
      "\n",
      "300\n",
      "(b)Trainingepochs\n",
      ">\n",
      "300\n",
      "FigureB.8.\n",
      "Linearevaluationofthemodel(ResNet-50)trainedwiththreetemperaturesondifferentbatchsizesonCIFAR-10.Eachbaris\n",
      "averagedovermultiplerunswithdifferentlearningratesandtotaltrainepochs.Errorbardenotesstandarddeviation.\n",
      "B.10.TuningForOtherLossFunctions\n",
      "ThelearningratethatworksbestforNT-Xentlossmaynotbeagoodlearningrateforotherlossfunctions.Toensurea\n",
      "faircomparison,wealsotunehyperparametersforbothmarginlossandlogisticloss.,wetunelearningrate\n",
      "in\n",
      "f\n",
      "0\n",
      ":\n",
      "01\n",
      ";\n",
      "0\n",
      ":\n",
      "1\n",
      ";\n",
      "0\n",
      ":\n",
      "3\n",
      ";\n",
      "0\n",
      ":\n",
      "5\n",
      ";\n",
      "1\n",
      ":\n",
      "0\n",
      "g\n",
      "forbothlossfunctions.Wefurthertunethemarginin\n",
      "f\n",
      "0\n",
      ";\n",
      "0\n",
      ":\n",
      "4\n",
      ";\n",
      "0\n",
      ":\n",
      "8\n",
      ";\n",
      "1\n",
      ":\n",
      "6\n",
      "g\n",
      "formarginloss,the\n",
      "temperaturein\n",
      "f\n",
      "0\n",
      ":\n",
      "1\n",
      ";\n",
      "0\n",
      ":\n",
      "2\n",
      ";\n",
      "0\n",
      ":\n",
      "5\n",
      ";\n",
      "1\n",
      ":\n",
      "0\n",
      "g\n",
      "forlogisticloss.Forsimplicity,weonlyconsiderthenegativesfromoneaugmentation\n",
      "view(insteadofbothsides),whichslightlyimpairsperformancebutensuresfaircomparison.\n",
      "C.FurtherComparisontoRelatedMethods\n",
      "Aswehavenotedinthemaintext,mostindividualcomponentsofSimCLRhaveappearedinpreviouswork,andthe\n",
      "improvedperformanceisaresultofacombinationofthesedesignchoices.Table\n",
      "C.1\n",
      "providesahigh-levelcomparisonof\n",
      "thedesignchoicesofourmethodwiththoseofpreviousmethods.Comparedwithpreviouswork,ourdesignchoicesare\n",
      "generallysimpler.\n",
      "ModelDataAugmentationBaseEncoderProjectionHeadLossBatchSizeTrainEpochs\n",
      "CPCv2CustomResNet-161(mPixelCNNXent512\n",
      "#\n",
      "˘\n",
      "200\n",
      "AMDIMFastAutoAug.CustomResNetNon-linearMLPXentw/clip,reg1008\n",
      "#\n",
      "150\n",
      "CMCFastAutoAug.ResNet-50(\n",
      "2\n",
      "\n",
      ",L+ab)LinearlayerXentw/\n",
      "`\n",
      "2\n",
      ";˝\n",
      "156\n",
      "\n",
      "280\n",
      "MoCoCrop+colorResNet-50(4\n",
      "\n",
      ")LinearlayerXentw/\n",
      "`\n",
      "2\n",
      ";˝\n",
      "256\n",
      "\n",
      "200\n",
      "PIRLCrop+colorResNet-50(2\n",
      "\n",
      ")LinearlayerXentw/\n",
      "`\n",
      "2\n",
      ";˝\n",
      "1024\n",
      "\n",
      "800\n",
      "SimCLRCrop+color+blurResNet-50(\n",
      "4\n",
      "\n",
      ")Non-linearMLPXentw/\n",
      "`\n",
      "2\n",
      ";˝\n",
      "40961000\n",
      "TableC.1.\n",
      "Ahigh-levelcomparisonofdesignchoicesandtrainingsetup(forbestresultonImageNet)foreachmethod.Notethat\n",
      "descriptionsprovidedherearegeneral;evenwhentheymatchfortwomethods,formulationsandimplementationsmaydiffer(e.g.for\n",
      "coloraugmentation).Refertotheoriginalpapersformoredetails.\n",
      "#\n",
      "Examplesaresplitintomultiplepatches,whichenlargestheeffective\n",
      "batchsize.\n",
      "\n",
      "Amemorybankisemployed.\n",
      "Inbelow,weprovideanin-depthcomparisonofourmethodtotherecentlyproposedcontrastiverepresentationlearning\n",
      "methods:\n",
      "\n",
      "DIM/AMDIM(\n",
      "Hjelmetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Bachmanetal.\n",
      ",\n",
      "2019\n",
      ")achieveglobal-to-local/local-to-neighborpredictionby\n",
      "predictingthemiddlelayerofConvNet.TheConvNetisaResNetthathasbewentoplace\n",
      "constraintsonthereceptiveofthenetwork(e.g.replacingmany3x3Convswith1x1Convs).Inourframework,\n",
      "wedecouplethepredictiontaskandencoderarchitecture,byrandomcropping(withresizing)andusingthe\n",
      "\n",
      "ASimpleFrameworkforContrastiveLearningofVisualRepresentations\n",
      "representationsoftwoaugmentedviewsforprediction,sowecanusestandardandmorepowerfulResNets.Our\n",
      "NT-Xentlossfunctionleveragesnormalizationandtemperaturetorestricttherangeofsimilarityscores,whereasthey\n",
      "useatanhfunctionwithregularization.Weuseasimplerdataaugmentationpolicy,whiletheyuseFastAutoAugment\n",
      "fortheirbestresult.\n",
      "\n",
      "CPCv1andv2(\n",
      "Oordetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Hénaffetal.\n",
      ",\n",
      "2019\n",
      ")thecontextpredictiontaskusingadeterministicstrategy\n",
      "tosplitexamplesintopatches,andacontextaggregationnetwork(aPixelCNN)toaggregatethesepatches.Thebase\n",
      "encodernetworkseesonlypatches,whichareconsiderablysmallerthantheoriginalimage.Wedecoupletheprediction\n",
      "taskandtheencoderarchitecture,sowedonotrequireacontextaggregationnetwork,andourencodercanlookatthe\n",
      "imagesofwiderspectrumofresolutions.Inaddition,weusetheNT-Xentlossfunction,whichleveragesnormalization\n",
      "andtemperature,whereastheyuseanunnormalizedcross-entropy-basedobjective.Weusesimplerdataaugmentation.\n",
      "\n",
      "InstDisc,MoCo,PIRL(\n",
      "Wuetal.\n",
      ",\n",
      "2018\n",
      ";\n",
      "Heetal.\n",
      ",\n",
      "2019\n",
      ";\n",
      "Misra&vanderMaaten\n",
      ",\n",
      "2019\n",
      ")generalizetheExemplar\n",
      "approachoriginallyproposedby\n",
      "Dosovitskiyetal.\n",
      "(\n",
      "2014\n",
      ")andleverageanexplicitmemorybank.Wedonotusea\n",
      "memorybank;wethat,withalargerbatchsize,in-batchnegativeexamplesamplingsufWealsoutilizea\n",
      "nonlinearprojectionhead,andusetherepresentationbeforetheprojectionhead.Althoughweusesimilartypesof\n",
      "augmentations(e.g.,randomcropandcolordistortion),weexpectparametersmaybedifferent.\n",
      "\n",
      "CMC(\n",
      "Tianetal.\n",
      ",\n",
      "2019\n",
      ")usesaseparatednetworkforeachview,whilewesimplyuseasinglenetworksharedforall\n",
      "randomlyaugmentedviews.Thedataaugmentation,projectionheadandlossfunctionarealsodifferent.Weuselarger\n",
      "batchsizeinsteadofamemorybank.\n",
      "\n",
      "Whereas\n",
      "Yeetal.\n",
      "(\n",
      "2019\n",
      ")maximizesimilaritybetweenaugmentedandunaugmentedcopiesofthesameimage,we\n",
      "applydataaugmentationsymmetricallytobothbranchesofourframework(Figure\n",
      "2\n",
      ").Wealsoapplyanonlinear\n",
      "projectionontheoutputofbasefeaturenetwork,andusetherepresentationbeforeprojectionnetwork,whereas\n",
      "Ye\n",
      "etal.\n",
      "(\n",
      "2019\n",
      ")usethelinearlyprojectedhiddenvectorastherepresentation.Whentrainingwithlargebatchsizes\n",
      "usingmultipleaccelerators,weuseglobalBNtoavoidshortcutsthatcangreatlydecreaserepresentationquality.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = PdfFileReader(pdf_name)\n",
    "\n",
    "for page in pdf.pages:\n",
    "    print(page.extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-tourist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "printable-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "text = textract.process(pdf_name, method='pdfminer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bacterial-liechtenstein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'A Simple Framework for Contrastive Learning of Visual Representations\\n\\nTing Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton 1\\n\\n0\\n2\\n0\\n2\\n\\n \\nl\\nu\\nJ\\n \\n\\n1\\n\\n \\n \\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n\\n3\\nv\\n9\\n0\\n7\\n5\\n0\\n\\n.\\n\\n2\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nThis paper presents SimCLR: a simple framework\\nfor contrastive learning of visual representations.\\nWe simplify recently proposed contrastive self-\\nsupervised learning algorithms without requiring\\nspecialized architectures or a memory bank. In\\norder to understand what enables the contrastive\\nprediction tasks to learn useful representations,\\nwe systematically study the major components of\\nour framework. We show that (1) composition of\\ndata augmentations plays a critical role in de\\xef\\xac\\x81ning\\neffective predictive tasks, (2) introducing a learn-\\nable nonlinear transformation between the repre-\\nsentation and the contrastive loss substantially im-\\nproves the quality of the learned representations,\\nand (3) contrastive learning bene\\xef\\xac\\x81ts from larger\\nbatch sizes and more training steps compared to\\nsupervised learning. By combining these \\xef\\xac\\x81ndings,\\nwe are able to considerably outperform previous\\nmethods for self-supervised and semi-supervised\\nlearning on ImageNet. A linear classi\\xef\\xac\\x81er trained\\non self-supervised representations learned by Sim-\\nCLR achieves 76.5% top-1 accuracy, which is a\\n7% relative improvement over previous state-of-\\nthe-art, matching the performance of a supervised\\nResNet-50. When \\xef\\xac\\x81ne-tuned on only 1% of the\\nlabels, we achieve 85.8% top-5 accuracy, outper-\\nforming AlexNet with 100\\xc3\\x97 fewer labels. 1\\n\\n1. Introduction\\nLearning effective visual representations without human\\nsupervision is a long-standing problem. Most mainstream\\napproaches fall into one of two classes: generative or dis-\\ncriminative. Generative approaches learn to generate or\\notherwise model pixels in the input space (Hinton et al.,\\n2006; Kingma & Welling, 2013; Goodfellow et al., 2014).\\n\\n1Google Research, Brain Team. Correspondence to: Ting Chen\\n\\n<iamtingchen@google.com>.\\n\\nProceedings of the 37 th International Conference on Machine\\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\\nthe author(s).\\n\\n1Code available at https://github.com/google-research/simclr.\\n\\nFigure 1. ImageNet Top-1 accuracy of linear classi\\xef\\xac\\x81ers trained\\non representations learned with different self-supervised meth-\\nods (pretrained on ImageNet). Gray cross indicates supervised\\nResNet-50. Our method, SimCLR, is shown in bold.\\n\\nHowever, pixel-level generation is computationally expen-\\nsive and may not be necessary for representation learning.\\nDiscriminative approaches learn representations using objec-\\ntive functions similar to those used for supervised learning,\\nbut train networks to perform pretext tasks where both the in-\\nputs and labels are derived from an unlabeled dataset. Many\\nsuch approaches have relied on heuristics to design pretext\\ntasks (Doersch et al., 2015; Zhang et al., 2016; Noroozi &\\nFavaro, 2016; Gidaris et al., 2018), which could limit the\\ngenerality of the learned representations. Discriminative\\napproaches based on contrastive learning in the latent space\\nhave recently shown great promise, achieving state-of-the-\\nart results (Hadsell et al., 2006; Dosovitskiy et al., 2014;\\nOord et al., 2018; Bachman et al., 2019).\\nIn this work, we introduce a simple framework for con-\\ntrastive learning of visual representations, which we call\\nSimCLR. Not only does SimCLR outperform previous work\\n(Figure 1), but it is also simpler, requiring neither special-\\nized architectures (Bachman et al., 2019; H\\xc3\\xa9naff et al., 2019)\\nnor a memory bank (Wu et al., 2018; Tian et al., 2019; He\\net al., 2019; Misra & van der Maaten, 2019).\\nIn order to understand what enables good contrastive repre-\\nsentation learning, we systematically study the major com-\\nponents of our framework and show that:\\n\\n2550100200400626Number of Parameters (Millions)5560657075ImageNet Top-1 Accuracy (%)InstDiscRotationBigBiGANLACPCv2CPCv2-LCMCAMDIMMoCoMoCo (2x)MoCo (4x)PIRLPIRL-ens.PIRL-c2xSimCLRSimCLR (2x)SimCLR (4x)Supervised\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\n\\xe2\\x80\\xa2 Composition of multiple data augmentation operations\\nis crucial in de\\xef\\xac\\x81ning the contrastive prediction tasks that\\nyield effective representations. In addition, unsupervised\\ncontrastive learning bene\\xef\\xac\\x81ts from stronger data augmen-\\ntation than supervised learning.\\n\\n\\xe2\\x80\\xa2 Introducing a learnable nonlinear transformation be-\\ntween the representation and the contrastive loss substan-\\ntially improves the quality of the learned representations.\\n\\xe2\\x80\\xa2 Representation learning with contrastive cross entropy\\nloss bene\\xef\\xac\\x81ts from normalized embeddings and an appro-\\npriately adjusted temperature parameter.\\n\\n\\xe2\\x80\\xa2 Contrastive learning bene\\xef\\xac\\x81ts from larger batch sizes and\\nlonger training compared to its supervised counterpart.\\nLike supervised learning, contrastive learning bene\\xef\\xac\\x81ts\\nfrom deeper and wider networks.\\n\\nWe combine these \\xef\\xac\\x81ndings to achieve a new state-of-the-art\\nin self-supervised and semi-supervised learning on Ima-\\ngeNet ILSVRC-2012 (Russakovsky et al., 2015). Under the\\nlinear evaluation protocol, SimCLR achieves 76.5% top-1\\naccuracy, which is a 7% relative improvement over previous\\nstate-of-the-art (H\\xc3\\xa9naff et al., 2019). When \\xef\\xac\\x81ne-tuned with\\nonly 1% of the ImageNet labels, SimCLR achieves 85.8%\\ntop-5 accuracy, a relative improvement of 10% (H\\xc3\\xa9naff et al.,\\n2019). When \\xef\\xac\\x81ne-tuned on other natural image classi\\xef\\xac\\x81ca-\\ntion datasets, SimCLR performs on par with or better than\\na strong supervised baseline (Kornblith et al., 2019) on 10\\nout of 12 datasets.\\n\\n2. Method\\n2.1. The Contrastive Learning Framework\\n\\nInspired by recent contrastive learning algorithms (see Sec-\\ntion 7 for an overview), SimCLR learns representations\\nby maximizing agreement between differently augmented\\nviews of the same data example via a contrastive loss in\\nthe latent space. As illustrated in Figure 2, this framework\\ncomprises the following four major components.\\n\\xe2\\x80\\xa2 A stochastic data augmentation module that transforms\\nany given data example randomly resulting in two cor-\\nrelated views of the same example, denoted \\xcb\\x9cxi and \\xcb\\x9cxj,\\nwhich we consider as a positive pair. In this work, we\\nsequentially apply three simple augmentations: random\\ncropping followed by resize back to the original size, ran-\\ndom color distortions, and random Gaussian blur. As\\nshown in Section 3, the combination of random crop and\\ncolor distortion is crucial to achieve a good performance.\\n\\xe2\\x80\\xa2 A neural network base encoder f (\\xc2\\xb7) that extracts repre-\\nsentation vectors from augmented data examples. Our\\nframework allows various choices of the network archi-\\ntecture without any constraints. We opt for simplicity\\nand adopt the commonly used ResNet (He et al., 2016)\\n\\nzi\\ng(\\xc2\\xb7)\\n\\nhi\\nf (\\xc2\\xb7)\\n\\n\\xcb\\x9cxi\\n\\nMaximize agreement\\n\\n\\xe2\\x86\\x90\\xe2\\x88\\x92 Representation\\xe2\\x88\\x92\\xe2\\x86\\x92\\n\\nt\\xe2\\x88\\xbcT\\n\\nx\\n\\nT\\n\\n\\xe2\\x88\\xbc\\n\\n(cid:48)\\nt\\n\\nzj\\ng(\\xc2\\xb7)\\n\\nhj\\n\\nf (\\xc2\\xb7)\\n\\n\\xcb\\x9cxj\\n\\nFigure 2. A simple framework for contrastive learning of visual\\nrepresentations. Two separate data augmentation operators are\\nsampled from the same family of augmentations (t \\xe2\\x88\\xbc T and\\nt(cid:48) \\xe2\\x88\\xbc T ) and applied to each data example to obtain two correlated\\nviews. A base encoder network f (\\xc2\\xb7) and a projection head g(\\xc2\\xb7)\\nare trained to maximize agreement using a contrastive loss. After\\ntraining is completed, we throw away the projection head g(\\xc2\\xb7) and\\nuse encoder f (\\xc2\\xb7) and representation h for downstream tasks.\\n\\nto obtain hi = f ( \\xcb\\x9cxi) = ResNet( \\xcb\\x9cxi) where hi \\xe2\\x88\\x88 Rd is\\nthe output after the average pooling layer.\\n\\n\\xe2\\x80\\xa2 A small neural network projection head g(\\xc2\\xb7) that maps\\nrepresentations to the space where contrastive loss is\\napplied. We use a MLP with one hidden layer to obtain\\nzi = g(hi) = W (2)\\xcf\\x83(W (1)hi) where \\xcf\\x83 is a ReLU non-\\nlinearity. As shown in section 4, we \\xef\\xac\\x81nd it bene\\xef\\xac\\x81cial to\\nde\\xef\\xac\\x81ne the contrastive loss on zi\\xe2\\x80\\x99s rather than hi\\xe2\\x80\\x99s.\\n\\n\\xe2\\x80\\xa2 A contrastive loss function de\\xef\\xac\\x81ned for a contrastive pre-\\ndiction task. Given a set { \\xcb\\x9cxk} including a positive pair\\nof examples \\xcb\\x9cxi and \\xcb\\x9cxj, the contrastive prediction task\\naims to identify \\xcb\\x9cxj in { \\xcb\\x9cxk}k(cid:54)=i for a given \\xcb\\x9cxi.\\n\\nWe randomly sample a minibatch of N examples and de\\xef\\xac\\x81ne\\nthe contrastive prediction task on pairs of augmented exam-\\nples derived from the minibatch, resulting in 2N data points.\\nWe do not sample negative examples explicitly. Instead,\\ngiven a positive pair, similar to (Chen et al., 2017), we treat\\nthe other 2(N \\xe2\\x88\\x92 1) augmented examples within a minibatch\\nas negative examples. Let sim(u, v) = u(cid:62)v/(cid:107)u(cid:107)(cid:107)v(cid:107) de-\\nnote the dot product between (cid:96)2 normalized u and v (i.e.\\ncosine similarity). Then the loss function for a positive pair\\nof examples (i, j) is de\\xef\\xac\\x81ned as\\n\\n(cid:96)i,j = \\xe2\\x88\\x92 log\\n\\nexp(sim(zi, zj)/\\xcf\\x84 )\\n1[k(cid:54)=i] exp(sim(zi, zk)/\\xcf\\x84 )\\n\\n,\\n\\n(1)\\n\\nwhere 1[k(cid:54)=i] \\xe2\\x88\\x88 {0, 1} is an indicator function evaluating to\\n1 iff k (cid:54)= i and \\xcf\\x84 denotes a temperature parameter. The \\xef\\xac\\x81-\\nnal loss is computed across all positive pairs, both (i, j)\\nand (j, i), in a mini-batch. This loss has been used in\\nprevious work (Sohn, 2016; Wu et al., 2018; Oord et al.,\\n2018); for convenience, we term it NT-Xent (the normalized\\ntemperature-scaled cross entropy loss).\\n\\n(cid:80)2N\\n\\nk=1\\n\\n\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nAlgorithm 1 SimCLR\\xe2\\x80\\x99s main learning algorithm.\\n\\ninput: batch size N, constant \\xcf\\x84, structure of f, g, T .\\nfor sampled minibatch {xk}N\\nfor all k \\xe2\\x88\\x88 {1, . . . , N} do\\n\\nk=1 do\\n\\ndraw two augmentation functions t\\xe2\\x88\\xbcT , t(cid:48)\\xe2\\x88\\xbcT\\n# the \\xef\\xac\\x81rst augmentation\\n\\xcb\\x9cx2k\\xe2\\x88\\x921 = t(xk)\\nh2k\\xe2\\x88\\x921 = f ( \\xcb\\x9cx2k\\xe2\\x88\\x921)\\nz2k\\xe2\\x88\\x921 = g(h2k\\xe2\\x88\\x921)\\n# the second augmentation\\n\\xcb\\x9cx2k = t(cid:48)(xk)\\nh2k = f ( \\xcb\\x9cx2k)\\nz2k = g(h2k)\\n\\n# representation\\n# projection\\n\\n# representation\\n# projection\\n\\nend for\\nfor all i \\xe2\\x88\\x88 {1, . . . , 2N} and j \\xe2\\x88\\x88 {1, . . . , 2N} do\\n\\nsi,j = z(cid:62)\\n\\ni zj/((cid:107)zi(cid:107)(cid:107)zj(cid:107))\\n(cid:80)N\\nk=1 [(cid:96)(2k\\xe2\\x88\\x921, 2k) + (cid:96)(2k, 2k\\xe2\\x88\\x921)]\\n\\nend for\\n(cid:80)2N\\nde\\xef\\xac\\x81ne (cid:96)(i, j) as (cid:96)(i, j) =\\xe2\\x88\\x92 log\\nL = 1\\nupdate networks f and g to minimize L\\n\\n2N\\n\\nk=1\\n\\nexp(si,j /\\xcf\\x84 )\\n1[k(cid:54)=i] exp(si,k/\\xcf\\x84 )\\n\\n# pairwise similarity\\n\\nend for\\nreturn encoder network f (\\xc2\\xb7), and throw away g(\\xc2\\xb7)\\n\\nAlgorithm 1 summarizes the proposed method.\\n\\n2.2. Training with Large Batch Size\\n\\nTo keep it simple, we do not train the model with a memory\\nbank (Wu et al., 2018; He et al., 2019). Instead, we vary\\nthe training batch size N from 256 to 8192. A batch size\\nof 8192 gives us 16382 negative examples per positive pair\\nfrom both augmentation views. Training with large batch\\nsize may be unstable when using standard SGD/Momentum\\nwith linear learning rate scaling (Goyal et al., 2017). To\\nstabilize the training, we use the LARS optimizer (You et al.,\\n2017) for all batch sizes. We train our model with Cloud\\nTPUs, using 32 to 128 cores depending on the batch size.2\\nGlobal BN. Standard ResNets use batch normaliza-\\ntion (Ioffe & Szegedy, 2015). In distributed training with\\ndata parallelism, the BN mean and variance are typically\\naggregated locally per device. In our contrastive learning,\\nas positive pairs are computed in the same device, the model\\ncan exploit the local information leakage to improve pre-\\ndiction accuracy without improving representations. We ad-\\ndress this issue by aggregating BN mean and variance over\\nall devices during the training. Other approaches include\\nshuf\\xef\\xac\\x82ing data examples across devices (He et al., 2019), or\\nreplacing BN with layer norm (H\\xc3\\xa9naff et al., 2019).\\n\\n2With 128 TPU v3 cores, it takes \\xe2\\x88\\xbc1.5 hours to train our\\n\\nResNet-50 with a batch size of 4096 for 100 epochs.\\n\\nA B\\n\\nD\\n\\nC\\n\\n(a) Global and local views.\\n\\n(b) Adjacent views.\\n\\nFigure 3. Solid rectangles are images, dashed rectangles are ran-\\ndom crops. By randomly cropping images, we sample contrastive\\nprediction tasks that include global to local view (B \\xe2\\x86\\x92 A) or\\nadjacent view (D \\xe2\\x86\\x92 C) prediction.\\n\\n2.3. Evaluation Protocol\\n\\nHere we lay out the protocol for our empirical studies, which\\naim to understand different design choices in our framework.\\nDataset and Metrics. Most of our study for unsupervised\\npretraining (learning encoder network f without labels)\\nis done using the ImageNet ILSVRC-2012 dataset (Rus-\\nsakovsky et al., 2015). Some additional pretraining experi-\\nments on CIFAR-10 (Krizhevsky & Hinton, 2009) can be\\nfound in Appendix B.9. We also test the pretrained results\\non a wide range of datasets for transfer learning. To evalu-\\nate the learned representations, we follow the widely used\\nlinear evaluation protocol (Zhang et al., 2016; Oord et al.,\\n2018; Bachman et al., 2019; Kolesnikov et al., 2019), where\\na linear classi\\xef\\xac\\x81er is trained on top of the frozen base net-\\nwork, and test accuracy is used as a proxy for representation\\nquality. Beyond linear evaluation, we also compare against\\nstate-of-the-art on semi-supervised and transfer learning.\\nDefault setting. Unless otherwise speci\\xef\\xac\\x81ed, for data aug-\\nmentation we use random crop and resize (with random\\n\\xef\\xac\\x82ip), color distortions, and Gaussian blur (for details, see\\nAppendix A). We use ResNet-50 as the base encoder net-\\nwork, and a 2-layer MLP projection head to project the\\nrepresentation to a 128-dimensional latent space. As the\\nloss, we use NT-Xent, optimized using LARS with learning\\nrate of 4.8 (= 0.3 \\xc3\\x97 BatchSize/256) and weight decay of\\n10\\xe2\\x88\\x926. We train at batch size 4096 for 100 epochs.3 Fur-\\nthermore, we use linear warmup for the \\xef\\xac\\x81rst 10 epochs,\\nand decay the learning rate with the cosine decay schedule\\nwithout restarts (Loshchilov & Hutter, 2016).\\n\\n3. Data Augmentation for Contrastive\\n\\nRepresentation Learning\\n\\nData augmentation de\\xef\\xac\\x81nes predictive tasks. While data\\naugmentation has been widely used in both supervised and\\nunsupervised representation learning (Krizhevsky et al.,\\n\\n3Although max performance is not reached in 100 epochs, rea-\\nsonable results are achieved, allowing fair and ef\\xef\\xac\\x81cient ablations.\\n\\n\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\n(a) Original\\n\\n(b) Crop and resize\\n\\n(c) Crop, resize (and \\xef\\xac\\x82ip) (d) Color distort. (drop)\\n\\n(e) Color distort. (jitter)\\n\\n(f) Rotate {90\\xe2\\x97\\xa6, 180\\xe2\\x97\\xa6, 270\\xe2\\x97\\xa6}\\n\\n(g) Cutout\\n\\n(h) Gaussian noise\\n\\n(i) Gaussian blur\\n\\n(j) Sobel \\xef\\xac\\x81ltering\\n\\nFigure 4. Illustrations of the studied data augmentation operators. Each augmentation can transform data stochastically with some internal\\nparameters (e.g. rotation degree, noise level). Note that we only test these operators in ablation, the augmentation policy used to train our\\nmodels only includes random crop (with \\xef\\xac\\x82ip and resize), color distortion, and Gaussian blur. (Original image cc-by: Von.grzanka)\\n\\n2012; H\\xc3\\xa9naff et al., 2019; Bachman et al., 2019), it has\\nnot been considered as a systematic way to de\\xef\\xac\\x81ne the con-\\ntrastive prediction task. Many existing approaches de\\xef\\xac\\x81ne\\ncontrastive prediction tasks by changing the architecture.\\nFor example, Hjelm et al. (2018); Bachman et al. (2019)\\nachieve global-to-local view prediction via constraining the\\nreceptive \\xef\\xac\\x81eld in the network architecture, whereas Oord\\net al. (2018); H\\xc3\\xa9naff et al. (2019) achieve neighboring view\\nprediction via a \\xef\\xac\\x81xed image splitting procedure and a con-\\ntext aggregation network. We show that this complexity can\\nbe avoided by performing simple random cropping (with\\nresizing) of target images, which creates a family of predic-\\ntive tasks subsuming the above mentioned two, as shown in\\nFigure 3. This simple design choice conveniently decouples\\nthe predictive task from other components such as the neural\\nnetwork architecture. Broader contrastive prediction tasks\\ncan be de\\xef\\xac\\x81ned by extending the family of augmentations\\nand composing them stochastically.\\n\\n3.1. Composition of data augmentation operations is\\n\\ncrucial for learning good representations\\n\\nTo systematically study the impact of data augmentation,\\nwe consider several common augmentations here. One type\\nof augmentation involves spatial/geometric transformation\\nof data, such as cropping and resizing (with horizontal\\n\\xef\\xac\\x82ipping), rotation (Gidaris et al., 2018) and cutout (De-\\nVries & Taylor, 2017). The other type of augmentation\\ninvolves appearance transformation, such as color distortion\\n(including color dropping, brightness, contrast, saturation,\\nhue) (Howard, 2013; Szegedy et al., 2015), Gaussian blur,\\nand Sobel \\xef\\xac\\x81ltering. Figure 4 visualizes the augmentations\\nthat we study in this work.\\n\\nFigure 5. Linear evaluation (ImageNet top-1 accuracy) under in-\\ndividual or composition of data augmentations, applied only to\\none branch. For all columns but the last, diagonal entries corre-\\nspond to single transformation, and off-diagonals correspond to\\ncomposition of two transformations (applied sequentially). The\\nlast column re\\xef\\xac\\x82ects the average over the row.\\n\\nTo understand the effects of individual data augmentations\\nand the importance of augmentation composition, we in-\\nvestigate the performance of our framework when applying\\naugmentations individually or in pairs. Since ImageNet\\nimages are of different sizes, we always apply crop and re-\\nsize images (Krizhevsky et al., 2012; Szegedy et al., 2015),\\nwhich makes it dif\\xef\\xac\\x81cult to study other augmentations in\\nthe absence of cropping. To eliminate this confound, we\\nconsider an asymmetric data transformation setting for this\\nablation. Speci\\xef\\xac\\x81cally, we always \\xef\\xac\\x81rst randomly crop im-\\nages and resize them to the same resolution, and we then\\napply the targeted transformation(s) only to one branch of\\nthe framework in Figure 2, while leaving the other branch\\nas the identity (i.e. t(xi) = xi). Note that this asymmet-\\n\\nCropCutoutColorSobelNoiseBlurRotateAverage2nd transformationCropCutoutColorSobelNoiseBlurRotate1st transformation33.133.956.346.039.935.030.239.232.225.633.940.026.525.222.429.455.835.518.821.011.416.520.825.746.240.620.94.09.36.24.218.838.825.87.57.69.89.89.615.535.125.216.65.89.72.66.714.530.022.520.74.39.76.52.613.81020304050\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\n(a) Without color distortion.\\n\\n(b) With color distortion.\\n\\nFigure 6. Histograms of pixel intensities (over all channels) for\\ndifferent crops of two different images (i.e. two rows). The image\\nfor the \\xef\\xac\\x81rst row is from Figure 4. All axes have the same range.\\n\\nMethods\\nSimCLR\\nSupervised\\n\\n1/8\\n59.6\\n77.0\\n\\nColor distortion strength\\n1/4\\n61.0\\n76.7\\n\\n1/2\\n62.6\\n76.5\\n\\n63.2\\n75.7\\n\\n1\\n\\n64.5\\n75.4\\n\\n1 (+Blur) AutoAug\\n\\n61.1\\n77.1\\n\\nTable 1. Top-1 accuracy of unsupervised ResNet-50 using linear\\nevaluation and supervised ResNet-505, under varied color distor-\\ntion strength (see Appendix A) and other data transformations.\\nStrength 1 (+Blur) is our default data augmentation policy.\\n\\nric data augmentation hurts the performance. Nonetheless,\\nthis setup should not substantively change the impact of\\nindividual data augmentations or their compositions.\\nFigure 5 shows linear evaluation results under individual\\nand composition of transformations. We observe that no\\nsingle transformation suf\\xef\\xac\\x81ces to learn good representations,\\neven though the model can almost perfectly identify the\\npositive pairs in the contrastive task. When composing aug-\\nmentations, the contrastive prediction task becomes harder,\\nbut the quality of representation improves dramatically. Ap-\\npendix B.2 provides a further study on composing broader\\nset of augmentations.\\nOne composition of augmentations stands out: random crop-\\nping and random color distortion. We conjecture that one\\nserious issue when using only random cropping as data\\naugmentation is that most patches from an image share a\\nsimilar color distribution. Figure 6 shows that color his-\\ntograms alone suf\\xef\\xac\\x81ce to distinguish images. Neural nets\\nmay exploit this shortcut to solve the predictive task. There-\\nfore, it is critical to compose cropping with color distortion\\nin order to learn generalizable features.\\n\\n3.2. Contrastive learning needs stronger data\\n\\naugmentation than supervised learning\\n\\nTo further demonstrate the importance of the color aug-\\nmentation, we adjust the strength of color augmentation as\\n\\n5Supervised models are trained for 90 epochs; longer training\\n\\nimproves performance of stronger augmentation by \\xe2\\x88\\xbc 0.5%.\\n\\nFigure 7. Linear evaluation of models with varied depth and width.\\nModels in blue dots are ours trained for 100 epochs, models in red\\nstars are ours trained for 1000 epochs, and models in green crosses\\nare supervised ResNets trained for 90 epochs7 (He et al., 2016).\\n\\nshown in Table 1. Stronger color augmentation substan-\\ntially improves the linear evaluation of the learned unsuper-\\nvised models. In this context, AutoAugment (Cubuk et al.,\\n2019), a sophisticated augmentation policy found using su-\\npervised learning, does not work better than simple cropping\\n+ (stronger) color distortion. When training supervised mod-\\nels with the same set of augmentations, we observe that\\nstronger color augmentation does not improve or even hurts\\ntheir performance. Thus, our experiments show that unsu-\\npervised contrastive learning bene\\xef\\xac\\x81ts from stronger (color)\\ndata augmentation than supervised learning. Although pre-\\nvious work has reported that data augmentation is useful\\nfor self-supervised learning (Doersch et al., 2015; Bachman\\net al., 2019; H\\xc3\\xa9naff et al., 2019; Asano et al., 2019), we\\nshow that data augmentation that does not yield accuracy\\nbene\\xef\\xac\\x81ts for supervised learning can still help considerably\\nwith contrastive learning.\\n\\n4. Architectures for Encoder and Head\\n4.1. Unsupervised contrastive learning bene\\xef\\xac\\x81ts (more)\\n\\nfrom bigger models\\n\\nFigure 7 shows, perhaps unsurprisingly, that increasing\\ndepth and width both improve performance. While similar\\n\\xef\\xac\\x81ndings hold for supervised learning (He et al., 2016), we\\n\\xef\\xac\\x81nd the gap between supervised models and linear classi\\xef\\xac\\x81ers\\ntrained on unsupervised models shrinks as the model size\\nincreases, suggesting that unsupervised learning bene\\xef\\xac\\x81ts\\nmore from bigger models than its supervised counterpart.\\n\\n7Training longer does not improve supervised ResNets (see\\n\\nAppendix B.3).\\n\\n050100150200250300350400450Number of Parameters (Millions)50556065707580Top 1R101R101(2x)R152R152(2x)R18R18(2x)R18(4x)R34R34(2x)R34(4x)R50R50(2x)R50(4x)Sup. R50Sup. R50(2x)Sup. R50(4x)R50*R50(2x)*R50(4x)*\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nName\\n\\nNT-Xent\\n\\nNT-Logistic\\n\\nMargin Triplet\\n\\nuT v+/\\xcf\\x84 \\xe2\\x88\\x92 log(cid:80)\\n\\nNegative loss function\\n\\nv\\xe2\\x88\\x88{v+,v\\xe2\\x88\\x92} exp(uT v/\\xcf\\x84 )\\n\\nlog \\xcf\\x83(uT v+/\\xcf\\x84 ) + log \\xcf\\x83(\\xe2\\x88\\x92uT v\\xe2\\x88\\x92/\\xcf\\x84 )\\n\\n\\xe2\\x88\\x92 max(uT v\\xe2\\x88\\x92 \\xe2\\x88\\x92 uT v+ + m, 0)\\n\\nGradient w.r.t. u\\n\\n)/\\xcf\\x84 v+ \\xe2\\x88\\x92(cid:80)\\n\\nZ(u)\\n\\n(1 \\xe2\\x88\\x92 exp(uT v+/\\xcf\\x84 )\\n\\nv\\xe2\\x88\\x92 exp(uT v\\xe2\\x88\\x92/\\xcf\\x84 )\\n(\\xcf\\x83(\\xe2\\x88\\x92uT v+/\\xcf\\x84 ))/\\xcf\\x84 v+ \\xe2\\x88\\x92 \\xcf\\x83(uT v\\xe2\\x88\\x92/\\xcf\\x84 )/\\xcf\\x84 v\\xe2\\x88\\x92\\nv+ \\xe2\\x88\\x92 v\\xe2\\x88\\x92 if uT v+ \\xe2\\x88\\x92 uT v\\xe2\\x88\\x92 < m else 0\\n\\nZ(u)\\n\\n/\\xcf\\x84 v\\xe2\\x88\\x92\\n\\nTable 2. Negative loss functions and their gradients. All input vectors, i.e. u, v+, v\\xe2\\x88\\x92, are (cid:96)2 normalized. NT-Xent is an abbreviation for\\n\\xe2\\x80\\x9cNormalized Temperature-scaled Cross Entropy\\xe2\\x80\\x9d. Different loss functions impose different weightings of positive and negative examples.\\n\\nWhat to predict?\\n\\nColor vs grayscale\\nRotation\\nOrig. vs corrupted\\nOrig. vs Sobel \\xef\\xac\\x81ltered\\n\\nRandom guess Representation\\ng(h)\\n97.4\\n25.6\\n59.6\\n56.3\\n\\nh\\n99.3\\n67.6\\n99.5\\n96.6\\n\\n80\\n25\\n50\\n50\\n\\nFigure 8. Linear evaluation of representations with different pro-\\njection heads g(\\xc2\\xb7) and various dimensions of z = g(h). The\\nrepresentation h (before projection) is 2048-dimensional here.\\n4.2. A nonlinear projection head improves the\\nrepresentation quality of the layer before it\\n\\nWe then study the importance of including a projection\\nhead, i.e. g(h). Figure 8 shows linear evaluation results\\nusing three different architecture for the head: (1) identity\\nmapping; (2) linear projection, as used by several previous\\napproaches (Wu et al., 2018); and (3) the default nonlinear\\nprojection with one additional hidden layer (and ReLU acti-\\nvation), similar to Bachman et al. (2019). We observe that a\\nnonlinear projection is better than a linear projection (+3%),\\nand much better than no projection (>10%). When a pro-\\njection head is used, similar results are observed regardless\\nof output dimension. Furthermore, even when nonlinear\\nprojection is used, the layer before the projection head, h,\\nis still much better (>10%) than the layer after, z = g(h),\\nwhich shows that the hidden layer before the projection\\nhead is a better representation than the layer after.\\nWe conjecture that the importance of using the representa-\\ntion before the nonlinear projection is due to loss of informa-\\ntion induced by the contrastive loss. In particular, z = g(h)\\nis trained to be invariant to data transformation. Thus, g can\\nremove information that may be useful for the downstream\\ntask, such as the color or orientation of objects. By leverag-\\ning the nonlinear transformation g(\\xc2\\xb7), more information can\\nbe formed and maintained in h. To verify this hypothesis,\\nwe conduct experiments that use either h or g(h) to learn\\nto predict the transformation applied during the pretraining.\\nHere we set g(h) = W (2)\\xcf\\x83(W (1)h), with the same input\\nand output dimensionality (i.e. 2048). Table 3 shows h\\ncontains much more information about the transformation\\napplied, while g(h) loses information. Further analysis can\\n\\nTable 3. Accuracy of training additional MLPs on different repre-\\nsentations to predict the transformation applied. Other than crop\\nand color augmentation, we additionally and independently add\\nrotation (one of {0\\xe2\\x97\\xa6, 90\\xe2\\x97\\xa6, 180\\xe2\\x97\\xa6, 270\\xe2\\x97\\xa6}), Gaussian noise, and So-\\nbel \\xef\\xac\\x81ltering transformation during the pretraining for the last three\\nrows. Both h and g(h) are of the same dimensionality, i.e. 2048.\\n\\nbe found in Appendix B.4.\\n\\n5. Loss Functions and Batch Size\\n5.1. Normalized cross entropy loss with adjustable\\n\\ntemperature works better than alternatives\\n\\nWe compare the NT-Xent loss against other commonly used\\ncontrastive loss functions, such as logistic loss (Mikolov\\net al., 2013), and margin loss (Schroff et al., 2015). Table\\n2 shows the objective function as well as the gradient to\\nthe input of the loss function. Looking at the gradient, we\\nobserve 1) (cid:96)2 normalization (i.e. cosine similarity) along\\nwith temperature effectively weights different examples, and\\nan appropriate temperature can help the model learn from\\nhard negatives; and 2) unlike cross-entropy, other objec-\\ntive functions do not weigh the negatives by their relative\\nhardness. As a result, one must apply semi-hard negative\\nmining (Schroff et al., 2015) for these loss functions: in-\\nstead of computing the gradient over all loss terms, one can\\ncompute the gradient using semi-hard negative terms (i.e.,\\nthose that are within the loss margin and closest in distance,\\nbut farther than positive examples).\\nTo make the comparisons fair, we use the same (cid:96)2 normaliza-\\ntion for all loss functions, and we tune the hyperparameters,\\nand report their best results.8 Table 4 shows that, while\\n(semi-hard) negative mining helps, the best result is still\\nmuch worse than our default NT-Xent loss.\\n\\n8Details can be found in Appendix B.10. For simplicity, we\\n\\nonly consider the negatives from one augmentation view.\\n\\n326412825651210242048Projection output dimensionality3040506070Top 1ProjectionLinearNon-linearNone\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nMargin NT-Logi. Margin (sh) NT-Logi.(sh) NT-Xent\\n50.9\\n\\n57.5\\n\\n57.9\\n\\n63.9\\n\\n51.6\\n\\nTable 4. Linear evaluation (top-1) for models trained with different\\nloss functions. \\xe2\\x80\\x9csh\\xe2\\x80\\x9d means using semi-hard negative mining.\\n\\n(cid:96)2 norm?\\n\\nYes\\n\\nNo\\n\\n\\xcf\\x84\\n0.05\\n0.1\\n0.5\\n1\\n10\\n100\\n\\nEntropy Contrastive acc.\\n\\n1.0\\n4.5\\n8.2\\n8.3\\n0.5\\n0.5\\n\\n90.5\\n87.8\\n68.2\\n59.1\\n91.7\\n92.1\\n\\nTop 1\\n59.7\\n64.4\\n60.7\\n58.0\\n57.2\\n57.0\\n\\nTable 5. Linear evaluation for models trained with different choices\\nof (cid:96)2 norm and temperature \\xcf\\x84 for NT-Xent loss. The contrastive\\ndistribution is over 4096 examples.\\n\\nFigure 9. Linear evaluation models (ResNet-50) trained with differ-\\nent batch size and epochs. Each bar is a single run from scratch.10\\n\\nWe next test the importance of the (cid:96)2 normalization (i.e.\\ncosine similarity vs dot product) and temperature \\xcf\\x84 in our\\ndefault NT-Xent loss. Table 5 shows that without normal-\\nization and proper temperature scaling, performance is sig-\\nni\\xef\\xac\\x81cantly worse. Without (cid:96)2 normalization, the contrastive\\ntask accuracy is higher, but the resulting representation is\\nworse under linear evaluation.\\n\\n5.2. Contrastive learning bene\\xef\\xac\\x81ts (more) from larger\\n\\nbatch sizes and longer training\\n\\nFigure 9 shows the impact of batch size when models are\\ntrained for different numbers of epochs. We \\xef\\xac\\x81nd that, when\\nthe number of training epochs is small (e.g. 100 epochs),\\nlarger batch sizes have a signi\\xef\\xac\\x81cant advantage over the\\nsmaller ones. With more training steps/epochs, the gaps\\nbetween different batch sizes decrease or disappear, pro-\\nvided the batches are randomly resampled. In contrast to\\n\\n10A linear learning rate scaling is used here. Figure B.1 shows\\nusing a square root learning rate scaling can improve performance\\nof ones with small batch sizes.\\n\\nArchitecture\\n\\nMethod\\nMethods using ResNet-50:\\nResNet-50\\nLocal Agg.\\nResNet-50\\nMoCo\\nResNet-50\\nPIRL\\nResNet-50\\nCPC v2\\nSimCLR (ours) ResNet-50\\nMethods using other architectures:\\nRevNet-50 (4\\xc3\\x97)\\nRotation\\nRevNet-50 (4\\xc3\\x97)\\nBigBiGAN\\nAMDIM\\nCustom-ResNet\\nResNet-50 (2\\xc3\\x97)\\nCMC\\nResNet-50 (4\\xc3\\x97)\\nMoCo\\nResNet-161 (\\xe2\\x88\\x97)\\nCPC v2\\nSimCLR (ours) ResNet-50 (2\\xc3\\x97)\\nSimCLR (ours) ResNet-50 (4\\xc3\\x97)\\n\\nParam (M) Top 1 Top 5\\n\\n24\\n24\\n24\\n24\\n24\\n\\n86\\n86\\n626\\n188\\n375\\n305\\n94\\n375\\n\\n60.2\\n60.6\\n63.6\\n63.8\\n69.3\\n\\n55.4\\n61.3\\n68.1\\n68.4\\n68.6\\n71.5\\n74.2\\n76.5\\n\\n-\\n-\\n-\\n\\n85.3\\n89.0\\n\\n-\\n\\n-\\n\\n-\\n\\n81.9\\n\\n88.2\\n\\n90.1\\n92.0\\n93.2\\n\\nTable 6. ImageNet accuracies of linear classi\\xef\\xac\\x81ers trained on repre-\\nsentations learned with different self-supervised methods.\\n\\nMethod\\n\\nArchitecture\\n\\nResNet-50\\n\\nSupervised baseline\\nMethods using other label-propagation:\\nResNet-50\\nPseudo-label\\nResNet-50\\nVAT+Entropy Min.\\nUDA (w. RandAug)\\nResNet-50\\nFixMatch (w. RandAug) ResNet-50\\nS4L (Rot+VAT+En. M.) ResNet-50 (4\\xc3\\x97)\\nMethods using representation learning only:\\nInstDisc\\nResNet-50\\nRevNet-50 (4\\xc3\\x97)\\nBigBiGAN\\nPIRL\\nResNet-50\\nResNet-161(\\xe2\\x88\\x97)\\nCPC v2\\nResNet-50\\nSimCLR (ours)\\nResNet-50 (2\\xc3\\x97)\\nSimCLR (ours)\\nResNet-50 (4\\xc3\\x97)\\nSimCLR (ours)\\n\\nLabel fraction\\n1%\\n10%\\n\\nTop 5\\n\\n48.4\\n\\n80.4\\n\\n51.6\\n47.0\\n\\n-\\n-\\n-\\n\\n39.2\\n55.2\\n57.2\\n77.9\\n75.5\\n83.0\\n85.8\\n\\n82.4\\n83.4\\n88.5\\n89.1\\n91.2\\n\\n77.4\\n78.8\\n83.8\\n91.2\\n87.8\\n91.2\\n92.6\\n\\nTable 7. ImageNet accuracy of models trained with few labels.\\n\\nsupervised learning (Goyal et al., 2017), in contrastive learn-\\ning, larger batch sizes provide more negative examples,\\nfacilitating convergence (i.e. taking fewer epochs and steps\\nfor a given accuracy). Training longer also provides more\\nnegative examples, improving the results. In Appendix B.1,\\nresults with even longer training steps are provided.\\n\\n6. Comparison with State-of-the-art\\nIn this subsection, similar to Kolesnikov et al. (2019); He\\net al. (2019), we use ResNet-50 in 3 different hidden layer\\nwidths (width multipliers of 1\\xc3\\x97, 2\\xc3\\x97, and 4\\xc3\\x97). For better\\nconvergence, our models here are trained for 1000 epochs.\\nLinear evaluation. Table 6 compares our results with previ-\\nous approaches (Zhuang et al., 2019; He et al., 2019; Misra\\n& van der Maaten, 2019; H\\xc3\\xa9naff et al., 2019; Kolesnikov\\net al., 2019; Donahue & Simonyan, 2019; Bachman et al.,\\n\\n1002003004005006007008009001000Training epochs50.052.555.057.560.062.565.067.570.0Top 1Batch size2565121024204840968192\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nFood CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers\\n\\nLinear evaluation:\\nSimCLR (ours) 76.9\\nSupervised\\n75.2\\nFine-tuned:\\nSimCLR (ours) 89.4\\n88.7\\nSupervised\\nRandom init\\n88.3\\n\\n95.3\\n95.7\\n\\n98.6\\n98.3\\n96.0\\n\\n80.2\\n81.2\\n\\n89.0\\n88.7\\n81.9\\n\\n48.4\\n56.4\\n\\n78.2\\n77.8\\n77.0\\n\\n65.9\\n64.9\\n\\n68.1\\n67.0\\n53.7\\n\\n60.0\\n68.8\\n\\n92.1\\n91.4\\n91.3\\n\\n61.2\\n63.8\\n\\n87.0\\n88.0\\n84.8\\n\\n84.2\\n83.8\\n\\n86.6\\n86.5\\n69.4\\n\\n78.9 89.2\\n78.7 92.3\\n\\n77.8 92.1\\n78.8 93.2\\n64.1 82.7\\n\\n93.9\\n94.1\\n\\n94.1\\n94.2\\n72.5\\n\\n95.0\\n94.2\\n\\n97.6\\n98.0\\n92.5\\n\\nTable 8. Comparison of transfer learning performance of our self-supervised approach with supervised baselines across 12 natural image\\nclassi\\xef\\xac\\x81cation datasets, for ResNet-50 (4\\xc3\\x97) models pretrained on ImageNet. Results not signi\\xef\\xac\\x81cantly worse than the best (p > 0.05,\\npermutation test) are shown in bold. See Appendix B.8 for experimental details and results with standard ResNet-50.\\n\\n2019; Tian et al., 2019) in the linear evaluation setting (see\\nAppendix B.6). Table 1 shows more numerical compar-\\nisons among different methods. We are able to use standard\\nnetworks to obtain substantially better results compared to\\nprevious methods that require speci\\xef\\xac\\x81cally designed archi-\\ntectures. The best result obtained with our ResNet-50 (4\\xc3\\x97)\\ncan match the supervised pretrained ResNet-50.\\nSemi-supervised learning. We follow Zhai et al. (2019)\\nand sample 1% or 10% of the labeled ILSVRC-12 training\\ndatasets in a class-balanced way (\\xe2\\x88\\xbc12.8 and \\xe2\\x88\\xbc128 images\\nper class respectively). 11 We simply \\xef\\xac\\x81ne-tune the whole\\nbase network on the labeled data without regularization\\n(see Appendix B.5). Table 7 shows the comparisons of\\nour results against recent methods (Zhai et al., 2019; Xie\\net al., 2019; Sohn et al., 2020; Wu et al., 2018; Donahue &\\nSimonyan, 2019; Misra & van der Maaten, 2019; H\\xc3\\xa9naff\\net al., 2019). The supervised baseline from (Zhai et al.,\\n2019) is strong due to intensive search of hyper-parameters\\n(including augmentation). Again, our approach signi\\xef\\xac\\x81cantly\\nimproves over state-of-the-art with both 1% and 10% of the\\nlabels. Interestingly, \\xef\\xac\\x81ne-tuning our pretrained ResNet-50\\n(2\\xc3\\x97, 4\\xc3\\x97) on full ImageNet are also signi\\xef\\xac\\x81cantly better then\\ntraining from scratch (up to 2%, see Appendix B.2).\\nTransfer learning. We evaluate transfer learning perfor-\\nmance across 12 natural image datasets in both linear evalu-\\nation (\\xef\\xac\\x81xed feature extractor) and \\xef\\xac\\x81ne-tuning settings. Fol-\\nlowing Kornblith et al. (2019), we perform hyperparameter\\ntuning for each model-dataset combination and select the\\nbest hyperparameters on a validation set. Table 8 shows\\nresults with the ResNet-50 (4\\xc3\\x97) model. When \\xef\\xac\\x81ne-tuned,\\nour self-supervised model signi\\xef\\xac\\x81cantly outperforms the su-\\npervised baseline on 5 datasets, whereas the supervised\\nbaseline is superior on only 2 (i.e. Pets and Flowers). On\\nthe remaining 5 datasets, the models are statistically tied.\\nFull experimental details as well as results with the standard\\nResNet-50 architecture are provided in Appendix B.8.\\n\\n11The details of sampling and exact subsets can be found in\\nhttps://www.tensor\\xef\\xac\\x82ow.org/datasets/catalog/imagenet2012_subset.\\n\\n7. Related Work\\nThe idea of making representations of an image agree with\\neach other under small transformations dates back to Becker\\n& Hinton (1992). We extend it by leveraging recent ad-\\nvances in data augmentation, network architecture and con-\\ntrastive loss. A similar consistency idea, but for class label\\nprediction, has been explored in other contexts such as semi-\\nsupervised learning (Xie et al., 2019; Berthelot et al., 2019).\\nHandcrafted pretext tasks. The recent renaissance of self-\\nsupervised learning began with arti\\xef\\xac\\x81cially designed pretext\\ntasks, such as relative patch prediction (Doersch et al., 2015),\\nsolving jigsaw puzzles (Noroozi & Favaro, 2016), coloriza-\\ntion (Zhang et al., 2016) and rotation prediction (Gidaris\\net al., 2018; Chen et al., 2019). Although good results\\ncan be obtained with bigger networks and longer train-\\ning (Kolesnikov et al., 2019), these pretext tasks rely on\\nsomewhat ad-hoc heuristics, which limits the generality of\\nlearned representations.\\nContrastive visual representation learning. Dating back\\nto Hadsell et al. (2006), these approaches learn represen-\\ntations by contrasting positive pairs against negative pairs.\\nAlong these lines, Dosovitskiy et al. (2014) proposes to\\ntreat each instance as a class represented by a feature vector\\n(in a parametric form). Wu et al. (2018) proposes to use\\na memory bank to store the instance class representation\\nvector, an approach adopted and extended in several recent\\npapers (Zhuang et al., 2019; Tian et al., 2019; He et al.,\\n2019; Misra & van der Maaten, 2019). Other work explores\\nthe use of in-batch samples for negative sampling instead\\nof a memory bank (Doersch & Zisserman, 2017; Ye et al.,\\n2019; Ji et al., 2019).\\nRecent literature has attempted to relate the success of their\\nmethods to maximization of mutual information between\\nlatent representations (Oord et al., 2018; H\\xc3\\xa9naff et al., 2019;\\nHjelm et al., 2018; Bachman et al., 2019). However, it is not\\nclear if the success of contrastive approaches is determined\\nby the mutual information, or by the speci\\xef\\xac\\x81c form of the\\ncontrastive loss (Tschannen et al., 2019).\\n\\n\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nWe note that almost all individual components of our frame-\\nwork have appeared in previous work, although the speci\\xef\\xac\\x81c\\ninstantiations may be different. The superiority of our frame-\\nwork relative to previous work is not explained by any single\\ndesign choice, but by their composition. We provide a com-\\nprehensive comparison of our design choices with those of\\nprevious work in Appendix C.\\n\\n8. Conclusion\\nIn this work, we present a simple framework and its in-\\nstantiation for contrastive visual representation learning.\\nWe carefully study its components, and show the effects\\nof different design choices. By combining our \\xef\\xac\\x81ndings,\\nwe improve considerably over previous methods for self-\\nsupervised, semi-supervised, and transfer learning.\\nOur approach differs from standard supervised learning on\\nImageNet only in the choice of data augmentation, the use of\\na nonlinear head at the end of the network, and the loss func-\\ntion. The strength of this simple framework suggests that,\\ndespite a recent surge in interest, self-supervised learning\\nremains undervalued.\\n\\nAcknowledgements\\nWe would like to thank Xiaohua Zhai, Rafael M\\xc3\\xbcller and\\nYani Ioannou for their feedback on the draft. We are also\\ngrateful for general support from Google Research teams in\\nToronto and elsewhere.\\n\\nReferences\\nAsano, Y. M., Rupprecht, C., and Vedaldi, A. A critical analysis\\nof self-supervision, or what we can learn from a single image.\\narXiv preprint arXiv:1904.13132, 2019.\\n\\nBachman, P., Hjelm, R. D., and Buchwalter, W. Learning rep-\\nresentations by maximizing mutual information across views.\\nIn Advances in Neural Information Processing Systems, pp.\\n15509\\xe2\\x80\\x9315519, 2019.\\n\\nBecker, S. and Hinton, G. E. Self-organizing neural network that\\ndiscovers surfaces in random-dot stereograms. Nature, 355\\n(6356):161\\xe2\\x80\\x93163, 1992.\\n\\nBerg, T., Liu, J., Lee, S. W., Alexander, M. L., Jacobs, D. W.,\\nand Belhumeur, P. N. Birdsnap: Large-scale \\xef\\xac\\x81ne-grained visual\\ncategorization of birds. In IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), pp. 2019\\xe2\\x80\\x932026. IEEE, 2014.\\n\\nBerthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver,\\nA., and Raffel, C. A. Mixmatch: A holistic approach to semi-\\nsupervised learning. In Advances in Neural Information Pro-\\ncessing Systems, pp. 5050\\xe2\\x80\\x935060, 2019.\\n\\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101\\xe2\\x80\\x93mining\\ndiscriminative components with random forests. In European\\nconference on computer vision, pp. 446\\xe2\\x80\\x93461. Springer, 2014.\\n\\nChen, T., Sun, Y., Shi, Y., and Hong, L. On sampling strategies\\nfor neural network-based collaborative \\xef\\xac\\x81ltering. In Proceed-\\nings of the 23rd ACM SIGKDD International Conference on\\nKnowledge Discovery and Data Mining, pp. 767\\xe2\\x80\\x93776, 2017.\\n\\nChen, T., Zhai, X., Ritter, M., Lucic, M., and Houlsby, N. Self-\\nsupervised gans via auxiliary rotation loss. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition,\\npp. 12154\\xe2\\x80\\x9312163, 2019.\\n\\nCimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi,\\nA. Describing textures in the wild. In IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR), pp. 3606\\xe2\\x80\\x93\\n3613. IEEE, 2014.\\n\\nCubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V.\\nAutoaugment: Learning augmentation strategies from data. In\\nProceedings of the IEEE conference on computer vision and\\npattern recognition, pp. 113\\xe2\\x80\\x93123, 2019.\\n\\nDeVries, T. and Taylor, G. W.\\n\\nImproved regularization of\\nconvolutional neural networks with cutout. arXiv preprint\\narXiv:1708.04552, 2017.\\n\\nDoersch, C. and Zisserman, A. Multi-task self-supervised visual\\nlearning. In Proceedings of the IEEE International Conference\\non Computer Vision, pp. 2051\\xe2\\x80\\x932060, 2017.\\n\\nDoersch, C., Gupta, A., and Efros, A. A. Unsupervised visual\\nrepresentation learning by context prediction. In Proceedings\\nof the IEEE International Conference on Computer Vision, pp.\\n1422\\xe2\\x80\\x931430, 2015.\\n\\nDonahue, J. and Simonyan, K. Large scale adversarial representa-\\ntion learning. In Advances in Neural Information Processing\\nSystems, pp. 10541\\xe2\\x80\\x9310551, 2019.\\n\\nDonahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E.,\\nand Darrell, T. Decaf: A deep convolutional activation feature\\nfor generic visual recognition. In International Conference on\\nMachine Learning, pp. 647\\xe2\\x80\\x93655, 2014.\\n\\nDosovitskiy, A., Springenberg, J. T., Riedmiller, M., and Brox, T.\\nDiscriminative unsupervised feature learning with convolutional\\nneural networks. In Advances in neural information processing\\nsystems, pp. 766\\xe2\\x80\\x93774, 2014.\\n\\nEveringham, M., Van Gool, L., Williams, C. K., Winn, J., and\\nZisserman, A. The pascal visual object classes (voc) challenge.\\nInternational Journal of Computer Vision, 88(2):303\\xe2\\x80\\x93338, 2010.\\n\\nFei-Fei, L., Fergus, R., and Perona, P. Learning generative visual\\nmodels from few training examples: An incremental bayesian\\napproach tested on 101 object categories. In IEEE Conference\\non Computer Vision and Pattern Recognition (CVPR) Workshop\\non Generative-Model Based Vision, 2004.\\n\\nGidaris, S., Singh, P., and Komodakis, N. Unsupervised represen-\\ntation learning by predicting image rotations. arXiv preprint\\narXiv:1803.07728, 2018.\\n\\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-\\nFarley, D., Ozair, S., Courville, A., and Bengio, Y. Generative\\nadversarial nets. In Advances in neural information processing\\nsystems, pp. 2672\\xe2\\x80\\x932680, 2014.\\n\\n\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nGoyal, P., Doll\\xc3\\xa1r, P., Girshick, R., Noordhuis, P., Wesolowski, L.,\\nKyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large\\nminibatch sgd: Training imagenet in 1 hour. arXiv preprint\\narXiv:1706.02677, 2017.\\n\\nHadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction\\nby learning an invariant mapping. In 2006 IEEE Computer So-\\nciety Conference on Computer Vision and Pattern Recognition\\n(CVPR\\xe2\\x80\\x9906), volume 2, pp. 1735\\xe2\\x80\\x931742. IEEE, 2006.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for\\nimage recognition. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pp. 770\\xe2\\x80\\x93778, 2016.\\n\\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum\\ncontrast for unsupervised visual representation learning. arXiv\\npreprint arXiv:1911.05722, 2019.\\n\\nH\\xc3\\xa9naff, O. J., Razavi, A., Doersch, C., Eslami, S., and Oord, A.\\nv. d. Data-ef\\xef\\xac\\x81cient image recognition with contrastive predictive\\ncoding. arXiv preprint arXiv:1905.09272, 2019.\\n\\nHinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning al-\\ngorithm for deep belief nets. Neural computation, 18(7):1527\\xe2\\x80\\x93\\n1554, 2006.\\n\\nHjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K.,\\nBachman, P., Trischler, A., and Bengio, Y. Learning deep repre-\\nsentations by mutual information estimation and maximization.\\narXiv preprint arXiv:1808.06670, 2018.\\n\\nHoward, A. G. Some improvements on deep convolutional\\nneural network based image classi\\xef\\xac\\x81cation. arXiv preprint\\narXiv:1312.5402, 2013.\\n\\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift. arXiv\\npreprint arXiv:1502.03167, 2015.\\n\\nJi, X., Henriques, J. F., and Vedaldi, A. Invariant information\\nclustering for unsupervised image classi\\xef\\xac\\x81cation and segmenta-\\ntion. In Proceedings of the IEEE International Conference on\\nComputer Vision, pp. 9865\\xe2\\x80\\x939874, 2019.\\n\\nKingma, D. P. and Welling, M. Auto-encoding variational bayes.\\n\\narXiv preprint arXiv:1312.6114, 2013.\\n\\nKolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-supervised\\nIn Proceedings of the IEEE\\nvisual representation learning.\\nconference on Computer Vision and Pattern Recognition, pp.\\n1920\\xe2\\x80\\x931929, 2019.\\n\\nKornblith, S., Shlens, J., and Le, Q. V. Do better ImageNet models\\nIn Proceedings of the IEEE conference on\\ntransfer better?\\ncomputer vision and pattern recognition, pp. 2661\\xe2\\x80\\x932671, 2019.\\n\\nKrause, J., Deng, J., Stark, M., and Fei-Fei, L. Collecting a\\nlarge-scale dataset of \\xef\\xac\\x81ne-grained cars. In Second Workshop on\\nFine-Grained Visual Categorization, 2013.\\n\\nKrizhevsky, A. and Hinton, G. Learning multiple layers of features\\nfrom tiny images. Technical report, University of Toronto,\\n2009. URL https://www.cs.toronto.edu/~kriz/\\nlearning-features-2009-TR.pdf.\\n\\nLoshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent\\n\\nwith warm restarts. arXiv preprint arXiv:1608.03983, 2016.\\n\\nMaaten, L. v. d. and Hinton, G. Visualizing data using t-sne. Jour-\\n\\nnal of machine learning research, 9(Nov):2579\\xe2\\x80\\x932605, 2008.\\n\\nMaji, S., Kannala, J., Rahtu, E., Blaschko, M., and Vedaldi, A.\\nFine-grained visual classi\\xef\\xac\\x81cation of aircraft. Technical report,\\n2013.\\n\\nMikolov, T., Chen, K., Corrado, G., and Dean, J. Ef\\xef\\xac\\x81cient esti-\\nmation of word representations in vector space. arXiv preprint\\narXiv:1301.3781, 2013.\\n\\nMisra,\\n\\nI. and van der Maaten, L.\\n\\ning of pretext-invariant representations.\\narXiv:1912.01991, 2019.\\n\\nSelf-supervised learn-\\narXiv preprint\\n\\nNilsback, M.-E. and Zisserman, A. Automated \\xef\\xac\\x82ower classi\\xef\\xac\\x81cation\\nover a large number of classes. In Computer Vision, Graphics &\\nImage Processing, 2008. ICVGIP\\xe2\\x80\\x9908. Sixth Indian Conference\\non, pp. 722\\xe2\\x80\\x93729. IEEE, 2008.\\n\\nNoroozi, M. and Favaro, P. Unsupervised learning of visual repre-\\nsentations by solving jigsaw puzzles. In European Conference\\non Computer Vision, pp. 69\\xe2\\x80\\x9384. Springer, 2016.\\n\\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748,\\n2018.\\n\\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. Cats\\nand dogs. In IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), pp. 3498\\xe2\\x80\\x933505. IEEE, 2012.\\n\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma,\\nS., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.\\nImagenet large scale visual recognition challenge. International\\njournal of computer vision, 115(3):211\\xe2\\x80\\x93252, 2015.\\n\\nSchroff, F., Kalenichenko, D., and Philbin, J. Facenet: A uni\\xef\\xac\\x81ed\\nIn Proceed-\\nembedding for face recognition and clustering.\\nings of the IEEE conference on computer vision and pattern\\nrecognition, pp. 815\\xe2\\x80\\x93823, 2015.\\n\\nSimonyan, K. and Zisserman, A. Very deep convolutional\\nnetworks for large-scale image recognition. arXiv preprint\\narXiv:1409.1556, 2014.\\n\\nSohn, K. Improved deep metric learning with multi-class n-pair\\nloss objective. In Advances in neural information processing\\nsystems, pp. 1857\\xe2\\x80\\x931865, 2016.\\n\\nSohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk,\\nE. D., Kurakin, A., Zhang, H., and Raffel, C. Fixmatch: Simpli-\\nfying semi-supervised learning with consistency and con\\xef\\xac\\x81dence.\\narXiv preprint arXiv:2001.07685, 2020.\\n\\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D.,\\nErhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper\\nwith convolutions. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pp. 1\\xe2\\x80\\x939, 2015.\\n\\nTian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding.\\n\\narXiv preprint arXiv:1906.05849, 2019.\\n\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classi\\xef\\xac\\x81-\\ncation with deep convolutional neural networks. In Advances in\\nneural information processing systems, pp. 1097\\xe2\\x80\\x931105, 2012.\\n\\nTschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S., and Lu-\\ncic, M. On mutual information maximization for representation\\nlearning. arXiv preprint arXiv:1907.13625, 2019.\\n\\n\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nWu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised feature\\nlearning via non-parametric instance discrimination. In Proceed-\\nings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pp. 3733\\xe2\\x80\\x933742, 2018.\\n\\nXiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun\\ndatabase: Large-scale scene recognition from abbey to zoo. In\\nIEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 3485\\xe2\\x80\\x933492. IEEE, 2010.\\n\\nXie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. Unsu-\\npervised data augmentation. arXiv preprint arXiv:1904.12848,\\n2019.\\n\\nYe, M., Zhang, X., Yuen, P. C., and Chang, S.-F. Unsupervised\\nembedding learning via invariant and spreading instance feature.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 6210\\xe2\\x80\\x936219, 2019.\\n\\nYou, Y., Gitman, I., and Ginsburg, B. Large batch training of con-\\nvolutional networks. arXiv preprint arXiv:1708.03888, 2017.\\n\\nZhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S4l: Self-\\nsupervised semi-supervised learning. In The IEEE International\\nConference on Computer Vision (ICCV), October 2019.\\n\\nZhang, R., Isola, P., and Efros, A. A. Colorful image coloriza-\\ntion. In European conference on computer vision, pp. 649\\xe2\\x80\\x93666.\\nSpringer, 2016.\\n\\nZhuang, C., Zhai, A. L., and Yamins, D. Local aggregation for\\nunsupervised learning of visual embeddings. In Proceedings\\nof the IEEE International Conference on Computer Vision, pp.\\n6002\\xe2\\x80\\x936012, 2019.\\n\\n\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nA. Data Augmentation Details\\nIn our default pretraining setting (which is used to train our best models), we utilize random crop (with resize and random\\n\\xef\\xac\\x82ip), random color distortion, and random Gaussian blur as the data augmentations. The details of these three augmentations\\nare provided below.\\n\\nRandom crop and resize to 224x224 We use standard Inception-style random cropping (Szegedy et al., 2015). The\\ncrop of random size (uniform from 0.08 to 1.0 in area) of the original size and a random aspect ratio (default: of\\n3/4 to 4/3) of the original aspect ratio is made. This crop is \\xef\\xac\\x81nally resized to the original size. This has been imple-\\nmented in Tensor\\xef\\xac\\x82ow as \\xe2\\x80\\x9cslim.preprocessing.inception_preprocessing.distorted_bounding_box_crop\\xe2\\x80\\x9d, or in Pytorch\\nas \\xe2\\x80\\x9ctorchvision.transforms.RandomResizedCrop\\xe2\\x80\\x9d. Additionally, the random crop (with resize) is always followed by a\\nrandom horizontal/left-to-right \\xef\\xac\\x82ip with 50% probability. This is helpful but not essential. By removing this from our default\\naugmentation policy, the top-1 linear evaluation drops from 64.5% to 63.4% for our ResNet-50 model trained in 100 epochs.\\n\\nColor distortion Color distortion is composed by color jittering and color dropping. We \\xef\\xac\\x81nd stronger color jittering\\nusually helps, so we set a strength parameter.\\nA pseudo-code for color distortion using TensorFlow is as follows.\\n\\nimport tensorflow as tf\\ndef color_distortion(image, s=1.0):\\n\\n# image is a tensor with value range in [0, 1].\\n# s is the strength of color distortion.\\n\\ndef color_jitter(x):\\n\\n# one can also shuffle the order of following augmentations\\n# each time they are applied.\\nx = tf.image.random_brightness(x, max_delta=0.8*s)\\nx = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s)\\nx = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s)\\nx = tf.image.random_hue(x, max_delta=0.2*s)\\nx = tf.clip_by_value(x, 0, 1)\\nreturn x\\n\\ndef color_drop(x):\\n\\nimage = tf.image.rgb_to_grayscale(image)\\nimage = tf.tile(image, [1, 1, 3])\\n\\n# randomly apply transformation with probability p.\\nimage = random_apply(color_jitter, image, p=0.8)\\nimage = random_apply(color_drop, image, p=0.2)\\nreturn image\\n\\nA pseudo-code for color distortion using Pytorch is as follows 12.\\n\\nfrom torchvision import transforms\\ndef get_color_distortion(s=1.0):\\n\\n# s is the strength of color distortion.\\ncolor_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\\nrnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\\nrnd_gray = transforms.RandomGrayscale(p=0.2)\\ncolor_distort = transforms.Compose([\\n\\nrnd_color_jitter,\\nrnd_gray])\\n\\n12Our code and results are based on Tensor\\xef\\xac\\x82ow, the Pytorch code here is a reference.\\n\\n\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nreturn color_distort\\n\\nGaussian blur This augmentation is in our default policy. We \\xef\\xac\\x81nd it helpful, as it improves our ResNet-50 trained for\\n100 epochs from 63.2% to 64.5%. We blur the image 50% of the time using a Gaussian kernel. We randomly sample\\n\\xcf\\x83 \\xe2\\x88\\x88 [0.1, 2.0], and the kernel size is set to be 10% of the image height/width.\\n\\nB. Additional Experimental Results\\nB.1. Batch Size and Training Steps\\n\\nFigure B.1 shows the top-5 accuracy on linear evaluation when trained with different batch sizes and training epochs. The\\nconclusion is very similar to top-1 accuracy shown before, except that the differences between different batch sizes and\\ntraining steps seems slightly smaller here.\\nIn both Figure 9 and Figure B.1, we use a linear scaling of learning rate similar to (Goyal et al., 2017) when training\\nwith different batch sizes. Although linear learning rate scaling is popular with SGD/Momentum optimizer, we \\xef\\xac\\x81nd a\\nsquare root learning rate scaling is more desirable with LARS optimizer. With square root learning rate scaling, we have\\nBatchSize, instead of LearningRate = 0.3 \\xc3\\x97 BatchSize/256 in the linear scaling case, but\\nthe learning rate is the same under both scaling methods when batch size of 4096 (our default batch size). A comparison is\\npresented in Table B.1, where we observe that square root learning rate scaling improves the performance for models trained\\nwith small batch sizes and in smaller number of epochs.\\n\\nLearningRate = 0.075 \\xc3\\x97 \\xe2\\x88\\x9a\\n\\nBatch size \\\\ Epochs\\n\\n100\\n\\n256\\n512\\n1024\\n2048\\n4096\\n8192\\n\\n57.5 / 62.8\\n60.7 / 63.8\\n62.8 / 64.3\\n64.0 / 64.7\\n64.6 / 64.5\\n64.8 / 64.8\\n\\n200\\n\\n61.9 / 64.3\\n64.0 / 65.6\\n65.3 / 66.1\\n66.1 / 66.8\\n66.5 / 66.8\\n66.6 / 67.0\\n\\n400\\n\\n64.7 / 65.7\\n66.2 / 66.7\\n67.2 / 67.2\\n68.1 / 67.9\\n68.2 / 68.0\\n67.8 / 68.3\\n\\n800\\n\\n66.6 / 66.5\\n67.8 / 67.4\\n68.5 / 68.3\\n68.9 / 68.8\\n68.9 / 69.1\\n69.0 / 69.1\\n\\nTable B.1. Linear evaluation (top-1) under different batch sizes and training epochs. On the left side of slash sign are models trained with\\nlinear LR scaling, and on the right are models trained with square root LR scaling. The result is bolded if it is more than 0.5% better.\\nSquare root LR scaling works better for smaller batch size trained in fewer epochs (with LARS optimizer).\\n\\nWe also train with larger batch size (up to 32K) and longer (up to 3200 epochs), with the square root learning rate scaling. A\\nshown in Figure B.2, the performance seems to saturate with a batch size of 8192, while training longer can still signi\\xef\\xac\\x81cantly\\nimprove the performance.\\n\\nFigure B.1. Linear evaluation (top-5) of ResNet-50 trained with\\ndifferent batch sizes and epochs. Each bar is a single run from\\nscratch. See Figure 9 for top-1 accuracy.\\n\\nFigure B.2. Linear evaluation (top-1) of ResNet-50 trained with\\ndifferent batch sizes and longer epochs. Here a square root learn-\\ning rate, instead of a linear one, is utilized.\\n\\n1002003004005006007008009001000Training epochs70.072.575.077.580.082.585.087.590.0Top 5Batch size25651210242048409681925010020040080016003200Training epochs60626466687072Top 1Batch size25651210242048409681921638432768\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nB.2. Broader composition of data augmentations further improves performance\\n\\nOur best results in the main text (Table 6 and 7) can be further improved when expanding the default augmentation policy to\\ninclude the following: (1) Sobel \\xef\\xac\\x81ltering, (2) additional color distortion (equalize, solarize), and (3) motion blur. For linear\\nevaluation protocol, the ResNet-50 models (1\\xc3\\x97, 2\\xc3\\x97, 4\\xc3\\x97) trained with broader data augmentations achieve 70.0 (+0.7), 74.4\\n(+0.2), 76.8 (+0.3), respectively.\\nTable B.2 shows ImageNet accuracy obtained by \\xef\\xac\\x81ne-tuning the SimCLR model (see Appendix B.5 for the details of\\n\\xef\\xac\\x81ne-tuning procedure). Interestingly, when \\xef\\xac\\x81ne-tuned on full (100%) ImageNet training set, our ResNet (4\\xc3\\x97) model\\nachieves 80.4% top-1 / 95.4% top-5 13, which is signi\\xef\\xac\\x81cantly better than that (78.4% top-1 / 94.2% top-5) of training from\\nscratch using the same set of augmentations (i.e. random crop and horizontal \\xef\\xac\\x82ip). For ResNet-50 (2\\xc3\\x97), \\xef\\xac\\x81ne-tuning our\\npre-trained ResNet-50 (2\\xc3\\x97) is also better than training from scratch (77.8% top-1 / 93.9% top-5). There is no improvement\\nfrom \\xef\\xac\\x81ne-tuning for ResNet-50.\\n\\nArchitecture\\n\\nResNet-50\\nResNet-50 (2\\xc3\\x97)\\nResNet-50 (4\\xc3\\x97)\\n\\n1%\\n\\nLabel fraction\\n\\n10%\\n\\n100%\\n\\nTop 1 Top 5 Top 1 Top 5 Top 1 Top 5\\n49.4\\n93.1\\n94.8\\n59.4\\n64.1\\n95.4\\n\\n88.1\\n91.2\\n92.8\\n\\n76.0\\n79.1\\n80.4\\n\\n76.6\\n83.7\\n86.6\\n\\n66.1\\n71.8\\n74.8\\n\\nTable B.2. Classi\\xef\\xac\\x81cation accuracy obtained by \\xef\\xac\\x81ne-tuning the SimCLR (which is pretrained with broader data augmentations) on 1%,\\n10% and full of ImageNet. As a reference, our ResNet-50 (4\\xc3\\x97) trained from scratch on 100% labels achieves 78.4% top-1 / 94.2% top-5.\\n\\nB.3. Effects of Longer Training for Supervised Models\\n\\nHere we perform experiments to see how training steps and stronger data augmentation affect supervised training. We test\\nResNet-50 and ResNet-50 (4\\xc3\\x97) under the same set of data augmentations (random crops, color distortion, 50% Gaussian\\nblur) as used in our unsupervised models. Figure B.3 shows the top-1 accuracy. We observe that there is no signi\\xef\\xac\\x81cant\\nbene\\xef\\xac\\x81t from training supervised models longer on ImageNet. Stronger data augmentation slightly improves the accuracy of\\nResNet-50 (4\\xc3\\x97) but does not help on ResNet-50. When stronger data augmentation is applied, ResNet-50 generally requires\\nlonger training (e.g. 500 epochs 14) to obtain the optimal result, while ResNet-50 (4\\xc3\\x97) does not bene\\xef\\xac\\x81t from longer training.\\n\\nModel\\n\\nTraining epochs\\n\\nResNet-50\\n\\nResNet-50 (4\\xc3\\x97)\\n\\n90\\n500\\n1000\\n90\\n500\\n1000\\n\\nTop 1\\n\\n+Color\\n75.6\\n76.5\\n75.2\\n78.9\\n78.4\\n78.2\\n\\nCrop\\n76.5\\n76.2\\n75.8\\n78.4\\n78.3\\n77.9\\n\\n+Color+Blur\\n\\n75.3\\n76.7\\n76.4\\n78.7\\n78.5\\n78.3\\n\\nTable B.3. Top-1 accuracy of supervised models trained longer under various data augmentation procedures (from the same set of data\\naugmentations for contrastive learning).\\n\\nB.4. Understanding The Non-Linear Projection Head\\nFigure B.3 shows the eigenvalue distribution of linear projection matrix W \\xe2\\x88\\x88 R2048\\xc3\\x972048 used to compute z = W h. This\\nmatrix has relatively few large eigenvalues, indicating that it is approximately low-rank.\\nFigure B.4 shows t-SNE (Maaten & Hinton, 2008) visualizations of h and z = g(h) for randomly selected 10 classes by\\nour best ResNet-50 (top-1 linear evaluation 69.3%). Classes represented by h are better separated compared to z.\\n\\n13It is 80.1% top-1 / 95.2% top-5 without broader augmentations for pretraining SimCLR.\\n14With AutoAugment (Cubuk et al., 2019), optimal test accuracy can be achieved between 900 and 500 epochs.\\n\\n\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\n(a) Y-axis in uniform scale.\\n\\n(b) Y-axis in log scale.\\n\\nFigure B.3. Squared real eigenvalue distribution of linear projection\\nmatrix W \\xe2\\x88\\x88 R2048\\xc3\\x972048 used to compute g(h) = W h.\\n\\n(a) h\\n\\n(b) z = g(h)\\n\\nFigure B.4. t-SNE visualizations of hidden vectors of images from\\na randomly selected 10 classes in the validation set.\\n\\nB.5. Semi-supervised Learning via Fine-Tuning\\nFine-tuning Procedure We \\xef\\xac\\x81ne-tune using the Nesterov momentum optimizer with a batch size of 4096, momentum of\\n0.9, and a learning rate of 0.8 (following LearningRate = 0.05\\xc3\\x97 BatchSize/256) without warmup. Only random cropping\\n(with random left-to-right \\xef\\xac\\x82ipping and resizing to 224x224) is used for preprocessing. We do not use any regularization\\n(including weight decay). For 1% labeled data we \\xef\\xac\\x81ne-tune for 60 epochs, and for 10% labeled data we \\xef\\xac\\x81ne-tune for 30\\nepochs. For the inference, we resize the given image to 256x256, and take a single center crop of 224x224.\\nTable B.4 shows the comparisons of top-1 accuracy for different methods for semi-supervised learning. Our models\\nsigni\\xef\\xac\\x81cantly improve state-of-the-art.\\n\\nMethod\\n\\nArchitecture\\n\\nLabel fraction\\n1%\\n10%\\n\\nTop 1\\n\\n25.4\\n\\nResNet-50\\n\\nResNet-50\\nResNet-50\\n\\nSupervised baseline\\nMethods using label-propagation:\\nUDA (w. RandAug)\\nFixMatch (w. RandAug)\\nS4L (Rot+VAT+Ent. Min.) ResNet-50 (4\\xc3\\x97)\\nMethods using self-supervised representation learning only:\\nCPC v2\\nSimCLR (ours)\\nSimCLR (ours)\\nSimCLR (ours)\\n\\nResNet-161(\\xe2\\x88\\x97)\\nResNet-50\\nResNet-50 (2\\xc3\\x97)\\nResNet-50 (4\\xc3\\x97)\\n\\n52.7\\n48.3\\n58.5\\n63.0\\n\\n-\\n-\\n-\\n\\n56.4\\n\\n68.8\\n71.5\\n73.2\\n\\n73.1\\n65.6\\n71.7\\n74.4\\n\\nTable B.4. ImageNet top-1 accuracy of models trained with few labels. See Table 7 for top-5 accuracy.\\n\\nB.6. Linear Evaluation\\n\\nFor linear evaluation, we follow similar procedure as \\xef\\xac\\x81ne-tuning (described in Appendix B.5), except that a larger learning\\nrate of 1.6 (following LearningRate = 0.1 \\xc3\\x97 BatchSize/256) and longer training of 90 epochs. Alternatively, using LARS\\noptimizer with the pretraining hyper-parameters also yield similar results. Furthermore, we \\xef\\xac\\x81nd that attaching the linear\\nclassi\\xef\\xac\\x81er on top of the base encoder (with a stop_gradient on the input to linear classi\\xef\\xac\\x81er to prevent the label information\\nfrom in\\xef\\xac\\x82uencing the encoder) and train them simultaneously during the pretraining achieves similar performance.\\n\\nB.7. Correlation Between Linear Evaluation and Fine-Tuning\\n\\nHere we study the correlation between linear evaluation and \\xef\\xac\\x81ne-tuning under different settings of training step and network\\narchitecture.\\nFigure B.5 shows linear evaluation versus \\xef\\xac\\x81ne-tuning when training epochs of a ResNet-50 (using batch size of 4096) are\\nvaried from 50 to 3200 as in Figure B.2. While they are almost linearly correlated, it seems \\xef\\xac\\x81ne-tuning on a small fraction\\n\\n0500100015002000Ranking0246810121416Squared eigenvalue0500100015002000Ranking10\\xe2\\x88\\x921110\\xe2\\x88\\x92910\\xe2\\x88\\x92710\\xe2\\x88\\x92510\\xe2\\x88\\x92310\\xe2\\x88\\x921101Squared eigenvalue\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nof labels bene\\xef\\xac\\x81ts more from training longer.\\n\\nFigure B.5. Top-1 accuracy of models trained in different epochs (from Figure B.2), under linear evaluation and \\xef\\xac\\x81ne-tuning.\\n\\nFigure B.6 shows shows linear evaluation versus \\xef\\xac\\x81ne-tuning for different architectures of choice.\\n\\nFigure B.6. Top-1 accuracy of different architectures under linear evaluation and \\xef\\xac\\x81ne-tuning.\\n\\nB.8. Transfer Learning\\n\\nWe evaluated the performance of our self-supervised representation for transfer learning in two settings: linear evaluation,\\nwhere a logistic regression classi\\xef\\xac\\x81er is trained to classify a new dataset based on the self-supervised representation learned\\non ImageNet, and \\xef\\xac\\x81ne-tuning, where we allow all weights to vary during training. In both cases, we follow the approach\\ndescribed by Kornblith et al. (2019), although our preprocessing differs slightly.\\n\\nB.8.1. METHODS\\nDatasets We investigated transfer learning performance on the Food-101 dataset (Bossard et al., 2014), CIFAR-10\\nand CIFAR-100 (Krizhevsky & Hinton, 2009), Birdsnap (Berg et al., 2014), the SUN397 scene dataset (Xiao et al.,\\n2010), Stanford Cars (Krause et al., 2013), FGVC Aircraft (Maji et al., 2013), the PASCAL VOC 2007 classi\\xef\\xac\\x81cation\\ntask (Everingham et al., 2010), the Describable Textures Dataset (DTD) (Cimpoi et al., 2014), Oxford-IIIT Pets (Parkhi et al.,\\n2012), Caltech-101 (Fei-Fei et al., 2004), and Oxford 102 Flowers (Nilsback & Zisserman, 2008). We follow the evaluation\\nprotocols in the papers introducing these datasets, i.e., we report top-1 accuracy for Food-101, CIFAR-10, CIFAR-100,\\nBirdsnap, SUN397, Stanford Cars, and DTD; mean per-class accuracy for FGVC Aircraft, Oxford-IIIT Pets, Caltech-101,\\nand Oxford 102 Flowers; and the 11-point mAP metric as de\\xef\\xac\\x81ned in Everingham et al. (2010) for PASCAL VOC 2007. For\\nDTD and SUN397, the dataset creators de\\xef\\xac\\x81ned multiple train/test splits; we report results only for the \\xef\\xac\\x81rst split. Caltech-101\\nde\\xef\\xac\\x81nes no train/test split, so we randomly chose 30 images per class and test on the remainder, for fair comparison with\\nprevious work (Donahue et al., 2014; Simonyan & Zisserman, 2014).\\nWe used the validation sets speci\\xef\\xac\\x81ed by the dataset creators to select hyperparameters for FGVC Aircraft, PASCAL VOC\\n\\n6264666870Linear eval35.037.540.042.545.047.550.0Fine-tuning on 1%6264666870Linear eval60626466Fine-tuning on 10%5055606570Linear eval303336394245485154Finetune (1%)Width1x2x4xDepth1834501011525055606570Linear eval51545760636669Finetune (10%)Width1x2x4xDepth183450101152\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\n2007, DTD, and Oxford 102 Flowers. For other datasets, we held out a subset of the training set for validation while\\nperforming hyperparameter tuning. After selecting the optimal hyperparameters on the validation set, we retrained the\\nmodel using the selected parameters using all training and validation images. We report accuracy on the test set.\\n\\nTransfer Learning via a Linear Classi\\xef\\xac\\x81er We trained an (cid:96)2-regularized multinomial logistic regression classi\\xef\\xac\\x81er on\\nfeatures extracted from the frozen pretrained network. We used L-BFGS to optimize the softmax cross-entropy objective\\nand we did not apply data augmentation. As preprocessing, all images were resized to 224 pixels along the shorter side\\nusing bicubic resampling, after which we took a 224 \\xc3\\x97 224 center crop. We selected the (cid:96)2 regularization parameter from a\\nrange of 45 logarithmically spaced values between 10\\xe2\\x88\\x926 and 105.\\n\\nTransfer Learning via Fine-Tuning We \\xef\\xac\\x81ne-tuned the entire network using the weights of the pretrained network as\\ninitialization. We trained for 20,000 steps at a batch size of 256 using SGD with Nesterov momentum with a momentum\\nparameter of 0.9. We set the momentum parameter for the batch normalization statistics to max(1 \\xe2\\x88\\x92 10/s, 0.9) where s is\\nthe number of steps per epoch. As data augmentation during \\xef\\xac\\x81ne-tuning, we performed only random crops with resize and\\n\\xef\\xac\\x82ips; in contrast to pretraining, we did not perform color augmentation or blurring. At test time, we resized images to 256\\npixels along the shorter side and took a 224 \\xc3\\x97 224 center crop. (Additional accuracy improvements may be possible with\\nfurther optimization of data augmentation, particularly on the CIFAR-10 and CIFAR-100 datasets.) We selected the learning\\nrate and weight decay, with a grid of 7 logarithmically spaced learning rates between 0.0001 and 0.1 and 7 logarithmically\\nspaced values of weight decay between 10\\xe2\\x88\\x926 and 10\\xe2\\x88\\x923, as well as no weight decay. We divide these values of weight decay\\nby the learning rate.\\n\\nTraining from Random Initialization We trained the network from random initialization using the same procedure\\nas for \\xef\\xac\\x81ne-tuning, but for longer, and with an altered hyperparameter grid. We chose hyperparameters from a grid of 7\\nlogarithmically spaced learning rates between 0.001 and 1.0 and 8 logarithmically spaced values of weight decay between\\n10\\xe2\\x88\\x925 and 10\\xe2\\x88\\x921.5. Importantly, our random initialization baselines are trained for 40,000 steps, which is suf\\xef\\xac\\x81ciently long to\\nachieve near-maximal accuracy, as demonstrated in Figure 8 of Kornblith et al. (2019).\\nOn Birdsnap, there are no statistically signi\\xef\\xac\\x81cant differences among methods, and on Food-101, Stanford Cars, and FGVC\\nAircraft datasets, \\xef\\xac\\x81ne-tuning provides only a small advantage over training from random initialization. However, on the\\nremaining 8 datasets, pretraining has clear advantages.\\n\\nSupervised Baselines We compare against architecturally identical ResNet models trained on ImageNet with standard\\ncross-entropy loss. These models are trained with the same data augmentation as our self-supervised models (crops, strong\\ncolor augmentation, and blur) and are also trained for 1000 epochs. We found that, although stronger data augmentation and\\nlonger training time do not bene\\xef\\xac\\x81t accuracy on ImageNet, these models performed signi\\xef\\xac\\x81cantly better than a supervised\\nbaseline trained for 90 epochs and ordinary data augmentation for linear evaluation on a subset of transfer datasets. The\\nsupervised ResNet-50 baseline achieves 76.3% top-1 accuracy on ImageNet, vs. 69.3% for the self-supervised counterpart,\\nwhile the ResNet-50 (4\\xc3\\x97) baseline achieves 78.3%, vs. 76.5% for the self-supervised model.\\n\\nStatistical Signi\\xef\\xac\\x81cance Testing We test for the signi\\xef\\xac\\x81cance of differences between model with a permutation test. Given\\npredictions of two models, we generate 100,000 samples from the null distribution by randomly exchanging predictions\\nfor each example and computing the difference in accuracy after performing this randomization. We then compute the\\npercentage of samples from the null distribution that are more extreme than the observed difference in predictions. For top-1\\naccuracy, this procedure yields the same result as the exact McNemar test. The assumption of exchangeability under the null\\nhypothesis is also valid for mean per-class accuracy, but not when computing average precision curves. Thus, we perform\\nsigni\\xef\\xac\\x81cance testing for a difference in accuracy on VOC 2007 rather than a difference in mAP. A caveat of this procedure is\\nthat it does not consider run-to-run variability when training the models, only variability arising from using a \\xef\\xac\\x81nite sample\\nof images for evaluation.\\n\\nB.8.2. RESULTS WITH STANDARD RESNET\\nThe ResNet-50 (4\\xc3\\x97) results shown in Table 8 of the text show no clear advantage to the supervised or self-supervised models.\\nWith the narrower ResNet-50 architecture, however, supervised learning maintains a clear advantage over self-supervised\\nlearning. The supervised ResNet-50 model outperforms the self-supervised model on all datasets with linear evaluation,\\nand most (10 of 12) datasets with \\xef\\xac\\x81ne-tuning. The weaker performance of the ResNet model compared to the ResNet (4\\xc3\\x97)\\n\\n\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nFood CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers\\n\\nLinear evaluation:\\nSimCLR (ours) 68.4\\n72.3\\nSupervised\\nFine-tuned:\\nSimCLR (ours) 88.2\\n88.3\\nSupervised\\nRandom init\\n86.9\\n\\n90.6\\n93.6\\n\\n97.7\\n97.5\\n95.9\\n\\n71.6\\n78.3\\n\\n85.9\\n86.4\\n80.2\\n\\n37.4\\n53.7\\n\\n75.9\\n75.8\\n76.1\\n\\n58.8\\n61.9\\n\\n63.5\\n64.3\\n53.6\\n\\n50.3\\n66.7\\n\\n91.3\\n92.1\\n91.4\\n\\n50.3\\n61.0\\n\\n88.1\\n86.0\\n85.9\\n\\n80.5\\n82.8\\n\\n84.1\\n85.0\\n67.3\\n\\n74.5 83.6\\n74.9 91.5\\n\\n73.2 89.2\\n74.6 92.1\\n64.8 81.5\\n\\n90.3\\n94.5\\n\\n92.1\\n93.3\\n72.6\\n\\n91.2\\n94.7\\n\\n97.0\\n97.6\\n92.0\\n\\nTable B.5. Comparison of transfer learning performance of our self-supervised approach with supervised baselines across 12 natural\\nimage datasets, using ImageNet-pretrained ResNet models. See also Figure 8 for results with the ResNet (4\\xc3\\x97) architecture.\\n\\nmodel may relate to the accuracy gap between the supervised and self-supervised models on ImageNet. The self-supervised\\nResNet gets 69.3% top-1 accuracy, 6.8% worse than the supervised model in absolute terms, whereas the self-supervised\\nResNet (4\\xc3\\x97) model gets 76.5%, which is only 1.8% worse than the supervised model.\\n\\nB.9. CIFAR-10\\n\\nWhile we focus on using ImageNet as the main dataset for pretraining our unsupervised model, our method also works with\\nother datasets. We demonstrate it by testing on CIFAR-10 as follows.\\n\\nSetup As our goal is not to optimize CIFAR-10 performance, but rather to provide further con\\xef\\xac\\x81rmation of our observations\\non ImageNet, we use the same architecture (ResNet-50) for CIFAR-10 experiments. Because CIFAR-10 images are much\\nsmaller than ImageNet images, we replace the \\xef\\xac\\x81rst 7x7 Conv of stride 2 with 3x3 Conv of stride 1, and also remove the \\xef\\xac\\x81rst\\nmax pooling operation. For data augmentation, we use the same Inception crop (\\xef\\xac\\x82ip and resize to 32x32) as ImageNet,15 and\\ncolor distortion (strength=0.5), leaving out Gaussian blur. We pretrain with learning rate in {0.5, 1.0, 1.5}, temperature in\\n{0.1, 0.5, 1.0}, and batch size in {256, 512, 1024, 2048, 4096}. The rest of the settings (including optimizer, weight decay,\\netc.) are the same as our ImageNet training.\\nOur best model trained with batch size 1024 can achieve a linear evaluation accuracy of 94.0%, compared to 95.1% from the\\nsupervised baseline using the same architecture and batch size. The best self-supervised model that reports linear evaluation\\nresult on CIFAR-10 is AMDIM (Bachman et al., 2019), which achieves 91.2% with a model 25\\xc3\\x97 larger than ours. We note\\nthat our model can be improved by incorporating extra data augmentations as well as using a more suitable base network.\\n\\nPerformance under different batch sizes and training steps Figure B.7 shows the linear evaluation performance under\\ndifferent batch sizes and training steps. The results are consistent with our observations on ImageNet, although the largest\\nbatch size of 4096 seems to cause a small degradation in performance on CIFAR-10.\\n\\nFigure B.7. Linear evaluation of ResNet-50 (with ad-\\njusted stem) trained with different batch size and\\nepochs on CIFAR-10 dataset. Each bar is averaged\\nover 3 runs with different learning rates (0.5, 1.0,\\n1.5) and temperature \\xcf\\x84 = 0.5. Error bar denotes\\nstandard deviation.\\n\\n15It is worth noting that, although CIFAR-10 images are much smaller than ImageNet images and image size does not differ among\\n\\nexamples, cropping with resizing is still a very effective augmentation for contrastive learning.\\n\\n1002003004005006007008009001000Training epochs8082848688909294Top 1Batch size256512102420484096\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nOptimal temperature under different batch sizes Figure B.8 shows the linear evaluation of model trained with three\\ndifferent temperatures under various batch sizes. We \\xef\\xac\\x81nd that when training to convergence (e.g. training epochs > 300), the\\noptimal temperature in {0.1, 0.5, 1.0} is 0.5 and seems consistent regardless of the batch sizes. However, the performance\\nwith \\xcf\\x84 = 0.1 improves as batch size increases, which may suggest a small shift of optimal temperature towards 0.1.\\n\\n(a) Training epochs \\xe2\\x89\\xa4 300\\n\\n(b) Training epochs > 300\\n\\nFigure B.8. Linear evaluation of the model (ResNet-50) trained with three temperatures on different batch sizes on CIFAR-10. Each bar is\\naveraged over multiple runs with different learning rates and total train epochs. Error bar denotes standard deviation.\\n\\nB.10. Tuning For Other Loss Functions\\n\\nThe learning rate that works best for NT-Xent loss may not be a good learning rate for other loss functions. To ensure a\\nfair comparison, we also tune hyperparameters for both margin loss and logistic loss. Speci\\xef\\xac\\x81cally, we tune learning rate\\nin {0.01, 0.1, 0.3, 0.5, 1.0} for both loss functions. We further tune the margin in {0, 0.4, 0.8, 1.6} for margin loss, the\\ntemperature in {0.1, 0.2, 0.5, 1.0} for logistic loss. For simplicity, we only consider the negatives from one augmentation\\nview (instead of both sides), which slightly impairs performance but ensures fair comparison.\\n\\nC. Further Comparison to Related Methods\\nAs we have noted in the main text, most individual components of SimCLR have appeared in previous work, and the\\nimproved performance is a result of a combination of these design choices. Table C.1 provides a high-level comparison of\\nthe design choices of our method with those of previous methods. Compared with previous work, our design choices are\\ngenerally simpler.\\n\\nData Augmentation\\nCustom\\n\\nModel\\nCPC v2\\nAMDIM Fast AutoAug.\\nCMC\\nFast AutoAug.\\nCrop+color\\nMoCo\\nPIRL\\nCrop+color\\nSimCLR Crop+color+blur\\n\\nBase Encoder\\nResNet-161 (modi\\xef\\xac\\x81ed)\\nCustom ResNet\\nResNet-50 (2\\xc3\\x97, L+ab)\\nResNet-50 (4\\xc3\\x97)\\nResNet-50 (2\\xc3\\x97)\\nResNet-50 (4\\xc3\\x97)\\n\\nLoss\\nXent\\n\\nProjection Head\\nPixelCNN\\nNon-linear MLP Xent w/ clip,reg\\nXent w/ (cid:96)2, \\xcf\\x84\\nLinear layer\\nXent w/ (cid:96)2, \\xcf\\x84\\nLinear layer\\nLinear layer\\nXent w/ (cid:96)2, \\xcf\\x84\\nNon-linear MLP Xent w/ (cid:96)2, \\xcf\\x84\\n\\nBatch Size\\n512#\\n1008#\\n156\\xe2\\x88\\x97\\n256\\xe2\\x88\\x97\\n1024\\xe2\\x88\\x97\\n4096\\n\\nTrain Epochs\\n\\xe2\\x88\\xbc200\\n150\\n280\\n200\\n800\\n1000\\n\\nTable C.1. A high-level comparison of design choices and training setup (for best result on ImageNet) for each method. Note that\\ndescriptions provided here are general; even when they match for two methods, formulations and implementations may differ (e.g. for\\ncolor augmentation). Refer to the original papers for more details. #Examples are split into multiple patches, which enlarges the effective\\nbatch size. \\xe2\\x88\\x97A memory bank is employed.\\n\\nIn below, we provide an in-depth comparison of our method to the recently proposed contrastive representation learning\\nmethods:\\n\\n\\xe2\\x80\\xa2 DIM/AMDIM (Hjelm et al., 2018; Bachman et al., 2019) achieve global-to-local/local-to-neighbor prediction by\\npredicting the middle layer of ConvNet. The ConvNet is a ResNet that has bewen modi\\xef\\xac\\x81ed to place signi\\xef\\xac\\x81cant\\nconstraints on the receptive \\xef\\xac\\x81elds of the network (e.g. replacing many 3x3 Convs with 1x1 Convs). In our framework,\\nwe decouple the prediction task and encoder architecture, by random cropping (with resizing) and using the \\xef\\xac\\x81nal\\n\\n256512102420484096Batch size75.077.580.082.585.087.590.092.595.0Top 1Temperature0.10.51.0256512102420484096Batch size909192939495Top 1Temperature0.10.51.0\\x0cA Simple Framework for Contrastive Learning of Visual Representations\\n\\nrepresentations of two augmented views for prediction, so we can use standard and more powerful ResNets. Our\\nNT-Xent loss function leverages normalization and temperature to restrict the range of similarity scores, whereas they\\nuse a tanh function with regularization. We use a simpler data augmentation policy, while they use FastAutoAugment\\nfor their best result.\\n\\n\\xe2\\x80\\xa2 CPC v1 and v2 (Oord et al., 2018; H\\xc3\\xa9naff et al., 2019) de\\xef\\xac\\x81ne the context prediction task using a deterministic strategy\\nto split examples into patches, and a context aggregation network (a PixelCNN) to aggregate these patches. The base\\nencoder network sees only patches, which are considerably smaller than the original image. We decouple the prediction\\ntask and the encoder architecture, so we do not require a context aggregation network, and our encoder can look at the\\nimages of wider spectrum of resolutions. In addition, we use the NT-Xent loss function, which leverages normalization\\nand temperature, whereas they use an unnormalized cross-entropy-based objective. We use simpler data augmentation.\\n\\xe2\\x80\\xa2 InstDisc, MoCo, PIRL (Wu et al., 2018; He et al., 2019; Misra & van der Maaten, 2019) generalize the Exemplar\\napproach originally proposed by Dosovitskiy et al. (2014) and leverage an explicit memory bank. We do not use a\\nmemory bank; we \\xef\\xac\\x81nd that, with a larger batch size, in-batch negative example sampling suf\\xef\\xac\\x81ces. We also utilize a\\nnonlinear projection head, and use the representation before the projection head. Although we use similar types of\\naugmentations (e.g., random crop and color distortion), we expect speci\\xef\\xac\\x81c parameters may be different.\\n\\n\\xe2\\x80\\xa2 CMC (Tian et al., 2019) uses a separated network for each view, while we simply use a single network shared for all\\nrandomly augmented views. The data augmentation, projection head and loss function are also different. We use larger\\nbatch size instead of a memory bank.\\n\\n\\xe2\\x80\\xa2 Whereas Ye et al. (2019) maximize similarity between augmented and unaugmented copies of the same image, we\\napply data augmentation symmetrically to both branches of our framework (Figure 2). We also apply a nonlinear\\nprojection on the output of base feature network, and use the representation before projection network, whereas Ye\\net al. (2019) use the linearly projected \\xef\\xac\\x81nal hidden vector as the representation. When training with large batch sizes\\nusing multiple accelerators, we use global BN to avoid shortcuts that can greatly decrease representation quality.\\n\\n\\x0c'\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "sharing-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_new = str(text)\n",
    "text_new = text_new.replace(\"\\\\n\", \" \")\n",
    "text_new = text_new.replace('\\\\x', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "duplicate-regulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'A Simple Framework for Contrastive Learning of Visual Representations  Ting Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton 1  0 2 0 2    l u J    1      ]  G L . s c [      3 v 9 0 7 5 0  .  2 0 0 2 : v i X r a  Abstract  This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self- supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in deefac81ning effective predictive tasks, (2) introducing a learn- able nonlinear transformation between the repre- sentation and the contrastive loss substantially im- proves the quality of the learned representations, and (3) contrastive learning beneefac81ts from larger batch sizes and more training steps compared to supervised learning. By combining these efac81ndings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classiefac81er trained on self-supervised representations learned by Sim- CLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of- the-art, matching the performance of a supervised ResNet-50. When efac81ne-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outper- forming AlexNet with 100c397 fewer labels. 1  1. Introduction Learning effective visual representations without human supervision is a long-standing problem. Most mainstream approaches fall into one of two classes: generative or dis- criminative. Generative approaches learn to generate or otherwise model pixels in the input space (Hinton et al., 2006; Kingma & Welling, 2013; Goodfellow et al., 2014).  1Google Research, Brain Team. Correspondence to: Ting Chen  <iamtingchen@google.com>.  Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).  1Code available at https://github.com/google-research/simclr.  Figure 1. ImageNet Top-1 accuracy of linear classiefac81ers trained on representations learned with different self-supervised meth- ods (pretrained on ImageNet). Gray cross indicates supervised ResNet-50. Our method, SimCLR, is shown in bold.  However, pixel-level generation is computationally expen- sive and may not be necessary for representation learning. Discriminative approaches learn representations using objec- tive functions similar to those used for supervised learning, but train networks to perform pretext tasks where both the in- puts and labels are derived from an unlabeled dataset. Many such approaches have relied on heuristics to design pretext tasks (Doersch et al., 2015; Zhang et al., 2016; Noroozi & Favaro, 2016; Gidaris et al., 2018), which could limit the generality of the learned representations. Discriminative approaches based on contrastive learning in the latent space have recently shown great promise, achieving state-of-the- art results (Hadsell et al., 2006; Dosovitskiy et al., 2014; Oord et al., 2018; Bachman et al., 2019). In this work, we introduce a simple framework for con- trastive learning of visual representations, which we call SimCLR. Not only does SimCLR outperform previous work (Figure 1), but it is also simpler, requiring neither special- ized architectures (Bachman et al., 2019; Hc3a9naff et al., 2019) nor a memory bank (Wu et al., 2018; Tian et al., 2019; He et al., 2019; Misra & van der Maaten, 2019). In order to understand what enables good contrastive repre- sentation learning, we systematically study the major com- ponents of our framework and show that:  2550100200400626Number of Parameters (Millions)5560657075ImageNet Top-1 Accuracy (%)InstDiscRotationBigBiGANLACPCv2CPCv2-LCMCAMDIMMoCoMoCo (2x)MoCo (4x)PIRLPIRL-ens.PIRL-c2xSimCLRSimCLR (2x)SimCLR (4x)Supervised0cA Simple Framework for Contrastive Learning of Visual Representations  e280a2 Composition of multiple data augmentation operations is crucial in deefac81ning the contrastive prediction tasks that yield effective representations. In addition, unsupervised contrastive learning beneefac81ts from stronger data augmen- tation than supervised learning.  e280a2 Introducing a learnable nonlinear transformation be- tween the representation and the contrastive loss substan- tially improves the quality of the learned representations. e280a2 Representation learning with contrastive cross entropy loss beneefac81ts from normalized embeddings and an appro- priately adjusted temperature parameter.  e280a2 Contrastive learning beneefac81ts from larger batch sizes and longer training compared to its supervised counterpart. Like supervised learning, contrastive learning beneefac81ts from deeper and wider networks.  We combine these efac81ndings to achieve a new state-of-the-art in self-supervised and semi-supervised learning on Ima- geNet ILSVRC-2012 (Russakovsky et al., 2015). Under the linear evaluation protocol, SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art (Hc3a9naff et al., 2019). When efac81ne-tuned with only 1% of the ImageNet labels, SimCLR achieves 85.8% top-5 accuracy, a relative improvement of 10% (Hc3a9naff et al., 2019). When efac81ne-tuned on other natural image classiefac81ca- tion datasets, SimCLR performs on par with or better than a strong supervised baseline (Kornblith et al., 2019) on 10 out of 12 datasets.  2. Method 2.1. The Contrastive Learning Framework  Inspired by recent contrastive learning algorithms (see Sec- tion 7 for an overview), SimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space. As illustrated in Figure 2, this framework comprises the following four major components. e280a2 A stochastic data augmentation module that transforms any given data example randomly resulting in two cor- related views of the same example, denoted cb9cxi and cb9cxj, which we consider as a positive pair. In this work, we sequentially apply three simple augmentations: random cropping followed by resize back to the original size, ran- dom color distortions, and random Gaussian blur. As shown in Section 3, the combination of random crop and color distortion is crucial to achieve a good performance. e280a2 A neural network base encoder f (c2b7) that extracts repre- sentation vectors from augmented data examples. Our framework allows various choices of the network archi- tecture without any constraints. We opt for simplicity and adopt the commonly used ResNet (He et al., 2016)  zi g(c2b7)  hi f (c2b7)  cb9cxi  Maximize agreement  e28690e28892 Representatione28892e28692  te288bcT  x  T  e288bc  (cid:48) t  zj g(c2b7)  hj  f (c2b7)  cb9cxj  Figure 2. A simple framework for contrastive learning of visual representations. Two separate data augmentation operators are sampled from the same family of augmentations (t e288bc T and t(cid:48) e288bc T ) and applied to each data example to obtain two correlated views. A base encoder network f (c2b7) and a projection head g(c2b7) are trained to maximize agreement using a contrastive loss. After training is completed, we throw away the projection head g(c2b7) and use encoder f (c2b7) and representation h for downstream tasks.  to obtain hi = f ( cb9cxi) = ResNet( cb9cxi) where hi e28888 Rd is the output after the average pooling layer.  e280a2 A small neural network projection head g(c2b7) that maps representations to the space where contrastive loss is applied. We use a MLP with one hidden layer to obtain zi = g(hi) = W (2)cf83(W (1)hi) where cf83 is a ReLU non- linearity. As shown in section 4, we efac81nd it beneefac81cial to deefac81ne the contrastive loss on zie28099s rather than hie28099s.  e280a2 A contrastive loss function deefac81ned for a contrastive pre- diction task. Given a set { cb9cxk} including a positive pair of examples cb9cxi and cb9cxj, the contrastive prediction task aims to identify cb9cxj in { cb9cxk}k(cid:54)=i for a given cb9cxi.  We randomly sample a minibatch of N examples and deefac81ne the contrastive prediction task on pairs of augmented exam- ples derived from the minibatch, resulting in 2N data points. We do not sample negative examples explicitly. Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N e28892 1) augmented examples within a minibatch as negative examples. Let sim(u, v) = u(cid:62)v/(cid:107)u(cid:107)(cid:107)v(cid:107) de- note the dot product between (cid:96)2 normalized u and v (i.e. cosine similarity). Then the loss function for a positive pair of examples (i, j) is deefac81ned as  (cid:96)i,j = e28892 log  exp(sim(zi, zj)/cf84 ) 1[k(cid:54)=i] exp(sim(zi, zk)/cf84 )  ,  (1)  where 1[k(cid:54)=i] e28888 {0, 1} is an indicator function evaluating to 1 iff k (cid:54)= i and cf84 denotes a temperature parameter. The efac81- nal loss is computed across all positive pairs, both (i, j) and (j, i), in a mini-batch. This loss has been used in previous work (Sohn, 2016; Wu et al., 2018; Oord et al., 2018); for convenience, we term it NT-Xent (the normalized temperature-scaled cross entropy loss).  (cid:80)2N  k=1  0cA Simple Framework for Contrastive Learning of Visual Representations  Algorithm 1 SimCLRe28099s main learning algorithm.  input: batch size N, constant cf84, structure of f, g, T . for sampled minibatch {xk}N for all k e28888 {1, . . . , N} do  k=1 do  draw two augmentation functions te288bcT , t(cid:48)e288bcT # the efac81rst augmentation cb9cx2ke288921 = t(xk) h2ke288921 = f ( cb9cx2ke288921) z2ke288921 = g(h2ke288921) # the second augmentation cb9cx2k = t(cid:48)(xk) h2k = f ( cb9cx2k) z2k = g(h2k)  # representation # projection  # representation # projection  end for for all i e28888 {1, . . . , 2N} and j e28888 {1, . . . , 2N} do  si,j = z(cid:62)  i zj/((cid:107)zi(cid:107)(cid:107)zj(cid:107)) (cid:80)N k=1 [(cid:96)(2ke288921, 2k) + (cid:96)(2k, 2ke288921)]  end for (cid:80)2N deefac81ne (cid:96)(i, j) as (cid:96)(i, j) =e28892 log L = 1 update networks f and g to minimize L  2N  k=1  exp(si,j /cf84 ) 1[k(cid:54)=i] exp(si,k/cf84 )  # pairwise similarity  end for return encoder network f (c2b7), and throw away g(c2b7)  Algorithm 1 summarizes the proposed method.  2.2. Training with Large Batch Size  To keep it simple, we do not train the model with a memory bank (Wu et al., 2018; He et al., 2019). Instead, we vary the training batch size N from 256 to 8192. A batch size of 8192 gives us 16382 negative examples per positive pair from both augmentation views. Training with large batch size may be unstable when using standard SGD/Momentum with linear learning rate scaling (Goyal et al., 2017). To stabilize the training, we use the LARS optimizer (You et al., 2017) for all batch sizes. We train our model with Cloud TPUs, using 32 to 128 cores depending on the batch size.2 Global BN. Standard ResNets use batch normaliza- tion (Ioffe & Szegedy, 2015). In distributed training with data parallelism, the BN mean and variance are typically aggregated locally per device. In our contrastive learning, as positive pairs are computed in the same device, the model can exploit the local information leakage to improve pre- diction accuracy without improving representations. We ad- dress this issue by aggregating BN mean and variance over all devices during the training. Other approaches include shufefac82ing data examples across devices (He et al., 2019), or replacing BN with layer norm (Hc3a9naff et al., 2019).  2With 128 TPU v3 cores, it takes e288bc1.5 hours to train our  ResNet-50 with a batch size of 4096 for 100 epochs.  A B  D  C  (a) Global and local views.  (b) Adjacent views.  Figure 3. Solid rectangles are images, dashed rectangles are ran- dom crops. By randomly cropping images, we sample contrastive prediction tasks that include global to local view (B e28692 A) or adjacent view (D e28692 C) prediction.  2.3. Evaluation Protocol  Here we lay out the protocol for our empirical studies, which aim to understand different design choices in our framework. Dataset and Metrics. Most of our study for unsupervised pretraining (learning encoder network f without labels) is done using the ImageNet ILSVRC-2012 dataset (Rus- sakovsky et al., 2015). Some additional pretraining experi- ments on CIFAR-10 (Krizhevsky & Hinton, 2009) can be found in Appendix B.9. We also test the pretrained results on a wide range of datasets for transfer learning. To evalu- ate the learned representations, we follow the widely used linear evaluation protocol (Zhang et al., 2016; Oord et al., 2018; Bachman et al., 2019; Kolesnikov et al., 2019), where a linear classiefac81er is trained on top of the frozen base net- work, and test accuracy is used as a proxy for representation quality. Beyond linear evaluation, we also compare against state-of-the-art on semi-supervised and transfer learning. Default setting. Unless otherwise speciefac81ed, for data aug- mentation we use random crop and resize (with random efac82ip), color distortions, and Gaussian blur (for details, see Appendix A). We use ResNet-50 as the base encoder net- work, and a 2-layer MLP projection head to project the representation to a 128-dimensional latent space. As the loss, we use NT-Xent, optimized using LARS with learning rate of 4.8 (= 0.3 c397 BatchSize/256) and weight decay of 10e288926. We train at batch size 4096 for 100 epochs.3 Fur- thermore, we use linear warmup for the efac81rst 10 epochs, and decay the learning rate with the cosine decay schedule without restarts (Loshchilov & Hutter, 2016).  3. Data Augmentation for Contrastive  Representation Learning  Data augmentation deefac81nes predictive tasks. While data augmentation has been widely used in both supervised and unsupervised representation learning (Krizhevsky et al.,  3Although max performance is not reached in 100 epochs, rea- sonable results are achieved, allowing fair and efefac81cient ablations.  0cA Simple Framework for Contrastive Learning of Visual Representations  (a) Original  (b) Crop and resize  (c) Crop, resize (and efac82ip) (d) Color distort. (drop)  (e) Color distort. (jitter)  (f) Rotate {90e297a6, 180e297a6, 270e297a6}  (g) Cutout  (h) Gaussian noise  (i) Gaussian blur  (j) Sobel efac81ltering  Figure 4. Illustrations of the studied data augmentation operators. Each augmentation can transform data stochastically with some internal parameters (e.g. rotation degree, noise level). Note that we only test these operators in ablation, the augmentation policy used to train our models only includes random crop (with efac82ip and resize), color distortion, and Gaussian blur. (Original image cc-by: Von.grzanka)  2012; Hc3a9naff et al., 2019; Bachman et al., 2019), it has not been considered as a systematic way to deefac81ne the con- trastive prediction task. Many existing approaches deefac81ne contrastive prediction tasks by changing the architecture. For example, Hjelm et al. (2018); Bachman et al. (2019) achieve global-to-local view prediction via constraining the receptive efac81eld in the network architecture, whereas Oord et al. (2018); Hc3a9naff et al. (2019) achieve neighboring view prediction via a efac81xed image splitting procedure and a con- text aggregation network. We show that this complexity can be avoided by performing simple random cropping (with resizing) of target images, which creates a family of predic- tive tasks subsuming the above mentioned two, as shown in Figure 3. This simple design choice conveniently decouples the predictive task from other components such as the neural network architecture. Broader contrastive prediction tasks can be deefac81ned by extending the family of augmentations and composing them stochastically.  3.1. Composition of data augmentation operations is  crucial for learning good representations  To systematically study the impact of data augmentation, we consider several common augmentations here. One type of augmentation involves spatial/geometric transformation of data, such as cropping and resizing (with horizontal efac82ipping), rotation (Gidaris et al., 2018) and cutout (De- Vries & Taylor, 2017). The other type of augmentation involves appearance transformation, such as color distortion (including color dropping, brightness, contrast, saturation, hue) (Howard, 2013; Szegedy et al., 2015), Gaussian blur, and Sobel efac81ltering. Figure 4 visualizes the augmentations that we study in this work.  Figure 5. Linear evaluation (ImageNet top-1 accuracy) under in- dividual or composition of data augmentations, applied only to one branch. For all columns but the last, diagonal entries corre- spond to single transformation, and off-diagonals correspond to composition of two transformations (applied sequentially). The last column reefac82ects the average over the row.  To understand the effects of individual data augmentations and the importance of augmentation composition, we in- vestigate the performance of our framework when applying augmentations individually or in pairs. Since ImageNet images are of different sizes, we always apply crop and re- size images (Krizhevsky et al., 2012; Szegedy et al., 2015), which makes it difefac81cult to study other augmentations in the absence of cropping. To eliminate this confound, we consider an asymmetric data transformation setting for this ablation. Speciefac81cally, we always efac81rst randomly crop im- ages and resize them to the same resolution, and we then apply the targeted transformation(s) only to one branch of the framework in Figure 2, while leaving the other branch as the identity (i.e. t(xi) = xi). Note that this asymmet-  CropCutoutColorSobelNoiseBlurRotateAverage2nd transformationCropCutoutColorSobelNoiseBlurRotate1st transformation33.133.956.346.039.935.030.239.232.225.633.940.026.525.222.429.455.835.518.821.011.416.520.825.746.240.620.94.09.36.24.218.838.825.87.57.69.89.89.615.535.125.216.65.89.72.66.714.530.022.520.74.39.76.52.613.810203040500cA Simple Framework for Contrastive Learning of Visual Representations  (a) Without color distortion.  (b) With color distortion.  Figure 6. Histograms of pixel intensities (over all channels) for different crops of two different images (i.e. two rows). The image for the efac81rst row is from Figure 4. All axes have the same range.  Methods SimCLR Supervised  1/8 59.6 77.0  Color distortion strength 1/4 61.0 76.7  1/2 62.6 76.5  63.2 75.7  1  64.5 75.4  1 (+Blur) AutoAug  61.1 77.1  Table 1. Top-1 accuracy of unsupervised ResNet-50 using linear evaluation and supervised ResNet-505, under varied color distor- tion strength (see Appendix A) and other data transformations. Strength 1 (+Blur) is our default data augmentation policy.  ric data augmentation hurts the performance. Nonetheless, this setup should not substantively change the impact of individual data augmentations or their compositions. Figure 5 shows linear evaluation results under individual and composition of transformations. We observe that no single transformation sufefac81ces to learn good representations, even though the model can almost perfectly identify the positive pairs in the contrastive task. When composing aug- mentations, the contrastive prediction task becomes harder, but the quality of representation improves dramatically. Ap- pendix B.2 provides a further study on composing broader set of augmentations. One composition of augmentations stands out: random crop- ping and random color distortion. We conjecture that one serious issue when using only random cropping as data augmentation is that most patches from an image share a similar color distribution. Figure 6 shows that color his- tograms alone sufefac81ce to distinguish images. Neural nets may exploit this shortcut to solve the predictive task. There- fore, it is critical to compose cropping with color distortion in order to learn generalizable features.  3.2. Contrastive learning needs stronger data  augmentation than supervised learning  To further demonstrate the importance of the color aug- mentation, we adjust the strength of color augmentation as  5Supervised models are trained for 90 epochs; longer training  improves performance of stronger augmentation by e288bc 0.5%.  Figure 7. Linear evaluation of models with varied depth and width. Models in blue dots are ours trained for 100 epochs, models in red stars are ours trained for 1000 epochs, and models in green crosses are supervised ResNets trained for 90 epochs7 (He et al., 2016).  shown in Table 1. Stronger color augmentation substan- tially improves the linear evaluation of the learned unsuper- vised models. In this context, AutoAugment (Cubuk et al., 2019), a sophisticated augmentation policy found using su- pervised learning, does not work better than simple cropping + (stronger) color distortion. When training supervised mod- els with the same set of augmentations, we observe that stronger color augmentation does not improve or even hurts their performance. Thus, our experiments show that unsu- pervised contrastive learning beneefac81ts from stronger (color) data augmentation than supervised learning. Although pre- vious work has reported that data augmentation is useful for self-supervised learning (Doersch et al., 2015; Bachman et al., 2019; Hc3a9naff et al., 2019; Asano et al., 2019), we show that data augmentation that does not yield accuracy beneefac81ts for supervised learning can still help considerably with contrastive learning.  4. Architectures for Encoder and Head 4.1. Unsupervised contrastive learning beneefac81ts (more)  from bigger models  Figure 7 shows, perhaps unsurprisingly, that increasing depth and width both improve performance. While similar efac81ndings hold for supervised learning (He et al., 2016), we efac81nd the gap between supervised models and linear classiefac81ers trained on unsupervised models shrinks as the model size increases, suggesting that unsupervised learning beneefac81ts more from bigger models than its supervised counterpart.  7Training longer does not improve supervised ResNets (see  Appendix B.3).  050100150200250300350400450Number of Parameters (Millions)50556065707580Top 1R101R101(2x)R152R152(2x)R18R18(2x)R18(4x)R34R34(2x)R34(4x)R50R50(2x)R50(4x)Sup. R50Sup. R50(2x)Sup. R50(4x)R50*R50(2x)*R50(4x)*0cA Simple Framework for Contrastive Learning of Visual Representations  Name  NT-Xent  NT-Logistic  Margin Triplet  uT v+/cf84 e28892 log(cid:80)  Negative loss function  ve28888{v+,ve28892} exp(uT v/cf84 )  log cf83(uT v+/cf84 ) + log cf83(e28892uT ve28892/cf84 )  e28892 max(uT ve28892 e28892 uT v+ + m, 0)  Gradient w.r.t. u  )/cf84 v+ e28892(cid:80)  Z(u)  (1 e28892 exp(uT v+/cf84 )  ve28892 exp(uT ve28892/cf84 ) (cf83(e28892uT v+/cf84 ))/cf84 v+ e28892 cf83(uT ve28892/cf84 )/cf84 ve28892 v+ e28892 ve28892 if uT v+ e28892 uT ve28892 < m else 0  Z(u)  /cf84 ve28892  Table 2. Negative loss functions and their gradients. All input vectors, i.e. u, v+, ve28892, are (cid:96)2 normalized. NT-Xent is an abbreviation for e2809cNormalized Temperature-scaled Cross Entropye2809d. Different loss functions impose different weightings of positive and negative examples.  What to predict?  Color vs grayscale Rotation Orig. vs corrupted Orig. vs Sobel efac81ltered  Random guess Representation g(h) 97.4 25.6 59.6 56.3  h 99.3 67.6 99.5 96.6  80 25 50 50  Figure 8. Linear evaluation of representations with different pro- jection heads g(c2b7) and various dimensions of z = g(h). The representation h (before projection) is 2048-dimensional here. 4.2. A nonlinear projection head improves the representation quality of the layer before it  We then study the importance of including a projection head, i.e. g(h). Figure 8 shows linear evaluation results using three different architecture for the head: (1) identity mapping; (2) linear projection, as used by several previous approaches (Wu et al., 2018); and (3) the default nonlinear projection with one additional hidden layer (and ReLU acti- vation), similar to Bachman et al. (2019). We observe that a nonlinear projection is better than a linear projection (+3%), and much better than no projection (>10%). When a pro- jection head is used, similar results are observed regardless of output dimension. Furthermore, even when nonlinear projection is used, the layer before the projection head, h, is still much better (>10%) than the layer after, z = g(h), which shows that the hidden layer before the projection head is a better representation than the layer after. We conjecture that the importance of using the representa- tion before the nonlinear projection is due to loss of informa- tion induced by the contrastive loss. In particular, z = g(h) is trained to be invariant to data transformation. Thus, g can remove information that may be useful for the downstream task, such as the color or orientation of objects. By leverag- ing the nonlinear transformation g(c2b7), more information can be formed and maintained in h. To verify this hypothesis, we conduct experiments that use either h or g(h) to learn to predict the transformation applied during the pretraining. Here we set g(h) = W (2)cf83(W (1)h), with the same input and output dimensionality (i.e. 2048). Table 3 shows h contains much more information about the transformation applied, while g(h) loses information. Further analysis can  Table 3. Accuracy of training additional MLPs on different repre- sentations to predict the transformation applied. Other than crop and color augmentation, we additionally and independently add rotation (one of {0e297a6, 90e297a6, 180e297a6, 270e297a6}), Gaussian noise, and So- bel efac81ltering transformation during the pretraining for the last three rows. Both h and g(h) are of the same dimensionality, i.e. 2048.  be found in Appendix B.4.  5. Loss Functions and Batch Size 5.1. Normalized cross entropy loss with adjustable  temperature works better than alternatives  We compare the NT-Xent loss against other commonly used contrastive loss functions, such as logistic loss (Mikolov et al., 2013), and margin loss (Schroff et al., 2015). Table 2 shows the objective function as well as the gradient to the input of the loss function. Looking at the gradient, we observe 1) (cid:96)2 normalization (i.e. cosine similarity) along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives; and 2) unlike cross-entropy, other objec- tive functions do not weigh the negatives by their relative hardness. As a result, one must apply semi-hard negative mining (Schroff et al., 2015) for these loss functions: in- stead of computing the gradient over all loss terms, one can compute the gradient using semi-hard negative terms (i.e., those that are within the loss margin and closest in distance, but farther than positive examples). To make the comparisons fair, we use the same (cid:96)2 normaliza- tion for all loss functions, and we tune the hyperparameters, and report their best results.8 Table 4 shows that, while (semi-hard) negative mining helps, the best result is still much worse than our default NT-Xent loss.  8Details can be found in Appendix B.10. For simplicity, we  only consider the negatives from one augmentation view.  326412825651210242048Projection output dimensionality3040506070Top 1ProjectionLinearNon-linearNone0cA Simple Framework for Contrastive Learning of Visual Representations  Margin NT-Logi. Margin (sh) NT-Logi.(sh) NT-Xent 50.9  57.5  57.9  63.9  51.6  Table 4. Linear evaluation (top-1) for models trained with different loss functions. e2809cshe2809d means using semi-hard negative mining.  (cid:96)2 norm?  Yes  No  cf84 0.05 0.1 0.5 1 10 100  Entropy Contrastive acc.  1.0 4.5 8.2 8.3 0.5 0.5  90.5 87.8 68.2 59.1 91.7 92.1  Top 1 59.7 64.4 60.7 58.0 57.2 57.0  Table 5. Linear evaluation for models trained with different choices of (cid:96)2 norm and temperature cf84 for NT-Xent loss. The contrastive distribution is over 4096 examples.  Figure 9. Linear evaluation models (ResNet-50) trained with differ- ent batch size and epochs. Each bar is a single run from scratch.10  We next test the importance of the (cid:96)2 normalization (i.e. cosine similarity vs dot product) and temperature cf84 in our default NT-Xent loss. Table 5 shows that without normal- ization and proper temperature scaling, performance is sig- niefac81cantly worse. Without (cid:96)2 normalization, the contrastive task accuracy is higher, but the resulting representation is worse under linear evaluation.  5.2. Contrastive learning beneefac81ts (more) from larger  batch sizes and longer training  Figure 9 shows the impact of batch size when models are trained for different numbers of epochs. We efac81nd that, when the number of training epochs is small (e.g. 100 epochs), larger batch sizes have a signiefac81cant advantage over the smaller ones. With more training steps/epochs, the gaps between different batch sizes decrease or disappear, pro- vided the batches are randomly resampled. In contrast to  10A linear learning rate scaling is used here. Figure B.1 shows using a square root learning rate scaling can improve performance of ones with small batch sizes.  Architecture  Method Methods using ResNet-50: ResNet-50 Local Agg. ResNet-50 MoCo ResNet-50 PIRL ResNet-50 CPC v2 SimCLR (ours) ResNet-50 Methods using other architectures: RevNet-50 (4c397) Rotation RevNet-50 (4c397) BigBiGAN AMDIM Custom-ResNet ResNet-50 (2c397) CMC ResNet-50 (4c397) MoCo ResNet-161 (e28897) CPC v2 SimCLR (ours) ResNet-50 (2c397) SimCLR (ours) ResNet-50 (4c397)  Param (M) Top 1 Top 5  24 24 24 24 24  86 86 626 188 375 305 94 375  60.2 60.6 63.6 63.8 69.3  55.4 61.3 68.1 68.4 68.6 71.5 74.2 76.5  - - -  85.3 89.0  -  -  -  81.9  88.2  90.1 92.0 93.2  Table 6. ImageNet accuracies of linear classiefac81ers trained on repre- sentations learned with different self-supervised methods.  Method  Architecture  ResNet-50  Supervised baseline Methods using other label-propagation: ResNet-50 Pseudo-label ResNet-50 VAT+Entropy Min. UDA (w. RandAug) ResNet-50 FixMatch (w. RandAug) ResNet-50 S4L (Rot+VAT+En. M.) ResNet-50 (4c397) Methods using representation learning only: InstDisc ResNet-50 RevNet-50 (4c397) BigBiGAN PIRL ResNet-50 ResNet-161(e28897) CPC v2 ResNet-50 SimCLR (ours) ResNet-50 (2c397) SimCLR (ours) ResNet-50 (4c397) SimCLR (ours)  Label fraction 1% 10%  Top 5  48.4  80.4  51.6 47.0  - - -  39.2 55.2 57.2 77.9 75.5 83.0 85.8  82.4 83.4 88.5 89.1 91.2  77.4 78.8 83.8 91.2 87.8 91.2 92.6  Table 7. ImageNet accuracy of models trained with few labels.  supervised learning (Goyal et al., 2017), in contrastive learn- ing, larger batch sizes provide more negative examples, facilitating convergence (i.e. taking fewer epochs and steps for a given accuracy). Training longer also provides more negative examples, improving the results. In Appendix B.1, results with even longer training steps are provided.  6. Comparison with State-of-the-art In this subsection, similar to Kolesnikov et al. (2019); He et al. (2019), we use ResNet-50 in 3 different hidden layer widths (width multipliers of 1c397, 2c397, and 4c397). For better convergence, our models here are trained for 1000 epochs. Linear evaluation. Table 6 compares our results with previ- ous approaches (Zhuang et al., 2019; He et al., 2019; Misra & van der Maaten, 2019; Hc3a9naff et al., 2019; Kolesnikov et al., 2019; Donahue & Simonyan, 2019; Bachman et al.,  1002003004005006007008009001000Training epochs50.052.555.057.560.062.565.067.570.0Top 1Batch size25651210242048409681920cA Simple Framework for Contrastive Learning of Visual Representations  Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers  Linear evaluation: SimCLR (ours) 76.9 Supervised 75.2 Fine-tuned: SimCLR (ours) 89.4 88.7 Supervised Random init 88.3  95.3 95.7  98.6 98.3 96.0  80.2 81.2  89.0 88.7 81.9  48.4 56.4  78.2 77.8 77.0  65.9 64.9  68.1 67.0 53.7  60.0 68.8  92.1 91.4 91.3  61.2 63.8  87.0 88.0 84.8  84.2 83.8  86.6 86.5 69.4  78.9 89.2 78.7 92.3  77.8 92.1 78.8 93.2 64.1 82.7  93.9 94.1  94.1 94.2 72.5  95.0 94.2  97.6 98.0 92.5  Table 8. Comparison of transfer learning performance of our self-supervised approach with supervised baselines across 12 natural image classiefac81cation datasets, for ResNet-50 (4c397) models pretrained on ImageNet. Results not signiefac81cantly worse than the best (p > 0.05, permutation test) are shown in bold. See Appendix B.8 for experimental details and results with standard ResNet-50.  2019; Tian et al., 2019) in the linear evaluation setting (see Appendix B.6). Table 1 shows more numerical compar- isons among different methods. We are able to use standard networks to obtain substantially better results compared to previous methods that require speciefac81cally designed archi- tectures. The best result obtained with our ResNet-50 (4c397) can match the supervised pretrained ResNet-50. Semi-supervised learning. We follow Zhai et al. (2019) and sample 1% or 10% of the labeled ILSVRC-12 training datasets in a class-balanced way (e288bc12.8 and e288bc128 images per class respectively). 11 We simply efac81ne-tune the whole base network on the labeled data without regularization (see Appendix B.5). Table 7 shows the comparisons of our results against recent methods (Zhai et al., 2019; Xie et al., 2019; Sohn et al., 2020; Wu et al., 2018; Donahue & Simonyan, 2019; Misra & van der Maaten, 2019; Hc3a9naff et al., 2019). The supervised baseline from (Zhai et al., 2019) is strong due to intensive search of hyper-parameters (including augmentation). Again, our approach signiefac81cantly improves over state-of-the-art with both 1% and 10% of the labels. Interestingly, efac81ne-tuning our pretrained ResNet-50 (2c397, 4c397) on full ImageNet are also signiefac81cantly better then training from scratch (up to 2%, see Appendix B.2). Transfer learning. We evaluate transfer learning perfor- mance across 12 natural image datasets in both linear evalu- ation (efac81xed feature extractor) and efac81ne-tuning settings. Fol- lowing Kornblith et al. (2019), we perform hyperparameter tuning for each model-dataset combination and select the best hyperparameters on a validation set. Table 8 shows results with the ResNet-50 (4c397) model. When efac81ne-tuned, our self-supervised model signiefac81cantly outperforms the su- pervised baseline on 5 datasets, whereas the supervised baseline is superior on only 2 (i.e. Pets and Flowers). On the remaining 5 datasets, the models are statistically tied. Full experimental details as well as results with the standard ResNet-50 architecture are provided in Appendix B.8.  11The details of sampling and exact subsets can be found in https://www.tensorefac82ow.org/datasets/catalog/imagenet2012_subset.  7. Related Work The idea of making representations of an image agree with each other under small transformations dates back to Becker & Hinton (1992). We extend it by leveraging recent ad- vances in data augmentation, network architecture and con- trastive loss. A similar consistency idea, but for class label prediction, has been explored in other contexts such as semi- supervised learning (Xie et al., 2019; Berthelot et al., 2019). Handcrafted pretext tasks. The recent renaissance of self- supervised learning began with artiefac81cially designed pretext tasks, such as relative patch prediction (Doersch et al., 2015), solving jigsaw puzzles (Noroozi & Favaro, 2016), coloriza- tion (Zhang et al., 2016) and rotation prediction (Gidaris et al., 2018; Chen et al., 2019). Although good results can be obtained with bigger networks and longer train- ing (Kolesnikov et al., 2019), these pretext tasks rely on somewhat ad-hoc heuristics, which limits the generality of learned representations. Contrastive visual representation learning. Dating back to Hadsell et al. (2006), these approaches learn represen- tations by contrasting positive pairs against negative pairs. Along these lines, Dosovitskiy et al. (2014) proposes to treat each instance as a class represented by a feature vector (in a parametric form). Wu et al. (2018) proposes to use a memory bank to store the instance class representation vector, an approach adopted and extended in several recent papers (Zhuang et al., 2019; Tian et al., 2019; He et al., 2019; Misra & van der Maaten, 2019). Other work explores the use of in-batch samples for negative sampling instead of a memory bank (Doersch & Zisserman, 2017; Ye et al., 2019; Ji et al., 2019). Recent literature has attempted to relate the success of their methods to maximization of mutual information between latent representations (Oord et al., 2018; Hc3a9naff et al., 2019; Hjelm et al., 2018; Bachman et al., 2019). However, it is not clear if the success of contrastive approaches is determined by the mutual information, or by the speciefac81c form of the contrastive loss (Tschannen et al., 2019).  0cA Simple Framework for Contrastive Learning of Visual Representations  We note that almost all individual components of our frame- work have appeared in previous work, although the speciefac81c instantiations may be different. The superiority of our frame- work relative to previous work is not explained by any single design choice, but by their composition. We provide a com- prehensive comparison of our design choices with those of previous work in Appendix C.  8. Conclusion In this work, we present a simple framework and its in- stantiation for contrastive visual representation learning. We carefully study its components, and show the effects of different design choices. By combining our efac81ndings, we improve considerably over previous methods for self- supervised, semi-supervised, and transfer learning. Our approach differs from standard supervised learning on ImageNet only in the choice of data augmentation, the use of a nonlinear head at the end of the network, and the loss func- tion. The strength of this simple framework suggests that, despite a recent surge in interest, self-supervised learning remains undervalued.  Acknowledgements We would like to thank Xiaohua Zhai, Rafael Mc3bcller and Yani Ioannou for their feedback on the draft. We are also grateful for general support from Google Research teams in Toronto and elsewhere.  References Asano, Y. M., Rupprecht, C., and Vedaldi, A. A critical analysis of self-supervision, or what we can learn from a single image. arXiv preprint arXiv:1904.13132, 2019.  Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning rep- resentations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. 15509e2809315519, 2019.  Becker, S. and Hinton, G. E. Self-organizing neural network that discovers surfaces in random-dot stereograms. Nature, 355 (6356):161e28093163, 1992.  Berg, T., Liu, J., Lee, S. W., Alexander, M. L., Jacobs, D. W., and Belhumeur, P. N. Birdsnap: Large-scale efac81ne-grained visual categorization of birds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2019e280932026. IEEE, 2014.  Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and Raffel, C. A. Mixmatch: A holistic approach to semi- supervised learning. In Advances in Neural Information Pro- cessing Systems, pp. 5050e280935060, 2019.  Bossard, L., Guillaumin, M., and Van Gool, L. Food-101e28093mining discriminative components with random forests. In European conference on computer vision, pp. 446e28093461. Springer, 2014.  Chen, T., Sun, Y., Shi, Y., and Hong, L. On sampling strategies for neural network-based collaborative efac81ltering. In Proceed- ings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 767e28093776, 2017.  Chen, T., Zhai, X., Ritter, M., Lucic, M., and Houlsby, N. Self- supervised gans via auxiliary rotation loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 12154e2809312163, 2019.  Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3606e28093 3613. IEEE, 2014.  Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 113e28093123, 2019.  DeVries, T. and Taylor, G. W.  Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.  Doersch, C. and Zisserman, A. Multi-task self-supervised visual learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2051e280932060, 2017.  Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422e280931430, 2015.  Donahue, J. and Simonyan, K. Large scale adversarial representa- tion learning. In Advances in Neural Information Processing Systems, pp. 10541e2809310551, 2019.  Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, pp. 647e28093655, 2014.  Dosovitskiy, A., Springenberg, J. T., Riedmiller, M., and Brox, T. Discriminative unsupervised feature learning with convolutional neural networks. In Advances in neural information processing systems, pp. 766e28093774, 2014.  Everingham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303e28093338, 2010.  Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop on Generative-Model Based Vision, 2004.  Gidaris, S., Singh, P., and Komodakis, N. Unsupervised represen- tation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.  Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde- Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672e280932680, 2014.  0cA Simple Framework for Contrastive Learning of Visual Representations  Goyal, P., Dollc3a1r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.  Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer So- ciety Conference on Computer Vision and Pattern Recognition (CVPRe2809906), volume 2, pp. 1735e280931742. IEEE, 2006.  He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770e28093778, 2016.  He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.  Hc3a9naff, O. J., Razavi, A., Doersch, C., Eslami, S., and Oord, A. v. d. Data-efefac81cient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.  Hinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning al- gorithm for deep belief nets. Neural computation, 18(7):1527e28093 1554, 2006.  Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep repre- sentations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.  Howard, A. G. Some improvements on deep convolutional neural network based image classiefac81cation. arXiv preprint arXiv:1312.5402, 2013.  Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.  Ji, X., Henriques, J. F., and Vedaldi, A. Invariant information clustering for unsupervised image classiefac81cation and segmenta- tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 9865e280939874, 2019.  Kingma, D. P. and Welling, M. Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114, 2013.  Kolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-supervised In Proceedings of the IEEE visual representation learning. conference on Computer Vision and Pattern Recognition, pp. 1920e280931929, 2019.  Kornblith, S., Shlens, J., and Le, Q. V. Do better ImageNet models In Proceedings of the IEEE conference on transfer better? computer vision and pattern recognition, pp. 2661e280932671, 2019.  Krause, J., Deng, J., Stark, M., and Fei-Fei, L. Collecting a large-scale dataset of efac81ne-grained cars. In Second Workshop on Fine-Grained Visual Categorization, 2013.  Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. URL https://www.cs.toronto.edu/~kriz/ learning-features-2009-TR.pdf.  Loshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent  with warm restarts. arXiv preprint arXiv:1608.03983, 2016.  Maaten, L. v. d. and Hinton, G. Visualizing data using t-sne. Jour-  nal of machine learning research, 9(Nov):2579e280932605, 2008.  Maji, S., Kannala, J., Rahtu, E., Blaschko, M., and Vedaldi, A. Fine-grained visual classiefac81cation of aircraft. Technical report, 2013.  Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efefac81cient esti- mation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.  Misra,  I. and van der Maaten, L.  ing of pretext-invariant representations. arXiv:1912.01991, 2019.  Self-supervised learn- arXiv preprint  Nilsback, M.-E. and Zisserman, A. Automated efac82ower classiefac81cation over a large number of classes. In Computer Vision, Graphics & Image Processing, 2008. ICVGIPe2809908. Sixth Indian Conference on, pp. 722e28093729. IEEE, 2008.  Noroozi, M. and Favaro, P. Unsupervised learning of visual repre- sentations by solving jigsaw puzzles. In European Conference on Computer Vision, pp. 69e2809384. Springer, 2016.  Oord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.  Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3498e280933505. IEEE, 2012.  Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211e28093252, 2015.  Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A uniefac81ed In Proceed- embedding for face recognition and clustering. ings of the IEEE conference on computer vision and pattern recognition, pp. 815e28093823, 2015.  Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.  Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. In Advances in neural information processing systems, pp. 1857e280931865, 2016.  Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C. Fixmatch: Simpli- fying semi-supervised learning with consistency and conefac81dence. arXiv preprint arXiv:2001.07685, 2020.  Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1e280939, 2015.  Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding.  arXiv preprint arXiv:1906.05849, 2019.  Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiefac81- cation with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097e280931105, 2012.  Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S., and Lu- cic, M. On mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.  0cA Simple Framework for Contrastive Learning of Visual Representations  Wu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised feature learning via non-parametric instance discrimination. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3733e280933742, 2018.  Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun database: Large-scale scene recognition from abbey to zoo. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3485e280933492. IEEE, 2010.  Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. Unsu- pervised data augmentation. arXiv preprint arXiv:1904.12848, 2019.  Ye, M., Zhang, X., Yuen, P. C., and Chang, S.-F. Unsupervised embedding learning via invariant and spreading instance feature. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6210e280936219, 2019.  You, Y., Gitman, I., and Ginsburg, B. Large batch training of con- volutional networks. arXiv preprint arXiv:1708.03888, 2017.  Zhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S4l: Self- supervised semi-supervised learning. In The IEEE International Conference on Computer Vision (ICCV), October 2019.  Zhang, R., Isola, P., and Efros, A. A. Colorful image coloriza- tion. In European conference on computer vision, pp. 649e28093666. Springer, 2016.  Zhuang, C., Zhai, A. L., and Yamins, D. Local aggregation for unsupervised learning of visual embeddings. In Proceedings of the IEEE International Conference on Computer Vision, pp. 6002e280936012, 2019.  0cA Simple Framework for Contrastive Learning of Visual Representations  A. Data Augmentation Details In our default pretraining setting (which is used to train our best models), we utilize random crop (with resize and random efac82ip), random color distortion, and random Gaussian blur as the data augmentations. The details of these three augmentations are provided below.  Random crop and resize to 224x224 We use standard Inception-style random cropping (Szegedy et al., 2015). The crop of random size (uniform from 0.08 to 1.0 in area) of the original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop is efac81nally resized to the original size. This has been imple- mented in Tensorefac82ow as e2809cslim.preprocessing.inception_preprocessing.distorted_bounding_box_crope2809d, or in Pytorch as e2809ctorchvision.transforms.RandomResizedCrope2809d. Additionally, the random crop (with resize) is always followed by a random horizontal/left-to-right efac82ip with 50% probability. This is helpful but not essential. By removing this from our default augmentation policy, the top-1 linear evaluation drops from 64.5% to 63.4% for our ResNet-50 model trained in 100 epochs.  Color distortion Color distortion is composed by color jittering and color dropping. We efac81nd stronger color jittering usually helps, so we set a strength parameter. A pseudo-code for color distortion using TensorFlow is as follows.  import tensorflow as tf def color_distortion(image, s=1.0):  # image is a tensor with value range in [0, 1]. # s is the strength of color distortion.  def color_jitter(x):  # one can also shuffle the order of following augmentations # each time they are applied. x = tf.image.random_brightness(x, max_delta=0.8*s) x = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s) x = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s) x = tf.image.random_hue(x, max_delta=0.2*s) x = tf.clip_by_value(x, 0, 1) return x  def color_drop(x):  image = tf.image.rgb_to_grayscale(image) image = tf.tile(image, [1, 1, 3])  # randomly apply transformation with probability p. image = random_apply(color_jitter, image, p=0.8) image = random_apply(color_drop, image, p=0.2) return image  A pseudo-code for color distortion using Pytorch is as follows 12.  from torchvision import transforms def get_color_distortion(s=1.0):  # s is the strength of color distortion. color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s) rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8) rnd_gray = transforms.RandomGrayscale(p=0.2) color_distort = transforms.Compose([  rnd_color_jitter, rnd_gray])  12Our code and results are based on Tensorefac82ow, the Pytorch code here is a reference.  0cA Simple Framework for Contrastive Learning of Visual Representations  return color_distort  Gaussian blur This augmentation is in our default policy. We efac81nd it helpful, as it improves our ResNet-50 trained for 100 epochs from 63.2% to 64.5%. We blur the image 50% of the time using a Gaussian kernel. We randomly sample cf83 e28888 [0.1, 2.0], and the kernel size is set to be 10% of the image height/width.  B. Additional Experimental Results B.1. Batch Size and Training Steps  Figure B.1 shows the top-5 accuracy on linear evaluation when trained with different batch sizes and training epochs. The conclusion is very similar to top-1 accuracy shown before, except that the differences between different batch sizes and training steps seems slightly smaller here. In both Figure 9 and Figure B.1, we use a linear scaling of learning rate similar to (Goyal et al., 2017) when training with different batch sizes. Although linear learning rate scaling is popular with SGD/Momentum optimizer, we efac81nd a square root learning rate scaling is more desirable with LARS optimizer. With square root learning rate scaling, we have BatchSize, instead of LearningRate = 0.3 c397 BatchSize/256 in the linear scaling case, but the learning rate is the same under both scaling methods when batch size of 4096 (our default batch size). A comparison is presented in Table B.1, where we observe that square root learning rate scaling improves the performance for models trained with small batch sizes and in smaller number of epochs.  LearningRate = 0.075 c397 e2889a  Batch size \\\\ Epochs  100  256 512 1024 2048 4096 8192  57.5 / 62.8 60.7 / 63.8 62.8 / 64.3 64.0 / 64.7 64.6 / 64.5 64.8 / 64.8  200  61.9 / 64.3 64.0 / 65.6 65.3 / 66.1 66.1 / 66.8 66.5 / 66.8 66.6 / 67.0  400  64.7 / 65.7 66.2 / 66.7 67.2 / 67.2 68.1 / 67.9 68.2 / 68.0 67.8 / 68.3  800  66.6 / 66.5 67.8 / 67.4 68.5 / 68.3 68.9 / 68.8 68.9 / 69.1 69.0 / 69.1  Table B.1. Linear evaluation (top-1) under different batch sizes and training epochs. On the left side of slash sign are models trained with linear LR scaling, and on the right are models trained with square root LR scaling. The result is bolded if it is more than 0.5% better. Square root LR scaling works better for smaller batch size trained in fewer epochs (with LARS optimizer).  We also train with larger batch size (up to 32K) and longer (up to 3200 epochs), with the square root learning rate scaling. A shown in Figure B.2, the performance seems to saturate with a batch size of 8192, while training longer can still signiefac81cantly improve the performance.  Figure B.1. Linear evaluation (top-5) of ResNet-50 trained with different batch sizes and epochs. Each bar is a single run from scratch. See Figure 9 for top-1 accuracy.  Figure B.2. Linear evaluation (top-1) of ResNet-50 trained with different batch sizes and longer epochs. Here a square root learn- ing rate, instead of a linear one, is utilized.  1002003004005006007008009001000Training epochs70.072.575.077.580.082.585.087.590.0Top 5Batch size25651210242048409681925010020040080016003200Training epochs60626466687072Top 1Batch size256512102420484096819216384327680cA Simple Framework for Contrastive Learning of Visual Representations  B.2. Broader composition of data augmentations further improves performance  Our best results in the main text (Table 6 and 7) can be further improved when expanding the default augmentation policy to include the following: (1) Sobel efac81ltering, (2) additional color distortion (equalize, solarize), and (3) motion blur. For linear evaluation protocol, the ResNet-50 models (1c397, 2c397, 4c397) trained with broader data augmentations achieve 70.0 (+0.7), 74.4 (+0.2), 76.8 (+0.3), respectively. Table B.2 shows ImageNet accuracy obtained by efac81ne-tuning the SimCLR model (see Appendix B.5 for the details of efac81ne-tuning procedure). Interestingly, when efac81ne-tuned on full (100%) ImageNet training set, our ResNet (4c397) model achieves 80.4% top-1 / 95.4% top-5 13, which is signiefac81cantly better than that (78.4% top-1 / 94.2% top-5) of training from scratch using the same set of augmentations (i.e. random crop and horizontal efac82ip). For ResNet-50 (2c397), efac81ne-tuning our pre-trained ResNet-50 (2c397) is also better than training from scratch (77.8% top-1 / 93.9% top-5). There is no improvement from efac81ne-tuning for ResNet-50.  Architecture  ResNet-50 ResNet-50 (2c397) ResNet-50 (4c397)  1%  Label fraction  10%  100%  Top 1 Top 5 Top 1 Top 5 Top 1 Top 5 49.4 93.1 94.8 59.4 64.1 95.4  88.1 91.2 92.8  76.0 79.1 80.4  76.6 83.7 86.6  66.1 71.8 74.8  Table B.2. Classiefac81cation accuracy obtained by efac81ne-tuning the SimCLR (which is pretrained with broader data augmentations) on 1%, 10% and full of ImageNet. As a reference, our ResNet-50 (4c397) trained from scratch on 100% labels achieves 78.4% top-1 / 94.2% top-5.  B.3. Effects of Longer Training for Supervised Models  Here we perform experiments to see how training steps and stronger data augmentation affect supervised training. We test ResNet-50 and ResNet-50 (4c397) under the same set of data augmentations (random crops, color distortion, 50% Gaussian blur) as used in our unsupervised models. Figure B.3 shows the top-1 accuracy. We observe that there is no signiefac81cant beneefac81t from training supervised models longer on ImageNet. Stronger data augmentation slightly improves the accuracy of ResNet-50 (4c397) but does not help on ResNet-50. When stronger data augmentation is applied, ResNet-50 generally requires longer training (e.g. 500 epochs 14) to obtain the optimal result, while ResNet-50 (4c397) does not beneefac81t from longer training.  Model  Training epochs  ResNet-50  ResNet-50 (4c397)  90 500 1000 90 500 1000  Top 1  +Color 75.6 76.5 75.2 78.9 78.4 78.2  Crop 76.5 76.2 75.8 78.4 78.3 77.9  +Color+Blur  75.3 76.7 76.4 78.7 78.5 78.3  Table B.3. Top-1 accuracy of supervised models trained longer under various data augmentation procedures (from the same set of data augmentations for contrastive learning).  B.4. Understanding The Non-Linear Projection Head Figure B.3 shows the eigenvalue distribution of linear projection matrix W e28888 R2048c3972048 used to compute z = W h. This matrix has relatively few large eigenvalues, indicating that it is approximately low-rank. Figure B.4 shows t-SNE (Maaten & Hinton, 2008) visualizations of h and z = g(h) for randomly selected 10 classes by our best ResNet-50 (top-1 linear evaluation 69.3%). Classes represented by h are better separated compared to z.  13It is 80.1% top-1 / 95.2% top-5 without broader augmentations for pretraining SimCLR. 14With AutoAugment (Cubuk et al., 2019), optimal test accuracy can be achieved between 900 and 500 epochs.  0cA Simple Framework for Contrastive Learning of Visual Representations  (a) Y-axis in uniform scale.  (b) Y-axis in log scale.  Figure B.3. Squared real eigenvalue distribution of linear projection matrix W e28888 R2048c3972048 used to compute g(h) = W h.  (a) h  (b) z = g(h)  Figure B.4. t-SNE visualizations of hidden vectors of images from a randomly selected 10 classes in the validation set.  B.5. Semi-supervised Learning via Fine-Tuning Fine-tuning Procedure We efac81ne-tune using the Nesterov momentum optimizer with a batch size of 4096, momentum of 0.9, and a learning rate of 0.8 (following LearningRate = 0.05c397 BatchSize/256) without warmup. Only random cropping (with random left-to-right efac82ipping and resizing to 224x224) is used for preprocessing. We do not use any regularization (including weight decay). For 1% labeled data we efac81ne-tune for 60 epochs, and for 10% labeled data we efac81ne-tune for 30 epochs. For the inference, we resize the given image to 256x256, and take a single center crop of 224x224. Table B.4 shows the comparisons of top-1 accuracy for different methods for semi-supervised learning. Our models signiefac81cantly improve state-of-the-art.  Method  Architecture  Label fraction 1% 10%  Top 1  25.4  ResNet-50  ResNet-50 ResNet-50  Supervised baseline Methods using label-propagation: UDA (w. RandAug) FixMatch (w. RandAug) S4L (Rot+VAT+Ent. Min.) ResNet-50 (4c397) Methods using self-supervised representation learning only: CPC v2 SimCLR (ours) SimCLR (ours) SimCLR (ours)  ResNet-161(e28897) ResNet-50 ResNet-50 (2c397) ResNet-50 (4c397)  52.7 48.3 58.5 63.0  - - -  56.4  68.8 71.5 73.2  73.1 65.6 71.7 74.4  Table B.4. ImageNet top-1 accuracy of models trained with few labels. See Table 7 for top-5 accuracy.  B.6. Linear Evaluation  For linear evaluation, we follow similar procedure as efac81ne-tuning (described in Appendix B.5), except that a larger learning rate of 1.6 (following LearningRate = 0.1 c397 BatchSize/256) and longer training of 90 epochs. Alternatively, using LARS optimizer with the pretraining hyper-parameters also yield similar results. Furthermore, we efac81nd that attaching the linear classiefac81er on top of the base encoder (with a stop_gradient on the input to linear classiefac81er to prevent the label information from inefac82uencing the encoder) and train them simultaneously during the pretraining achieves similar performance.  B.7. Correlation Between Linear Evaluation and Fine-Tuning  Here we study the correlation between linear evaluation and efac81ne-tuning under different settings of training step and network architecture. Figure B.5 shows linear evaluation versus efac81ne-tuning when training epochs of a ResNet-50 (using batch size of 4096) are varied from 50 to 3200 as in Figure B.2. While they are almost linearly correlated, it seems efac81ne-tuning on a small fraction  0500100015002000Ranking0246810121416Squared eigenvalue0500100015002000Ranking10e288921110e28892910e28892710e28892510e28892310e288921101Squared eigenvalue0cA Simple Framework for Contrastive Learning of Visual Representations  of labels beneefac81ts more from training longer.  Figure B.5. Top-1 accuracy of models trained in different epochs (from Figure B.2), under linear evaluation and efac81ne-tuning.  Figure B.6 shows shows linear evaluation versus efac81ne-tuning for different architectures of choice.  Figure B.6. Top-1 accuracy of different architectures under linear evaluation and efac81ne-tuning.  B.8. Transfer Learning  We evaluated the performance of our self-supervised representation for transfer learning in two settings: linear evaluation, where a logistic regression classiefac81er is trained to classify a new dataset based on the self-supervised representation learned on ImageNet, and efac81ne-tuning, where we allow all weights to vary during training. In both cases, we follow the approach described by Kornblith et al. (2019), although our preprocessing differs slightly.  B.8.1. METHODS Datasets We investigated transfer learning performance on the Food-101 dataset (Bossard et al., 2014), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009), Birdsnap (Berg et al., 2014), the SUN397 scene dataset (Xiao et al., 2010), Stanford Cars (Krause et al., 2013), FGVC Aircraft (Maji et al., 2013), the PASCAL VOC 2007 classiefac81cation task (Everingham et al., 2010), the Describable Textures Dataset (DTD) (Cimpoi et al., 2014), Oxford-IIIT Pets (Parkhi et al., 2012), Caltech-101 (Fei-Fei et al., 2004), and Oxford 102 Flowers (Nilsback & Zisserman, 2008). We follow the evaluation protocols in the papers introducing these datasets, i.e., we report top-1 accuracy for Food-101, CIFAR-10, CIFAR-100, Birdsnap, SUN397, Stanford Cars, and DTD; mean per-class accuracy for FGVC Aircraft, Oxford-IIIT Pets, Caltech-101, and Oxford 102 Flowers; and the 11-point mAP metric as deefac81ned in Everingham et al. (2010) for PASCAL VOC 2007. For DTD and SUN397, the dataset creators deefac81ned multiple train/test splits; we report results only for the efac81rst split. Caltech-101 deefac81nes no train/test split, so we randomly chose 30 images per class and test on the remainder, for fair comparison with previous work (Donahue et al., 2014; Simonyan & Zisserman, 2014). We used the validation sets speciefac81ed by the dataset creators to select hyperparameters for FGVC Aircraft, PASCAL VOC  6264666870Linear eval35.037.540.042.545.047.550.0Fine-tuning on 1%6264666870Linear eval60626466Fine-tuning on 10%5055606570Linear eval303336394245485154Finetune (1%)Width1x2x4xDepth1834501011525055606570Linear eval51545760636669Finetune (10%)Width1x2x4xDepth1834501011520cA Simple Framework for Contrastive Learning of Visual Representations  2007, DTD, and Oxford 102 Flowers. For other datasets, we held out a subset of the training set for validation while performing hyperparameter tuning. After selecting the optimal hyperparameters on the validation set, we retrained the model using the selected parameters using all training and validation images. We report accuracy on the test set.  Transfer Learning via a Linear Classiefac81er We trained an (cid:96)2-regularized multinomial logistic regression classiefac81er on features extracted from the frozen pretrained network. We used L-BFGS to optimize the softmax cross-entropy objective and we did not apply data augmentation. As preprocessing, all images were resized to 224 pixels along the shorter side using bicubic resampling, after which we took a 224 c397 224 center crop. We selected the (cid:96)2 regularization parameter from a range of 45 logarithmically spaced values between 10e288926 and 105.  Transfer Learning via Fine-Tuning We efac81ne-tuned the entire network using the weights of the pretrained network as initialization. We trained for 20,000 steps at a batch size of 256 using SGD with Nesterov momentum with a momentum parameter of 0.9. We set the momentum parameter for the batch normalization statistics to max(1 e28892 10/s, 0.9) where s is the number of steps per epoch. As data augmentation during efac81ne-tuning, we performed only random crops with resize and efac82ips; in contrast to pretraining, we did not perform color augmentation or blurring. At test time, we resized images to 256 pixels along the shorter side and took a 224 c397 224 center crop. (Additional accuracy improvements may be possible with further optimization of data augmentation, particularly on the CIFAR-10 and CIFAR-100 datasets.) We selected the learning rate and weight decay, with a grid of 7 logarithmically spaced learning rates between 0.0001 and 0.1 and 7 logarithmically spaced values of weight decay between 10e288926 and 10e288923, as well as no weight decay. We divide these values of weight decay by the learning rate.  Training from Random Initialization We trained the network from random initialization using the same procedure as for efac81ne-tuning, but for longer, and with an altered hyperparameter grid. We chose hyperparameters from a grid of 7 logarithmically spaced learning rates between 0.001 and 1.0 and 8 logarithmically spaced values of weight decay between 10e288925 and 10e288921.5. Importantly, our random initialization baselines are trained for 40,000 steps, which is sufefac81ciently long to achieve near-maximal accuracy, as demonstrated in Figure 8 of Kornblith et al. (2019). On Birdsnap, there are no statistically signiefac81cant differences among methods, and on Food-101, Stanford Cars, and FGVC Aircraft datasets, efac81ne-tuning provides only a small advantage over training from random initialization. However, on the remaining 8 datasets, pretraining has clear advantages.  Supervised Baselines We compare against architecturally identical ResNet models trained on ImageNet with standard cross-entropy loss. These models are trained with the same data augmentation as our self-supervised models (crops, strong color augmentation, and blur) and are also trained for 1000 epochs. We found that, although stronger data augmentation and longer training time do not beneefac81t accuracy on ImageNet, these models performed signiefac81cantly better than a supervised baseline trained for 90 epochs and ordinary data augmentation for linear evaluation on a subset of transfer datasets. The supervised ResNet-50 baseline achieves 76.3% top-1 accuracy on ImageNet, vs. 69.3% for the self-supervised counterpart, while the ResNet-50 (4c397) baseline achieves 78.3%, vs. 76.5% for the self-supervised model.  Statistical Signiefac81cance Testing We test for the signiefac81cance of differences between model with a permutation test. Given predictions of two models, we generate 100,000 samples from the null distribution by randomly exchanging predictions for each example and computing the difference in accuracy after performing this randomization. We then compute the percentage of samples from the null distribution that are more extreme than the observed difference in predictions. For top-1 accuracy, this procedure yields the same result as the exact McNemar test. The assumption of exchangeability under the null hypothesis is also valid for mean per-class accuracy, but not when computing average precision curves. Thus, we perform signiefac81cance testing for a difference in accuracy on VOC 2007 rather than a difference in mAP. A caveat of this procedure is that it does not consider run-to-run variability when training the models, only variability arising from using a efac81nite sample of images for evaluation.  B.8.2. RESULTS WITH STANDARD RESNET The ResNet-50 (4c397) results shown in Table 8 of the text show no clear advantage to the supervised or self-supervised models. With the narrower ResNet-50 architecture, however, supervised learning maintains a clear advantage over self-supervised learning. The supervised ResNet-50 model outperforms the self-supervised model on all datasets with linear evaluation, and most (10 of 12) datasets with efac81ne-tuning. The weaker performance of the ResNet model compared to the ResNet (4c397)  0cA Simple Framework for Contrastive Learning of Visual Representations  Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers  Linear evaluation: SimCLR (ours) 68.4 72.3 Supervised Fine-tuned: SimCLR (ours) 88.2 88.3 Supervised Random init 86.9  90.6 93.6  97.7 97.5 95.9  71.6 78.3  85.9 86.4 80.2  37.4 53.7  75.9 75.8 76.1  58.8 61.9  63.5 64.3 53.6  50.3 66.7  91.3 92.1 91.4  50.3 61.0  88.1 86.0 85.9  80.5 82.8  84.1 85.0 67.3  74.5 83.6 74.9 91.5  73.2 89.2 74.6 92.1 64.8 81.5  90.3 94.5  92.1 93.3 72.6  91.2 94.7  97.0 97.6 92.0  Table B.5. Comparison of transfer learning performance of our self-supervised approach with supervised baselines across 12 natural image datasets, using ImageNet-pretrained ResNet models. See also Figure 8 for results with the ResNet (4c397) architecture.  model may relate to the accuracy gap between the supervised and self-supervised models on ImageNet. The self-supervised ResNet gets 69.3% top-1 accuracy, 6.8% worse than the supervised model in absolute terms, whereas the self-supervised ResNet (4c397) model gets 76.5%, which is only 1.8% worse than the supervised model.  B.9. CIFAR-10  While we focus on using ImageNet as the main dataset for pretraining our unsupervised model, our method also works with other datasets. We demonstrate it by testing on CIFAR-10 as follows.  Setup As our goal is not to optimize CIFAR-10 performance, but rather to provide further conefac81rmation of our observations on ImageNet, we use the same architecture (ResNet-50) for CIFAR-10 experiments. Because CIFAR-10 images are much smaller than ImageNet images, we replace the efac81rst 7x7 Conv of stride 2 with 3x3 Conv of stride 1, and also remove the efac81rst max pooling operation. For data augmentation, we use the same Inception crop (efac82ip and resize to 32x32) as ImageNet,15 and color distortion (strength=0.5), leaving out Gaussian blur. We pretrain with learning rate in {0.5, 1.0, 1.5}, temperature in {0.1, 0.5, 1.0}, and batch size in {256, 512, 1024, 2048, 4096}. The rest of the settings (including optimizer, weight decay, etc.) are the same as our ImageNet training. Our best model trained with batch size 1024 can achieve a linear evaluation accuracy of 94.0%, compared to 95.1% from the supervised baseline using the same architecture and batch size. The best self-supervised model that reports linear evaluation result on CIFAR-10 is AMDIM (Bachman et al., 2019), which achieves 91.2% with a model 25c397 larger than ours. We note that our model can be improved by incorporating extra data augmentations as well as using a more suitable base network.  Performance under different batch sizes and training steps Figure B.7 shows the linear evaluation performance under different batch sizes and training steps. The results are consistent with our observations on ImageNet, although the largest batch size of 4096 seems to cause a small degradation in performance on CIFAR-10.  Figure B.7. Linear evaluation of ResNet-50 (with ad- justed stem) trained with different batch size and epochs on CIFAR-10 dataset. Each bar is averaged over 3 runs with different learning rates (0.5, 1.0, 1.5) and temperature cf84 = 0.5. Error bar denotes standard deviation.  15It is worth noting that, although CIFAR-10 images are much smaller than ImageNet images and image size does not differ among  examples, cropping with resizing is still a very effective augmentation for contrastive learning.  1002003004005006007008009001000Training epochs8082848688909294Top 1Batch size2565121024204840960cA Simple Framework for Contrastive Learning of Visual Representations  Optimal temperature under different batch sizes Figure B.8 shows the linear evaluation of model trained with three different temperatures under various batch sizes. We efac81nd that when training to convergence (e.g. training epochs > 300), the optimal temperature in {0.1, 0.5, 1.0} is 0.5 and seems consistent regardless of the batch sizes. However, the performance with cf84 = 0.1 improves as batch size increases, which may suggest a small shift of optimal temperature towards 0.1.  (a) Training epochs e289a4 300  (b) Training epochs > 300  Figure B.8. Linear evaluation of the model (ResNet-50) trained with three temperatures on different batch sizes on CIFAR-10. Each bar is averaged over multiple runs with different learning rates and total train epochs. Error bar denotes standard deviation.  B.10. Tuning For Other Loss Functions  The learning rate that works best for NT-Xent loss may not be a good learning rate for other loss functions. To ensure a fair comparison, we also tune hyperparameters for both margin loss and logistic loss. Speciefac81cally, we tune learning rate in {0.01, 0.1, 0.3, 0.5, 1.0} for both loss functions. We further tune the margin in {0, 0.4, 0.8, 1.6} for margin loss, the temperature in {0.1, 0.2, 0.5, 1.0} for logistic loss. For simplicity, we only consider the negatives from one augmentation view (instead of both sides), which slightly impairs performance but ensures fair comparison.  C. Further Comparison to Related Methods As we have noted in the main text, most individual components of SimCLR have appeared in previous work, and the improved performance is a result of a combination of these design choices. Table C.1 provides a high-level comparison of the design choices of our method with those of previous methods. Compared with previous work, our design choices are generally simpler.  Data Augmentation Custom  Model CPC v2 AMDIM Fast AutoAug. CMC Fast AutoAug. Crop+color MoCo PIRL Crop+color SimCLR Crop+color+blur  Base Encoder ResNet-161 (modiefac81ed) Custom ResNet ResNet-50 (2c397, L+ab) ResNet-50 (4c397) ResNet-50 (2c397) ResNet-50 (4c397)  Loss Xent  Projection Head PixelCNN Non-linear MLP Xent w/ clip,reg Xent w/ (cid:96)2, cf84 Linear layer Xent w/ (cid:96)2, cf84 Linear layer Linear layer Xent w/ (cid:96)2, cf84 Non-linear MLP Xent w/ (cid:96)2, cf84  Batch Size 512# 1008# 156e28897 256e28897 1024e28897 4096  Train Epochs e288bc200 150 280 200 800 1000  Table C.1. A high-level comparison of design choices and training setup (for best result on ImageNet) for each method. Note that descriptions provided here are general; even when they match for two methods, formulations and implementations may differ (e.g. for color augmentation). Refer to the original papers for more details. #Examples are split into multiple patches, which enlarges the effective batch size. e28897A memory bank is employed.  In below, we provide an in-depth comparison of our method to the recently proposed contrastive representation learning methods:  e280a2 DIM/AMDIM (Hjelm et al., 2018; Bachman et al., 2019) achieve global-to-local/local-to-neighbor prediction by predicting the middle layer of ConvNet. The ConvNet is a ResNet that has bewen modiefac81ed to place signiefac81cant constraints on the receptive efac81elds of the network (e.g. replacing many 3x3 Convs with 1x1 Convs). In our framework, we decouple the prediction task and encoder architecture, by random cropping (with resizing) and using the efac81nal  256512102420484096Batch size75.077.580.082.585.087.590.092.595.0Top 1Temperature0.10.51.0256512102420484096Batch size909192939495Top 1Temperature0.10.51.00cA Simple Framework for Contrastive Learning of Visual Representations  representations of two augmented views for prediction, so we can use standard and more powerful ResNets. Our NT-Xent loss function leverages normalization and temperature to restrict the range of similarity scores, whereas they use a tanh function with regularization. We use a simpler data augmentation policy, while they use FastAutoAugment for their best result.  e280a2 CPC v1 and v2 (Oord et al., 2018; Hc3a9naff et al., 2019) deefac81ne the context prediction task using a deterministic strategy to split examples into patches, and a context aggregation network (a PixelCNN) to aggregate these patches. The base encoder network sees only patches, which are considerably smaller than the original image. We decouple the prediction task and the encoder architecture, so we do not require a context aggregation network, and our encoder can look at the images of wider spectrum of resolutions. In addition, we use the NT-Xent loss function, which leverages normalization and temperature, whereas they use an unnormalized cross-entropy-based objective. We use simpler data augmentation. e280a2 InstDisc, MoCo, PIRL (Wu et al., 2018; He et al., 2019; Misra & van der Maaten, 2019) generalize the Exemplar approach originally proposed by Dosovitskiy et al. (2014) and leverage an explicit memory bank. We do not use a memory bank; we efac81nd that, with a larger batch size, in-batch negative example sampling sufefac81ces. We also utilize a nonlinear projection head, and use the representation before the projection head. Although we use similar types of augmentations (e.g., random crop and color distortion), we expect speciefac81c parameters may be different.  e280a2 CMC (Tian et al., 2019) uses a separated network for each view, while we simply use a single network shared for all randomly augmented views. The data augmentation, projection head and loss function are also different. We use larger batch size instead of a memory bank.  e280a2 Whereas Ye et al. (2019) maximize similarity between augmented and unaugmented copies of the same image, we apply data augmentation symmetrically to both branches of our framework (Figure 2). We also apply a nonlinear projection on the output of base feature network, and use the representation before projection network, whereas Ye et al. (2019) use the linearly projected efac81nal hidden vector as the representation. When training with large batch sizes using multiple accelerators, we use global BN to avoid shortcuts that can greatly decrease representation quality.  0c'\n"
     ]
    }
   ],
   "source": [
    "print(text_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-render",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-original",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-calcium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-irish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-fields",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "arranged-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "proper-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_name = 'simclr.pdf'\n",
    "\n",
    "file = open(pdf_name, 'rb')\n",
    "fileReader = pdftotext.PDF(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "external-strip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:                                                A Simple Framework for Contrastive Learning of Visual Representations\n",
      "1:                                                                 Ting Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton 1\n",
      "2:                                                                   Abstract                                                                       Supervised                        SimCLR (4x)\n",
      "3:                                                                                                                                            75                     SimCLR (2x)\n",
      "4:                                                                                                              ImageNet Top-1 Accuracy (%)\n",
      "5:                                                This paper presents SimCLR: a simple framework\n",
      "6:                                                                                                                                                                             CPCv2-L\n",
      "7:                                                for contrastive learning of visual representations.                                         70    SimCLR                       MoCo (4x)\n",
      "8:                                                We simplify recently proposed contrastive self-                                                                          CMC\n",
      "9: arXiv:2002.05709v3 [cs.LG] 1 Jul 2020\n",
      "10:                                                supervised learning algorithms without requiring\n",
      "11:                                                                                                                                                                PIRL-c2x      AMDIM\n",
      "12:                                                                                                                                            65                  MoCo (2x)\n",
      "13:                                                specialized architectures or a memory bank. In                                                    CPCv2 PIRL-ens.\n",
      "14:                                                order to understand what enables the contrastive                                                  PIRL         BigBiGAN\n",
      "15:                                                prediction tasks to learn useful representations,                                           60    MoCo\n",
      "16:                                                we systematically study the major components of                                                   LA\n",
      "17:                                                our framework. We show that (1) composition of                                              55                    Rotation\n",
      "18:                                                data augmentations plays a critical role in defining                                              InstDisc\n",
      "19:                                                effective predictive tasks, (2) introducing a learn-                                             25        50    100       200      400 626\n",
      "20:                                                able nonlinear transformation between the repre-                                                       Number of Parameters (Millions)\n",
      "21:                                                sentation and the contrastive loss substantially im-\n",
      "22:                                                proves the quality of the learned representations,                  Figure 1. ImageNet Top-1 accuracy of linear classifiers trained\n",
      "23:                                                and (3) contrastive learning benefits from larger                   on representations learned with different self-supervised meth-\n",
      "24:                                                batch sizes and more training steps compared to                     ods (pretrained on ImageNet). Gray cross indicates supervised\n",
      "25:                                                supervised learning. By combining these findings,                   ResNet-50. Our method, SimCLR, is shown in bold.\n",
      "26:                                                we are able to considerably outperform previous\n",
      "27:                                                methods for self-supervised and semi-supervised                      However, pixel-level generation is computationally expen-\n",
      "28:                                                learning on ImageNet. A linear classifier trained                    sive and may not be necessary for representation learning.\n",
      "29:                                                on self-supervised representations learned by Sim-                   Discriminative approaches learn representations using objec-\n",
      "30:                                                CLR achieves 76.5% top-1 accuracy, which is a                        tive functions similar to those used for supervised learning,\n",
      "31:                                                7% relative improvement over previous state-of-                      but train networks to perform pretext tasks where both the in-\n",
      "32:                                                the-art, matching the performance of a supervised                    puts and labels are derived from an unlabeled dataset. Many\n",
      "33:                                                ResNet-50. When fine-tuned on only 1% of the                         such approaches have relied on heuristics to design pretext\n",
      "34:                                                labels, we achieve 85.8% top-5 accuracy, outper-                     tasks (Doersch et al., 2015; Zhang et al., 2016; Noroozi &\n",
      "35:                                                forming AlexNet with 100× fewer labels. 1                            Favaro, 2016; Gidaris et al., 2018), which could limit the\n",
      "36:                                                                                                                     generality of the learned representations. Discriminative\n",
      "37:                                                                                                                     approaches based on contrastive learning in the latent space\n",
      "38:                                                                                                                     have recently shown great promise, achieving state-of-the-\n",
      "39:                                         1. Introduction                                                             art results (Hadsell et al., 2006; Dosovitskiy et al., 2014;\n",
      "40:                                         Learning effective visual representations without human                     Oord et al., 2018; Bachman et al., 2019).\n",
      "41:                                         supervision is a long-standing problem. Most mainstream                     In this work, we introduce a simple framework for con-\n",
      "42:                                         approaches fall into one of two classes: generative or dis-                 trastive learning of visual representations, which we call\n",
      "43:                                         criminative. Generative approaches learn to generate or                     SimCLR. Not only does SimCLR outperform previous work\n",
      "44:                                         otherwise model pixels in the input space (Hinton et al.,                   (Figure 1), but it is also simpler, requiring neither special-\n",
      "45:                                         2006; Kingma & Welling, 2013; Goodfellow et al., 2014).                     ized architectures (Bachman et al., 2019; Hénaff et al., 2019)\n",
      "46:                                            1\n",
      "47:                                             Google Research, Brain Team. Correspondence to: Ting Chen               nor a memory bank (Wu et al., 2018; Tian et al., 2019; He\n",
      "48:                                         <iamtingchen@google.com>.                                                   et al., 2019; Misra & van der Maaten, 2019).\n",
      "49:                                         Proceedings of the 37 th International Conference on Machine                In order to understand what enables good contrastive repre-\n",
      "50:                                         Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by                sentation learning, we systematically study the major com-\n",
      "51:                                         the author(s).                                                              ponents of our framework and show that:\n",
      "52:                                             1\n",
      "53:                                               Code available at https://github.com/google-research/simclr.\n",
      "54: \n"
     ]
    }
   ],
   "source": [
    "for idx, m in enumerate(fileReader[0].split('\\n')):\n",
    "    print('{}: {}'.format(idx, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-latino",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-bride",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-contractor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pressed-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "\n",
    "pdf_name = 'pdf/simclr.pdf'\n",
    "\n",
    "text = textract.process(pdf_name, method='pdfminer')\n",
    "\n",
    "text_new = str(text)\n",
    "text_new = text_new.replace(\"\\\\n\", \" \")\n",
    "text_new = text_new.replace('\\\\x', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "remarkable-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pororo import Pororo\n",
    "\n",
    "mt = Pororo(task=\"translation\", lang=\"multi\")\n",
    "Summarizer = Pororo(task='summarization', model='extractive', lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "convenient-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trans = mt(text_new, src=\"en\", tgt=\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "canadian-franchise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'A 간단한 시각대표 컨트라스트 학습 기본 Ting Chen 1 Simon Kornblith 1 Mohammad Noruzi 1 George Hunton 1,02 2 luj 1 1) GL'이다. sc [3 대 9 0 7 5 0] 2002: viXr a bustruct is SimCLR: 시각적 표현에 대한 대조적 학습을 위한 간단한 틀을 제시한다. 최근 제안된 대조적 자기감독 알고리즘을 전문 건축가나 메모리 뱅크가 필요 없이 간소화한다. 대조적인 예측 과제가 유용한 논평을 배울 수 있는 것을 이해하기 위해 우리는 체계적으로 우리 틀의 주요 부품을 연구한다. 데이터 조작의 구성(1)이 효과적인 예측 과제인 데파크 81회에서 중요한 역할을 하고, 보고서 송환과 대조적 손실 사이에서 학습 가능한 비라인어 변신을 도입하는 것을 보여주고, (3) 대조 학습에 비해 더 큰 부분 크기와 더 많은 훈련 단계에서 비교적 학습 이러한 스파크81회를 결합해 이미지넷에서 자기초과,준초과 학습을 위한 기존 방식을 상당히 뛰어넘을 수 있다. 심클R이 배운 자체 감시 표현에 대해 훈련한 라인어 클래스피어 81명은 이전 최첨단보다 7% 상대적으로 개선된 상위 1위 정확도 76.5%를 달성해 감독한 RNet-50의 성과와 일치한다. 라벨의 1%에 불과한 스파크81n을 탑재하면 85.8%의 상위 5 정확도를 달성해 100c397개의 라벨이 적은 알렉스넷을 형성하는 퍼포먼스가 뛰어나다. 1. 인간의 감독 없이 효과적인 시각적 표현을 배우는 것은 오랜 문제다. 대부분의 주류 접근법이 세대적이거나 불범죄적인 두 가지 수업 중 하나에 빠진다. 세대적인 접근법은 입력 공간(2006년, 킹마 앤 웰링, 2013년, 굿 라운지 등 2014년)에서 모델 픽셀을 생성하거나 발생시키는 방법을 배운다. 1구글 리서치, 브레인 팀이다. 기준: 팅첸 <아이밍첸@google.com>. 2020년 PMLR 119 오스트리아 빈에서 열린 제37회 기계학습 국제회의 진행이 이뤄졌다. 저자(작가)가 2020년 저작권을 발행했다. https://gittub.com/google-research/simclr에서 1코드가 가능하다. 그림 1. 라인어 클래스피어 81명의 이미지Net Top-1 정확도는 서로 다른 자체 초음속 메스트악(이미지넷에서 해석)으로 학습한 것이다. 그레이 크로스는 UNET-50을 감독했다 우리의 방식인 심클루가 과감하게 보여준다. 다만 픽셀 수준의 세대는 계산적으로 수출이 확대되고 있어 대표 학습에 필요한 것은 아닐 수 있다. 차별적인 접근법은 감독 학습에 사용되는 기능과 유사한 비집 티브 기능을 활용해 배우지만 열차 네트워크는 무표준 데이터넷에서 인테리어와 라벨이 모두 파생되는 핑계적인 과제를 수행하고 있다. 그런 다수의 접근법은 그동안 패류에 의존해 학습된 표현의 불가능성을 제한할 수 있는 명목(2015년 도르치  et 알, 2016년 장 등, 노루지 & 파바로, 지도리 등)을 설계했다. 최근 잠재적 공간에서의 대조적 학습을 바탕으로 한 차별적 접근법이 큰 약속을 보이면서 최첨단 예술 결과(2006년 하드셀  et 알, 2014년 도소비츠키  et 아르, 오르트  et 르,2018년, 바흐만 등)를 달성하고 있다. 이 작품에서는 시각적 표현에 대한 콘트라스티브 학습을 위한 간단한 틀을 도입해 심클R이라고 부른다. 기존 작품(Figuer 1)은 SimCLR이 성과를 낼 뿐만 아니라 특화된 건축가(Bachman the Al., 2019;Hc3a9naffer, Ar., 19)도 필요 없고 메모리 뱅크(2018; Tian et., 그리고 2019; He an., Misra&Van Der Maa 우리는 어떤 좋은 대조적 리퍼레이션 학습을 가능하게 하는지 이해하기 위해 조직적으로 우리 프레임의 주요 코믹스를 연구하고 있으며, 2550102040626번 파라미터(Millions) 5560657075ImageNet Top-1 Accuracy(%) InstDicRotation BigBi LACPCv2CPCV2-LCMCAMDIMMoCoCo(2x) 그 외에도 감독 학습보다는 강력한 데이터 인터랙션으로부터 초과 대조적인 학습이 불가능하다. 배울 수 있는 비라인 변신을 선보이는 e280a2는 대표성과 대조적인 손실을 조직적으로 개선하는 방식으로 배운 표현의 질을 향상시킨다. 이280a2 대표 학습은 대조적인 크로스 기업 손실이 발생하면서 정상화된 배출 가능성과 전기 조절 온도 파라미터에서 발생한다. 이280a2 컨트라스티브 학습은 감독 대상자에 비해 더 큰 부분의 크기와 더 긴 훈련에서 비교하여 281t이다. 감독된 학습과 마찬가지로 더 깊고 넓은 네트워크에서 대조적인 학습 비공개 81t이 나온다. 우리는 이들 스파크81회를 결합해 이마-지넷 ILSVRC-2012(러시아코프스키 등 2015년)에서 자체 초,준 초등학습에 새로운 첨단학습을 달성했다. 라인어 평가 프로토콜에 따르면 심클R은 상위 1위 정확도 76.5%를 달성해 이전 첨단(2019년 Hc3a9나프 등)보다 상대적으로 7% 개선된 것이다. 이미지넷 라벨의 1%에 불과한 스파크81ntune을 달성하면 심클R은 상대적으로 10%(Hc3a9naff et, 2019) 개선이 가능한 85.8% 상위 5 정확도를 달성한다. 다른 자연 이미지 클래스피어 클래식 81ca tion 데이터에서 EPAC81n을 조회하면 심클R이 12개 데이타시트 중 10개에 대해 강력한 감독 기본선(2019년 Kornblith the All)보다 파트 이상 2. 방법 2.1. 최근 대조적인 학습 알고리즘(전시 시스템 7)이 주제로 한 컨트라스티브 러닝 프레임 인스페이스는 잠재적 공간에서 대비적인 손실을 통해 같은 데이터 사례의 다양한 견해를 극대화하는 방식으로 SIMCLR이 발휘한다. 그림 2에서 보여준 것처럼 이 틀은 다음 네 가지 주요 부품으로 구성된다. 이280a2 A 스토커틱 데이터 변형 모듈은 주어진 데ー터 사례를 무작정 변형하여 같은 사례의 두 가지 코르 관련 조회를 가져오게 하는데, 우리가 긍정적인 쌍으로 여기는 cb9cxi와 cB9cxj를 조회 이 작품에서 우리는 순차적으로 3가지 간단한 문구를 적용하고 있으며, 무작정 크로핑을 하고 원작 크기로 다시 리사이즈, 주거 컬러 왜곡, 랜덤 가우시아 모호 등을 적용한 3부에서 보여준 것처럼 좋은 성과를 거두기 위해서는 무작위 작물과 색채 왜곡이 결합되는 것이 결정적이다. e280a2 A 신경망 기반 암호화 f(c2b7)가 데이터 사례를 보여주는 레퍼레이션 베이커를 추출한다. 우리의 틀은 어떤 제약도 없이 네트워크 아키텍터의 다양한 선택이 가능하다. 우리는 일반적으로 사용되는 RNet(He et., 2016) Zig(c2b7) Hif(c2B7) cb9cxi 최대 합의 e28690e28892 대표 2892e287692 the 288bcT x T e298bc(cid.48) t zjg(c 2b7) 시각적 표현에 대한 대조적 학습을 위한 단순한 틀이다 동일한 가족(e288bc T, t(cid 48)e289bc t)과 별도의 데이터 조회 사업자 두 명이 표본으로 나눠 관련 조회를 얻기 위해 각각의 자료 사례에 적용됐다. 대조적 손실을 이용한 합의를 극대화하기 위해 기지 암호화 네트워크 F(c2b7)와 프로젝트 헤드 G(c2B7)가 훈련된다. 훈련이 끝난 후 프로젝트 헤드 G(c2b7)를 버리고 다운스트림 과제에 암호화 F(c2B7)와 대표 H를 활용한다. 하이 e2888 Rd가 평균 수영층 이후 생산량인 하이  = F(cb9cxi)  = ResNet (Cb9csi)를 얻기 위해서다. 이280a2 작은 신경망 헤드 G(c2b7)는 대조적 손실이 적용되는 공간에 지도를 표현한다. 숨겨진 층이 하나 있는 MLP를 이용해 cf83가 RELU 비선을 이루는 지 = g(i) = W(2)cp83(W(1)hi)를 얻을 수 있다. 4구간에서 보여준 바와 같이 우리는 28099년대가 아닌 지 28,099대에서 대조적인 손실을 배제하기 위해 스파크81을 제거했다. 이280a2 대조적인 손실 기능 디파크81이 대비적인 사전 과제로 예정되었다. cb9xi와 cb9cxj를 포함한 세트  { cb9xk}를 감안하면 주어진 Cb9ccxi에서 CB9cxk;k(cid54)=i를 파악하는 것이 대조적인 과제다. 우리는 미니바치에서 파생된 시험 플레이 쌍에 대한 대조적인 예측 과제를 N 사례의 마이니바치와 데파크81을 무작위로 표본으로 삼아 2N 데이터 포인트가 나온다. 우리는 명시적으로 부정적인 사례를 표시하지 않는다 대신 긍정적인 쌍(2017년 Chen et al)을 보면, 나머지 2(Ne28892 1)가 미니바치 안에서 부정적 사례로 나타나는 사례를 치료한다. Let Sim(u, v) = u(cid : 62)v/(시드 1007)u(cid : 107)v(cyd 1017) de - 2 정상화된 U와 v(i.e 사이의 점수를 주목한다. 화장품 유사성) 그 다음으로 손실 기능은 (i, j) e28892 로그 수출(sim(zi, zj)/cf84) 1 [k(cid 54) = i] 수출 (심(zim, zk)/cp84), (1) 1(k(cid 554)=i] e 2888  {0, 1}이 1 이상의 기능을 평가하는 기능이다. 스파크81 나올 손실은 모든 긍정적인 쌍(i, J)과 (j, i) 모두 미니 경기에서 계산된다. 이번 손실은 기존 작품(손, 2016, 우에트알, 오르드 등)에서 사용됐으며, 편의를 위해서는 NT-Xent(정상화된 온도 규모의 십자가 손실)라고 말한다. 2N k=1 0cA 간단한 시각대표 알고리즘 1 심클레이28099s 메인 학습  al고리즘을 위한 기본 학습이 가능하다. 입력: 배트 크기 N, 끊임없는 cf84, F, G, T 구조 등이다. 모든 ke2888  { 1에 대해서는 샘플이 나온 미니바치  { xk } N이다. …. …. N: do k=1 down the divertation te288bcT, t(cid 48)e289bcT #스파크81rstive cb9cx2 key288921 = t(xk) h2ke287921 …. …. 2N, Je28888  { 1. …. …. 2N} do, j = z(cid 1007) izj / (cid 1017) zj (시드 1077) (cyd 1900) N k = 1 [(cit 10:06) (2ke 288921, 2k) + (ciid 110) 2N deefac81ne (i, J) 2.2. 간단하게 유지하기 위해 대형 배치 사이즈와 함께 훈련하는 것은 메모리 뱅크(2018년 우에트, 2019년 헤이트, 알, 알)로 모델을 교육하지 않는다. 대신 우리는 256개에서 8192개로 훈련 배트 크기가 달라진다. 8192개의 배트 크기는 양쪽 측면에서 긍정적인 한 쌍당 1632건의 마이너스 사례가 나온다. 라인러닝 스케일링(2017년 충성도, 17일)으로 표준 SGD/M 순간을 활용할 때 대량 배트 크기의 훈련이 불안정할 수 있다. 훈련을 안정시키기 위해서는 모든 부분 크기를 위해 US 최적화기(You et al, 2017)를 사용한다. 우리는 클라우드 TPU와 함께 모델을 훈련하며 배트 크기 2.2 글로벌 BN에 따라 32개에서 128개의 코어를 사용하고 있다. 스탠더드 리사넷은 배트 정상화 티온(2015년 이오프앤세지)을 사용한다. 데이터 평행성으로 배포된 훈련에서는 BN의 의미와 발음이 일반적으로 장치당 현지에서 집계된다. 대조적인 학습에서는 같은 기기에서 긍정적인 쌍을 산출하기 때문에 모델이 지역 정보 유출을 활용해 표현을 개선하지 않고 사전 정확도를 개선할 수 있다. 우리는 이 문제를 훈련 중 모든 기기에 BN의 의미를 집계해 광고 드레스를 하고 있다. 다른 접근법에는 기기(Hee et al, 2019)를 넘나드는 데이터 사례(Shuffec82ing), 또는 레이어 규범(Hc3a9naf the All, 2019)으로 BN을 교체하는 방 2W TPU v3 코어가 128bc1.5시간 소요되는데, 100 코어에 4096대 규모의 UNET-50을 훈련하는 데 시간이 걸린다. B D C(a) 글로벌 및 지역 관점이 있다. (b) 부적절한 견해 그림 3. 솔직한 직사각형은 이미지이며, 직경이 달린 작물이다. 무작위로 꾸미는 이미지를 표본적으로 보면 글로벌 전망(Be28692 A)이나 인접한 뷰(De28792 C) 예측 등이 포함된 대조적인 과제를 샘플로 한다. 2.3. 평가 의전은 여기서 우리가 실험 연구를 위한 의전을 마련하고 있는데, 이는 우리의 틀에서 다른 디자인 선택을 이해하기 위해서다. 데이터셋과 메트릭스 우리의 대부분의 미초보전 연구(라벨이 없는 암호화 네트워크 F)는 이미지넷 ILSVRC-2012 데이터셋(러스-색소프스키 등 2015년)을 활용해 진행된다. CIFAR-10(Krizhevsky & Hinton, 2009)에서 추가로 체험하는 체험용 물질을 추가 검출할 수 있다. 이전학습을 위해 광범위한 데이터에 대한 미리 설명된 결과를 테스트하기도 한다. 배운 것을 평가하기 위해서는 냉동기지 네트워크 위에서 라인어 클래스피어 81명이 훈련되는 라인러 평가 프로토콜(2016년 장 등, 2018년 오르드 등,2019년 바흐만 등)을 따르고 있으며, 대표 품질의 대리로 시험 정확도가 활용된다. 라인별 평가를 넘어 준초,이적 학습에서도 첨단 학습과 비교한다. 결함 설정 그렇지 않으면 데이터 조작 멘토링을 위해 무작위 작물을 사용하고 (무작정 스파크82ip과 함께) 컬러 왜곡, 가우시아 모호함(세부적인 내용을 보면 A)을 재활용하는 것이 아니다. 리넷-50을 기지 암호화 네트워크로, 2단계 MLP 프로젝션 헤드로 활용해 128차원 잠재적 공간으로 대표성을 사업한다. 손실로는 NT-Xent를 활용해 학습률 4.8(0.3c397 배치사이즈/256), 체중 부패 10e288926으로 최적화했다. 우리는 10018.3 Furmer Hutter를 위해 4096 부분으로 훈련을 하고, EPAC81rt 10번지에 대한 라인어 온난화를 이용하며, 리시트 없이 화장품 부패 일정(2016년 Loshchilov & 3번. 컨트라스티브 대표학습 데이터 분석 데스크래프트 81억 개의 예측 과제가 있다. 그동안 감독과 미초보 대표 학습(크리제프스키 등)에서 데이터 분석이 널리 활용됐지만 최대 성과는 100도에 이르지 못했지만 재분석 결과는 달성돼 공정하고 스페프 81개의 과학적 발견이 가능하다. 0cA 간단한 시각대표 컨트라스틱 러닝 프레임워크(a) 오리지널(b) 크롭과 리모델링(c)크롭, 리조즈(and Evac82ip)(d) 컬러 왜곡. (drop) (e) 컬러 왜곡이다. (jitter) (f) 로테이트  {90e297a6, 180e 297A6, 270e287a6}(g) 커트(h) 가우시아 소음(i)가우시안 모호(j) 소벨 스파크81 터치 4. 연구된 데이터 조작 사업자들의 수가 많았다 각 변형은 일부 내부 파라미터(eg)로 데이터를 안정적으로 변형할 수 있다. 회전 정도, 소음 수준) 우리가 이들 사업자들을 발굴에서만 테스트한다는 점, 우리 모델을 훈련하던 발화 정책에는 무작위 작물(EPAC82ip과 리사이즈), 컬러 왜곡, 가우시아 모호 등이 포함돼 있다. (오리지널 이미지 cc-by: Von.grzanka) 2012; Hc3a9naffe et,2019; Bachman et al, 20 19) 콘트라스틱 예측 과제를 제거하는 체계적인 방법으로는 고려되지 않고 있다. 기존의 많은 사람들이 건축물을 바꿔 데파크81n 대비 예측 과제에 접근한다. 예를 들어, 헬름 에트 알이다. (2018); 바흐만 이트 알 (2019년) 네트워크 건축물에서 수용성 스파크81eld를 억제하는 것을 통해 글로벌 현지 시각 예측을 달성하는 한편 오르트 et 알 수 있다. (2018); Hc3a9naff et al. (2019) 인근 시야 예측을 통해 EPAC815 이미지 분리 절차와 콘 텍스트 집계 네트워크를 거쳐 달성한다. 이러한 복잡성을 피할 수 있다는 것을 보여주는 것은 지적 3에서 보여준 것처럼 위의 두 가지를 가리키는 단순한 태도의 가족을 만들어내는 대상 이미지의 무작위 크로핑(리사이즈)을 이러한 간단한 디자인 선택은 신경망 건축 등 다른 부품의 예측 과제를 편리하게 해결한다. 방향성 대비 예측 과제는 가족을 연장하고 이를 안정적으로 구현하는 방식으로 탈피하는 것이 가능하다. 3.1이요. 데이터 분석 작업의 구성이 좋은 논평을 배우는 데 결정적으로 필요한 것으로, 데이터의 분석이 미치는 영향을 체계적 연구하기 위해서는 여러 가지 공통된 논쟁을 한 종류의 변형은 크로핑과 리사이징(수평 스파크82pp), 로테이션(Gidaris et 2018), 컷아웃(2017년 디베리&테일러) 등 데이터의 공간/지형 변형이 포함된다. 다른 유형의 변형에는 컬러 왜곡(컬러 떨어지기, 밝기, 대조, 포화, 화려함)(2013년, 2015년), 가우시아 모래, 소벨 스파크81ling 등의 모습이 포함된다. 이 작품에서 우리가 공부하는 비전을 시각적으로 4화한다. 그림 5. 데이터 분류 또는 구성 아래 라인어 평가(ImageNet Top-1 정확도)가 한 지점에만 적용됐다. 모든 칼럼을 제외하고는 마지막으로 대각 출품이 단일 변신에 발목을 잡았고, 오프디각형은 두 개의 변형(응용 순서대로)의 구성에 해당한다. 마지막 암초 82호가 연속 평균을 연결한다. 개별 데이터 조작의 효과와 조작 구성의 중요성을 이해하기 위해 개별적으로나 쌍으로 조각을 적용할 때 틀의 성능을 조절한다. 이미지넷 영상은 크기가 다르기 때문에 우리는 항상 작물과 재규모 영상(2012년 크리제프스키 등, 2015년 제지 등)을 적용해 크로핑이 없는 상황에서 다른 작품을 연구하는 데 생명공학 81문화가 된다. 이런 불안을 없애기 위해서는 이번 불안정성을 위한 비대칭 데이터 전환 설정을 고려한다. 스피스텍81cally 우리는 항상 무작위로 수확량이 불가능한 나이에 수확하고 같은 해결책으로 거듭나고, 그 다음에는 표적 변신(speced trans)을 그 틀의 한 지점에만 적용하면서 다른 지점을 정체성(i.e.) t(xi) = 1818) Cropcuttor Color SobelNovel BlurRotate Apactorte 2차 TropCutute ColorsoberNoviel Nowelnowsblurrote 1차 변신 33.133.956.346.039.35.035.339.25.22.42.455.835.821.01.416.525.746.240.620.94.09.36.28.838.87.57.69.89.6535.15.26.6526.265.04040.2625.5188.16.46.516.250.725.39.765.52.72.7. (b) 색채 왜곡으로 말이다. 그림 6. 두 개의 다양한 이미지(i.e.)의 작물을 위한 픽셀 강도(모든 채널 전체) 역사가 다르다. 두 줄) 스파크81위 이미지는 그림 4에서 나온다. 모든 축은 같은 범위를 가지고 있다 방법 심클R 감독 1/8 69.6.0 컬러 왜곡 강도 1/4 61.0 76.7 62.6 63.2 7 7 1 64.5 7 4 1 (+ 블루) 오토액 6 1.1 77.1  Table 1. 라인별 평가를 활용한 미초보 RNet-50의 1위 정확도와 RNET-505를 감독했는데, 다양한 색상의 왜곡 tion 강도(추천 A)와 기타 데이터 변형이 가능하다. 강도 1(+Blur)가 우리의 디폴트 데이터 조작 정책이다. 리치 데이터 분석이 성능을 해치고 있다. 그럼에도 불구하고 이번 설정은 개별 데이터 조작이나 그 조작의 영향을 실질적으로 바꾸어서는 안 된다. 그림 5는 개별적이고 변형적인 구성으로 라인별 평가 결과를 보여준다. 대조적인 과제에서 모델이 긍정적인 쌍을 거의 완벽하게 파악할 수 있음에도 불구하고 단 한 개의 변신 Suffac81은 좋은 의미를 배울 수 없다는 관측이다. 반대 멘토를 구현할 때는 대조적인 예측 과제가 더 어려워지지만 대표성의 질은 극적으로 향상된다. 에픽스 B.2는 보다 넓은 세트의 편견에 대한 추가 연구를 제공한다. 무작위 작물 핑과 무작위적인 색채 왜곡 등 한 구성이 돋보인다. 이미지에서 나온 대부분의 패치가 비슷한 컬러 유통을 공유하는 것은 무작위 크로핑만을 데이터 조작으로 사용할 때 한 가지 심각한 문제라고 본다. 이미지를 구분하기 위해 자신의 토그램만으로도 수피 81ce의 색을 보여준다. 신경망은 예측 과제를 해결하기 위해 이 짧은 길을 이용할 수 있다 일반적으로 눈에 띄는 특징을 배우기 위해서는 색채 왜곡으로 꾸미는 것이 비판적이다. 3.2. 컨트라스틱 학습은 감독 학습보다 더 강력한 데이터 교육이 필요하며, 5S 초과 모델이 90개의 교육을 받아 훈련되고, e288bc 0.5% 강화된 교육 성능을 향상시키기 때문에 더 중요한 컬러 교육의 강도를 조절한다. 그림 7. 깊이와 폭이 다양한 모델에 대한 라인어 평가가 이뤄졌다. 파란색 점수의 모델은 우리 모델이 100점으로 훈련되어 있으며, 붉은색 스타 모델들은 1,000점을 대상으로 우리가 훈련하고 있고, 그린 십자가 모델 모델도 90점7점(2016년 하이 테이블 1에 나타났다 강력한 색채 변형이 초대형 비주얼 모델에 대한 라인 평가를 시기적으로 개선한다. 이런 맥락에서 초음속 학습을 이용해 발견된 세련된 정교화 정책인 오토아쿠먼트(2019년 큐브쿠엔트 알)가 간단한 크로핑+(강력한) 컬러 왜곡보다 더 잘 작동하지 않는다. 같은 세트의 조각을 가진 모듈 엘들을 훈련을 감독했을 때, 더 강한 색채 조화가 개선되지 않거나 실적까지 아프다는 것을 관찰한다. 따라서 우리의 실험에서는 감독된 학습보다 강력한(색깔) 데이터 분석에서 비대조적인 학습이 불가능하다는 것을 보여준다. 종교 전 작업에서는 자체 초과학습(2015년 도르치 등, 2019년 바흐만  et 알, Hc3a9naffer, 2919년, 아사노 등)에 데이터 전파가 유용하다고 보고했지만, 감독학습을 위해 정확성을 얻지 못하는 자료 전파는 여전히 대조적인 학습에 상당히 도움이 될 수 있다 4번. 엔코더와 헤드를 위한 건축가 4.1이다. 더 큰 모델인 'Phote7'에서 초과 대조적인 학습 'Persefac81t(이상)'은 어쩌면 심도가 높아지고 성능이 모두 개선되는 것은 놀랍지 않은 것으로 나타났다. 비슷한 스파크81회가 감독학습(그리고 2016년 등)을 실시하고 있는 가운데, 감독 모델과 라인라인 클래스피어 81명 사이의 격차는 모델 크기가 늘어나면서 미초보 모델을 훈련한 것으로 나타나 감독된 상대방보다 더 큰 모델에서 초보 학습 스파이 7교육이 더 이상 감독된 리넷(추가 B.3)을 개선하지 않는다. 050100150200250300350400450번 파라미터(조달) 50556065707580Top 1R101R101(2x) R152R52(2x,2x)R18R18(2x R18 (4x)R34R34(2x-R34) R50R50(2x.4x) 섭니다. R50Sup R50(2x) 업 R50(4x)R50*R50(2x)*R50 (4x)*0cA 간편 학습 기본 NT-Xent NT -Logital Margin Tripple uT v+/cf84 e28892 Log(cid 190) 부정적 손실 기능 V2888{v+,v2892} 수출(uT  v/cF84) log cf83(ut v+ u)/cf84 v+e28892(cid:0) Z(u)(1e2892 ex(uT v+/cp84 ) Ve28792 Exple(uTeve28692/cf 84) (cf83(e28392uT  v+/cF84) / cf85 v+ e282 cf 892 (uT ve28842 / Cf84) 부정적인 손실 기능과 그들의 졸업생들 모든 입력 자동차, I.e. U, v+, VE28892는 (시드 196) 2가 정상화됐다. NT-Xent는 e2809cNomal 온도 규모의 크로스 인트로피 28009d에 대한 약식이다. 각기 다른 손실 기능이 긍정적이고 부정적인 사례에 대한 다양한 가중치를 부여한다. 무엇을 예측할 것인가? 컬러 대 그레이스케일 로테이션 오리지 오리그 대 파손되었다 대 소벨 스파크81 http://www.andom 추측 g(h) 97.4 25.6 56.3 h 99.3 67.6 96.6 80 50 high 8. 친보호 헤드 G(c2b7)와 다양한 차원의 Z = g(h)에 대한 라인어 평가가 다르다. 이곳에서는 2048 차원의 표현 ht(예측)가 있다. 4.2입니다. 비라인어 프로젝션 헤드가 그 전에 층의 표현 질을 향상시킨 뒤 프러젝트 헤드를 포함한 중요성을 연구한다. 그(h) 헤드에 대한 세 가지 다른 건축물을 활용한 라인별 평가 결과를 보여준다. (1) 아이덴티티 맵핑, (2) 라인 프로젝트는 기존 여러 접근법(2018년 우에트 알)에서 사용했던 것처럼, (3) 바흐만 엔트 알과 비슷한 하나의 추가 숨겨진 층(RE (2019) 비라인 프로젝트가 라인 전망(+3%)보다 낫고, 전망이 없는(10% 이상) 것보다 훨씬 좋다는 관측이 나온다. 친절감 머리를 쓰면 출력 차원에 상관없이 비슷한 결과가 관측된다. 나아가 비라인어를 사용하더라도 머리 앞의 층은 그 뒤의 층보다 훨씬 더 좋은(10% 이상) 것으로 나타나는데, 그 이후에는 프로젝트 헤드 이전의 숨겨진 층이 층 이후보다 더 나은 표현임을 보여준다. 비라인 프로젝트 이전에 대표 티온을 사용하는 것이 중요하다는 것은 대조적 손실로 인한 알리타 티온의 손실 때문이라고 단언한다. 특히 Z=g(h)는 데이터 전환에 불투명하도록 훈련을 받는다. 따라서 G는 물체의 색깔이나 지향 등 다운스트림 과제에 유용할 수 있는 정보를 제거할 수도 있다. 비라인 변신 G(c2b7)를 레버그로 인해 더 많은 정보가 형성되고 유지될 수 있다. 이러한 가설을 검증하기 위해서는 H(h) 또는 g(h)를 사용하는 실험을 실시해 자존감 속에서 적용된 변신을 예측 여기서 G(h) = W(2)cf83(W(1)h)를 설정하는데, 입력과 출력 차원(i.e.)이 동일하다. 2048) 테이블3에는 h가 적용된 변신에 대한 정보를 훨씬 더 많이 담고 있고, g(h)는 정보가 없어진다. 추가 분석은 테이블 3이 가능하다. 신청된 변신을 예측하기 위해 서로 다른 송환에 대한 MLP 추가 훈련이 가능하다. 작물과 색채 조화를 제외하고는 지난 3개 줄을 선점하는 과정에서 독립적으로 로테이션(1:  {0e297a6, 90e2297A6, 180e 2907a6), 가우시아 소음, 소 블랙81로테이너 변신 등)을 추가로 추가하고 있다. h와 g(h) 모두 같은 차원의 차원이다. 2048. And B.4에서 발견된다 5번. 손실 재미와 배치 규모는 5.1이다. 대안보다 조절 가능한 온도로 정상화된 크로스트로피 손실이 좋아 물류적 손실(2013년 미콜로프 등 다른 일반적으로 사용되는 대조적인 손실 기능에 비해 NT-Xent 손실을 비교한다). 손실 기능 입력 졸업자는 물론 객관적인 기능을 테이블2에서 보여준다. 졸업생을 보면 1)(시드 6)2 정상화(이하 1)를 관찰한다. 온도가 효율적으로 다른 사례와 함께 화장품 유사성이 있으며, 적절한 온도는 크로스엔트로피와 달리 모델이 하드 부정에서 배우는 데 도움이 될 수 있다. 그리고 2) 기타 비젝티브 기능은 상대적으로 결과적으로 이러한 손실 기능에 대해서는 반하드 마이너스 채굴(2015년 기준, 스크로프 등)을 적용해야 하는데, 전체 손실조건에서 졸업자를 계산하는 기능이 있어야 하며, 준하드 부정적인 용어(i.e., 손실마진과 거리가 가장 가깝지만 긍정적 박람회를 만들기 위해서는 모든 손실 기능을 위해 동일한 (시드 1966) 2 정상화를 사용하고, 초파라미터를 조율해 최고의 성과를 보고하는 한편 (세미하드) 마이너스 채굴이 도움이 되는 반면 최상의 결과는 여전히 NT-Xent 손실보다 8개의 디테일은 B.10  Appendix에서 확인할 수 있다 단순히 우리는 단 한 가지 시각에서 부정적인 부분만 고려한다 326412825651210242048 프로젝트 출력 차원성 3040506070Top 1 Prosult LinearNon-linerNornone0cA 간편학습 마르긴 NT-Logi. 마르진(sh) NT-Logi이다. 57.5 57.9 63.9 51.6 테이블 4. 손실 기능이 다른 모델을 대상으로 훈련한 라인어 평가(톱1)다. 이2809ch28009d는 반하드 마이너스 채굴을 이용한다는 뜻이다. (시드 6) 2 규모? 예, Cf84 0.05 0.1 0.51 100 Etropy Contrastive Ac. 1.0 4.5 8.2 8.3  0.5 90.5 68.8 29.1 91.7 92.1 톱1 59.7 64.4 60.7 58.0 57.2 7.0  Table 5. NT-Xent 손실을 위해 2 규범과 온도 cf84의 다른 선택을 가지고 훈련된 모델에 대한 선별평가가 이뤄졌다. 대조적인 유통은 4096건이 넘는 예다 그림 9. 라인어 평가 모델(ResNet-50)은 차별화된 배트 크기와 부드러움으로 훈련을 받았다. 각 술집은 처음부터 단 한 번씩 달린다.10 우리는 다음으로 (시드 6) 2 정상화(i.e.)의 중요성을 테스트한다. 우리의 디폴트 NT-Xent 손실에서 화장품 대 제품)과 온도 Cf84가 유사하다. 테이블 5는 정상화와 적절한 온도 스케일링 없이 성능이 시그니프 나이프 81을 중심으로 더 나쁘다는 것을 보여준다. (Ciders6) 2 정상화 없이는 대조적 과제 정확도가 높지만 결과적인 대표성은 라인 평가 하에서 더 나빠진다. 5.2. 컨트라스티브 학습 코드파크 81t(이상)는 더 큰 배트 크기와 더 길게 훈련되는데, 그림 9에서는 모델들이 다른 배트의 숫자를 위해 훈련을 받을 때 배트량 크기의 영향을 우리는 훈련 시간이 적은 곳(eg)이라는 것을 알파크81일 말한다. 크기가 커진 100개 크기는 작은 크기에 비해 신호탄 81센티미터의 장점이 있다. 훈련 단계, 에포치가 더 많아지면서 크기가 다르게 줄어들거나 사라지는 격차가 발생하면서 프로 측면의 타구가 무작위로 재탄생하고 있다. 이곳에서는 10A 라인어 학습률 스케일링이 사용되는 것과 대조적이다. 사각형 근본학습률 스케일링을 활용한 수치 B.1은 작은 배트 크기의 성능을 향상시킬 수 있다. ResNet-50을 활용한 건축 방식 : RESNeT-50 지역 Agg. Renet-50 MoCo ResNet-50 PINET-50 CPC v2 SimCLR (시간) Rennet - 50 The Arters : Revente-50(4c397) BigBi AMDIM Cust-ResNet Risnets-50 이미지 네트워크는 다양한 자기 감시 방식으로 배운 레퍼레이션에서 훈련된 라인어 클래스피어 81명의 이미지를 배웠다. 방식 건축 ResNet-50 감독 기본 방식의 다른 레이블 프로그래밍: RESNET-50PICO-labelle ReSNet - 50 VAT+Entropy Min. UDA(w.RandAug) ResNet-50 FixMatch(w. Landag) Lessnet-50 S4L(Rot+VAT+En)이다. M.) ResNet-50(4c397) 방식만 사용하는 대표 학습 방식: IntDisc RONET-50 RESNet-161(e28897)CPC v2 RROSNet - 50 SIMCLR (ORS) RNESNeT-50 ROWNET - 10% TOP5 48.4 81.6 47.0 - 39.25.2 57.2 77.9 75.8 82. 라벨이 거의 없이 훈련된 모델들에 대한 이미지넷 정확도가 높다. 감독된 학습(2017년 기준)은 대조적으로 학습하기 위해 더 큰 부분의 크기가 더 부정적인 사례를 제공해 융합을 촉진(i.e.)한다. 주어진 정확도를 위해 적은 단계와 조치를 취하고 있다) 훈련이 더 길어지는 것도 더 부정적인 사례를 제공해 결과를 개선한다. 추가 B.1에서는 훈련 단계가 더 길어진 결과가 제공된다. 6번. 이번 서브섹션에서 스테이트 아웃과 비교하면 콜스니코프 에트 알과 비슷하다. (2019); 그는 알 것이다 (2019) 3개의 다른 숨겨진 층(1c397, 2c398, 4C397)에서 유네트-50을 사용한다. 더 잘 융합하기 위해서는 이곳의 우리 모델들이 1,000개의 시대를 위해 훈련을 받고 있다 선별 평가 88.97 CIFA100 SUN397 이미지넷 50(4c397) 모델을 대상으로 12개 자연이미지 클래스피어 81개 데이터 전반에 걸쳐 감독한 기본 기준으로 우리 자체감독 접근법의 이전학습 성과를 비교한다. 결과는 가장 좋은(p > 0.05, 복식 테스트)보다 신호 81이 더 나쁘지 않은 결과가 과감하게 나타난다. 실험적인 내용과 표준 UNET-50으로 결과를 보여주는 B.8 보기가 있다. 2019; 선별평가 설정(추천 B.6)에서 천 이트 알(2018)이 나왔다. 테이블 1은 서로 다른 방식 중에서 수치 비교가 더 많이 나타난다. 표준 네트워크를 활용해 기존 방식에 비해 스피스텍81cally 설계된 아키 테크놀러지가 필요한 것과 비교해 훨씬 더 좋은 결과를 얻을 수 있다. 우리의 RNET-50(4c397)으로 얻은 최고의 결과는 감독 미숙한 RNet-50과 일치할 수 있다. 반 초과 학습 우리는 지하이에 알을 따른다 (2019)와 표기된 ILSVRC-12 훈련 데이터 1% 또는 10%의 표본을 수업 균형 방식으로 표기(각 수업별 e288bc128 이미지)한다. 11 우리는 단순히 정규화 없이 표시된 데이터에 기지 네트워크 전체를 스파크 81ne으로 구분하는 것만 보면 된다. 테이블 7에서는 최근 방식(2019년 Zhay et, 2018년 Xie et al, 손에트 2020년, 우에트 Al, Donahue & Simonyan, 2919년, Misra & Van Der Maten, Hc3a9naf 등)에 대한 비교 결과가 초파라미터(조작 포함)를 집중적으로 검색해 감독한 기본선(2019년 Zhie Et All)이 강하다. 다시 한번 우리의 접근방식은 라벨의 1%와 10% 모두 첨단보다 시그니처 81가지로 개선된다. 흥미롭게도 전체 이미지넷에 대한 우리의 미리 설명한 리넷-50(2c397,4C397)을 조절하는 것도 시그니처 81을 중심으로 더 나은 훈련(최대 2%,  Appendix B.2)이다. 이전 학습 라인어 평가 아이온(Fefac81x 기능 추출기)과 스파크81ntuning 설정 모두 12개의 천연 이미지 데이터를 통해 이적학습 공연 매뉴얼을 평가한다. 사람들이 눈에 띄는 코르블리트 등이 있다 (2019) 우리는 모델 데이터넷 조합별 초파라미터 튜닝을 수행하고 인증 세트에서 최고의 초퍼라미터를 선택한다. 유네트-50(4c397) 모델로 결과가 나온다. 스파크81원을 조작할 때 우리 자체 초과 모델 시그니어팩81이 5개 데이터 시스템에서 초기 기준을 초과하는 반면 감독 기본선은 2개(i.e)에 불과하다. 애완동물과 꽃) 나머지 5개 데이터에서는 모델이 통계적으로 동결된다. 기준 UNET-50 건축물과 함께 실험적인 내용은 물론이고 결과가 나온 결과는 B8  Appendix에서 제공된다. 샘플 및 정확한 지하철 세부 내용은 https://www.tensorefac82ow.org/datasett/cataget/imagent 2012_subett에서 확인할 수 있다. 7번. 관련 작업은 작은 변신 속에서 이미지를 표현하는 것이 서로 동의한다는 생각이 베커앤혼턴(1992년)으로 돌아간다. 최근 데이터 조작, 네트워크 건축, 콘트라스티브 손실에 대한 광고 균형을 이용해 연장한다. 비슷한 일관성 아이디어지만 반 감독 학습(Xie et all, 2019; Bert Hellot teal, 19) 등 다른 맥락에서 반 레이블 예측이 탐구됐다. 수작된 구실 과제들 최근 자체 감독 학습 재조사는 상대적 패치 예측(도르치 등, 2015), 지그사 퍼즐(2016년 노루지 & 파바로, 색채 조각(20만6000년), 회전예측(Gidariza tion,2018년 Chen et al, 2919년) 등 아트리피어 81개의 명목으로 시작됐다. 더 큰 네트워크를 통해 좋은 결과를 얻을 수 있고 더 긴 열차 수용(2019년 콜리니코프 등)이 가능하지만, 이러한 핑계는 다소 광고체류에 의존해 학습된 발언의 불가능성을 제한한다. 컨트라스틱 비주얼 표현 학습 다시 하드셀 에트 알로 데이트하고 있다 (2006년) 이러한 접근법들은 부정적인 쌍에 대해 긍정적으로 짝을 맞추는 방식으로 오렌지 테이션을 배운다. 이 라인들을 따라 도소비츠키 에트 알이 있다. (2014년) 각 학원을 특징 베이터(패러메트릭 형태)가 대표하는 수업으로 취급하는 방안을 제시한다. 우에트알 (2018) 메모리뱅크를 이용해 최근 여러 논문에서 채택,연장된 접근 방식인 렌스 클래스 대표 베이터를 저장할 수 있도록 제안하고 있다. 다른 작품들은 메모리뱅크(2017년 도르치앤지서만, 2019년 예 et al, 지아 등) 대신 음성 샘플을 위한 인바치 샘플 사용을 탐구한다. 최근 문학은 자신들의 방식이 잠재적 정보를 극대화하기 위한 성공을 시도해 왔다(2018년 오르드 알, 2019년 Hc3a9naff the Al, Hjelm et al, 2918년, 바흐만 등). 다만 대조적 접근의 성공이 상호 정보에 의해 결정되거나, 대비적 손실(2019년 스크린엔트 알) 형태의 스펙 81c 형태로 결정될 경우는 명확하지 않다. 시각대표 컨트라스틱 학습을 위한 0cA 간편 프레임워크 우리는 스펙 81c 순간이 다를 수 있지만 우리의 틀 작품의 거의 모든 개별 부품이 이전 작품에 등장했음을 주목한다. 기존 작품과 관련된 우리의 프레임 작품의 우위는 단 한 가지 디자인 선택이 아니라 그들의 구성으로 설명된다. 우리는 기존 작품 C. 8에서 디자인 선택을 만화 선점 비교한다. 이 작품에서는 단순한 틀과 대조적인 시각 표현 학습을 위한 입장을 제시한다. 부품을 꼼꼼히 공부하고 다른 디자인 선택의 효과를 보여준다. 우리의 스파크81회를 결합해 기존의 자기감독, 준초과, 이적학습 방식을 놓고 상당히 개선했다. 데이터 조작 선택에만 이미지넷에 대한 표준 감독 학습, 네트워크 끝에 비라인 헤드 사용, 손실 재미 tion 등과 다른 접근법이 있다. 이 단순한 틀의 강점은 최근 관심이 급증하고 있음에도 불구하고 자체 초과 학습은 여전히 저평가되고 있다는 것을 시사한다. 우리는 샤오와 자하이, 라파엘 맥3bcler, 야니 이오누에게 드래프트에 대한 피드백에 감사드립니다. 토론토 등에서 구글 연구팀의 일반적인 지원에도 감사하다. 아사노, Y.M., 로프레흐트, C., 베달디 등을 참고한다. 자기감시에 대한 비판적인 분석, 또는 단 한 가지 이미지로 배울 수 있는 것이 무엇인지 등이다. 2019년 ARXIV Print ArXive 1904.1332가 출시됐다. 바흐만, P., Hjelm, R. D., 부치월터는 상호 정보를 조회 전반에 걸쳐 극대화하는 방식으로 레퍼런스를 배우고 있다. 신경정보처리시스템의 고도화에서는 pp가 있다. 2019년 15509e2809315519년이다. Becker, S., Hinton, G. E. 셀프가 무작위 고정관념으로 표면을 드러내는 신경망을 조직하고 있다. 자연은 1992년 355년(6356년) 1e28093163년이다. 버그, T, 류, J, 이, S. W, 알렉산더, M. L., 제이콥스, D. W., 벨롬우르, P.N. N. 나프: 대규모 스파크81ne급 조류의 시각적 범주화다. 컴퓨터 비전 및 패턴 인식(CVPR)에 관한 IEEE 컨퍼런스에서는 PP가 열렸다. 2019e280932026. 2014년 IEEE. 버트헬로트, D., 칼리니, N., Good Collini, I., Peoper Not, N, 올리버, A., 라벨, C. A. 믹스매치: 준감독 학습에 대한 홀로틱한 접근법이다. 신경정보 검출 시스템의 진보에서는 pp가 있다. 2019년 5050e 280935060년이다. 보사드, L., 구라민, M., 반 구울, L 푸드-101e28093 차별화 성분이 무작위 숲과 함께 있다. 컴퓨터 비전에 관한 유럽 콘퍼런스에서, pp. 446e28093461입니다 스프링거, 2014년. 천,T,선,Y,시,Y,홍 등이 신경망을 기반으로 한 협업 스파크81 입력을 위한 샘플 전략에 나섰다. 제23회 ACM SIGKDD 국제 지식발굴 및 데이터 마이닝 컨퍼런스 진행 기간 PP가 열렸다. 2017년 767e2809376이다. 천, 티, 자이, 엑스, 리터, 미, 루시크, 엠, 휴슬비, N. 셀프가 보조회전 손실을 통해 간을 감독했다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스가 진행된다. 2019년 12154e2809312163년이다. 심포이, M, 마이니, S, 코키노스, I, 모하메드, S., 베달디, A. 야생 속에서 교과서를 다듬었다 컴퓨터 비전 및 패턴 인식(CVPR)에 관한 IEEE 컨퍼런스에서는 PP가 열렸다. 3606e28093 3613입니다 2014년 IEEE. 쿠바쿠, ED, Zoff, B., 마네, D., 바스데반, V., 레, Q. V. 오토오플레이: 데이터로부터의 학습 전략이다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스가 진행되고 있다. 2019년 113e 28093123입니다 디브리, T., 테일러, G. W.는 컷아웃과 함께 소모적인 신경망의 정규화를 개선했다. 2017년 ARXIV Print ArXive 17708.04522가 출시됐다. 도어치, C., 지저만, A. 멀티태스크 자체 초등 시각학습이 진행된다. 컴퓨터 비전에 관한 IEEE 국제 콘퍼런스가 진행되고 있다. 2017년 2051e280932060년이다. 도어치, C, 괌타, A, 에브로스, A 등이 참여했다. 맥락 예측을 통해 A. 미초보 시각 표현 학습이 이뤄진다. 컴퓨터 비전에 관한 IEEE 국제 콘퍼런스가 진행되고 있다. 2015년 1422e280931430년이다. 도나휴, J, 시모니안, K. 스케일의 대형 비타리얼이 tion 학습을 대표한다. 신경정보처리시스템의 고도화에서는 pp가 있다. 2019년 10541e2809310551년이다. 도나휴, J, 지아, Y, 비닐스, O, 호프만, J., 장, N., 땡, E., 다렐, T. 데카프: 세대적인 시각적 인식을 위한 심화적 활성화 기능이 특징이다. 기계학습에 관한 국제 컨퍼런스, pp. 2014년 647e28093655년이다. 도소비츠키, A, 스프링크, J. T., 리드밀러, M., 브록스, T. 차별적 미초보 특징은 소모적인 신경망으로 학습하는 것이 특징이다. 신경정보처리 시스템의 발전에 있어서는 pp이다. 2014년 766e2809374년이다. 에버햄, M, 반 골, L., 윌리엄스, C. K., 캐슬링, J., 지저만, A. 패스컬 비주얼 오브젝트 클래스(변호사) 도전이 이뤄졌다. 2010년 국제 컴퓨터 비전, 88(2):303e2809338. 페이-페이, L., 포르투갈, R., 페로나, P. 러닝 세대적인 비주얼 모델 몇 가지 훈련 사례에서 나온다. 101개 항목 부문에서 점점 더 많은 베이니시아 접근법이 시험되었다. 2004년 비전을 기반으로 한 세대모델에서 컴퓨터 비전 및 패턴 인식(CVPR) 워크숍에서 열린 IEEE 컨퍼런스에서다. 기드라리스, S, 싱, P, 코모다키스, N. Unsuperviden tatement 학습은 이미지 로테이션을 예측해 학습한다. 2018년 ARXIV Print ArXive 1983.0728이다. 좋은 선생님, I., Pouget-Abadie, J., 미르자, M., Xu, B., Warde-Farley, D., Ozair, S., 쿠르빌, A., 그리고 바르시오, Y. 세대적인 네트워크가 있다. 신경정보처리 시스템의 발전에 있어서는 pp이다. 2014년 2672e280932680년이다. 0cA 간단한 비주얼 대표 성실함, P., 달러카3a1r, P, 길릭, R., 노르데시, 피, 위솔로스키, L., 교로라, A., 툴로치, A, 지아, Y., 그리고 K. 정확하고 대형 미니바치 sgd: 1시간 만에 트레이닝 모습이다. 2017년 ARXIV Print ArXive 17706.02677이다. 하드셀, R., 초프라, S., 레쿤, Y. 디펜션 감축 등이 불안정한 맵핑을 배우는 방식으로 이뤄졌다. 2006년 컴퓨터 비전 및 패턴 인식(CVPRe2809906)에 대한 IEEE 전산 소비자 불안 콘퍼런스에서 2, ppp. 1735e280931742입니다. 2006년 IEEE 그, K, 장, X, 렌, S, 선, J 등이 있다. 이미지 인식을 위한 잔류학습이 깊다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스가 진행되고 있다. 2016년 770e2809378년이다. 그, K, 판, H, 우, Y, 시, 에스, 기적, R. 모멘텀 대조는 초대형 비주얼 표현 학습을 위한 것이다. 2019년 ARXIV Print ArXive 1911.05722가 출시됐다. Hc3a9naff, O.J., Rasavi, A., Doersch, C., Eslami, S., Oord, A v. D. 데이터 방식 81 지적 이미지 인식은 대조적인 예측 코딩으로 가능하다. 아엑시브 프린트 아크시브 1905.09272, 2019년이다. 힌튼, G.E., 오산도, S., 텔레, Y.W.는 깊은 신념망을 위한 빠른 학습 알고리즘이다. 신경계산, 2006년 18(7)1527e28093년 1554년이다. Hjelm, R. D., Fedorov, A., Lavoi-Marchildon, S., C., K., Bachman, P., Trischler, A, Buncio, Y. 상호 정보 추정과 극대화로 깊은 반송을 배운다. 2018년 ARXIV Print ArXive 1988.06670이 출시됐다. Howard, A. G. 일부는 깊은 신경망을 기반으로 한 이미지 클래스피어 81 분야에 대한 개선이 있다. 2013년 ARXIV Print ArXive 13312.5402가 출시됐다. 이오프, S, 제지, C매치 정상화: 내부 코바리트 전환을 줄여 딥네트워크 훈련을 가속화한다. 2015년 ARXIV Print ArXive 1502.03167이다. 지, 엑스, 헨리, J. F. 그리고 베달디, A. 인바라이언트 정보는 초대형 이미지 클래스피어 81분위기와 세분위기 등을 대상으로 한다. 컴퓨터 비전에 관한 IEEE 국제 콘퍼런스가 진행되고 있다. 2019년 9865e280939874년이다. 킹마, D. P. 그리고 웰링, M. 오토 앤코딩 변주만이 있다. 2013년 ARXIV Print ArXive 13312.614가 출시됐다. 코르스니코프, A., 자이, X., 비어, L. 리베이스 등이 IEEE 비주얼 대표 학습을 진행하고 있다. 컴퓨터 비전과 패턴 인식에 관한 컨퍼런스, pp. 2019년 1920년 280931929년이다. Kornblith, S., Shlens, J., Le, Q. V. 더 나은 이적에 대한 IEEE 컨퍼런스 진행에 더 좋은 이미지넷 모델을 할 수 있을까? 컴퓨터 비전과 패턴 인식, pp. 2019년 2661e280932671년이다. 크라우스, J, 덩, J., 스타크, M., 페이-페이는 스파크81ne 교육 차량의 대규모 데이터셋을 수집하고 있다. 2013년 미세훈련 비주얼 지정 2차 워크숍에서 열렸다. 크리셰프스키, A, 혼턴, G. 등이 작은 이미지에서 여러 겹의 특징을 배우고 있다. 2009년 토론토대학교 기술보고서가 나왔다. URL https://www.ch.toronto.edu/~kriz/ 학습-features-2009-TR.pdf. 로시칠로프, 나, 허터, F Sgdr: 스토커틱 졸업생이 따뜻한 리조트로 내려간다. 2016년 ARXIV Print ArXive 10:08.03983이다. 마텐, L. v. D., 혼턴, G. 비주얼화 데이터가 t-sne을 활용한 것이다. 2008년 기계학습 연구 NAL, 9(Nov) 2579e280932605. 마이지, S, 칸발라, J, 라투, E, 블라시코, M, 베달디, A. 미세한 훈련을 받은 비주얼 클래스피어 81 분위기의 항공기 운항이 이뤄졌다. 2013년 기술 보고서 미콜로프, T, 첸, K, 코로도, G, 딘, J. 에스파크81 과학적 에스티메이션 단어가 베이터 공간에서 표현된다. 2013년 ARXIV Print ArXive 1301.3781이 출시됐다. 미스라, 나, 반데르 마텐, L. 등이 구실 없는 발언을 했다. 2019년 ARXIV 1912.0191이다. 자기 초등학습 - ARXIV 프린트 닐스백, M.-E., 지저만, A. 자동화된 EPAC82WAR 클래스피어 81 분위기가 많은 수업을 둘러싸고 있다. 컴퓨터 비전에서는 2008년 그래픽 & 이미지 프로세싱이 진행된다. ICVGIPe 2809908입니다 6차 인도 회의, pp. 722e28093729입니다 2008년 IEEE. 노루지, M, 파바로, P. 미스터디한 학습은 지스aw 퍼즐을 풀어 시각적 리프레이션을 배운다. 유럽 컴퓨터 비전 컨퍼런스에서는 PP가 열렸다. 69e2809384 스프링거, 2016년. 오드, A v. D., 리, Y., 비닐스, O. 대조적인 예측 코딩으로 대표 학습을 한다 2018년 ARXIV Print ArXive 1987.03748이다. 박시, 오엠, 베달디, A, 지저만, A., 자와하르, C. 고양이, 개 등이 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 관한 IEEE 컨퍼런스에서는 PP가 열렸다. 3498e280933505입니다 2012년 IEEE. 러시아프스키, O., 덩, J., 수, H., 크라우스, 제이, 시아쉬, S., 마, 에스, 황, Z., 카르파시, 에이, 코슬라, A., 버른슈타인, M., 등이다. 대규모 시각 인식 도전이 불가피하다 국제 컴퓨터 비전 저널인 2015년 115(3) 211e28093252년이다. 슈로프, F., 칼레니첸코, D., 필빈, J. 파크리에르: 안드리에이드 81ed In Proveding Everidgent 등이 얼굴 인식과 조롱을 위해 배출되었다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스의 결과, pp. 2015년 815e28093823입니다. 시모니안, K., 지저만, A씨 등이 참여했다. 대규모 이미지 인식을 위한 매우 심층적인 네트워크가 있다. 2014년 ARXIV 프린트 아크시브 09.1556이 출시됐다. 손씨, K씨는 다학년 N페어 손실 객관적으로 심화된 메트릭 학습을 개선했다. 신경정보처리 시스템의 발전에 있어서는 pp이다. 2016년 11865년 186e 28093년이다. 손, K, 헬로트, D, 리, C, L, 장, Z, 칼리니, N, 쿠바쿠, E. D., 쿠라킨, A, 창, H., 라펠, C. 픽스매치: 심플리 파이프 준 초과 학습 일관성과 콘셉트 81dence가 있다. 2020년 ARXIV Print ArXiv 2001.07685. 스지디, C, 류, W, 지아, Y, 세르만넷, P, 리드, S, 앙겔로프, D, 에르한, D., 반저우케, V., 라비노비치, A. 소용돌이로 더 깊이 들어갔다 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스가 진행되고 있다. 2015년 1e280939입니다. 텐, Y, 크리쉬난, D, 이슬라, P 컨트라스티브 멀티뷰 코딩이다. 2019년 ARXIV 프린트 아르엑시브 1906.05849이다. 크리셰프스키, A., 솔츠케버, I., 혼턴, G. E. 유전자 클래스피어 81 세이션은 깊은 복합 신경망을 갖추고 있다. 신경정보처리 시스템의 발전에 있어서는 pp이다. 2012년 1097e280931105년이다. 티샤넨, M, 돌, J, 루벤스턴, P. K., 겔리, S., 루시크 등에서는 대표학습을 위한 상호 정보 극대화에 대한 논의가 이뤄졌다. 2019년 ARXIV 프린트 아르엑시브 1907.13625이다. 시각대표 우, Z, Xiong, Y, 유, S.X., 린, D. 불초등학습 기본 0cA 단순 학습 프레임워크는 비초등학교 차별을 통해 학습한다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스의 진행 - pp. 2018년 3733e280933742년이다. 샤오, J, 헤이스, 제이, 잉거, K. A., 올리바, A. 그리고 에살바 A. 태양 데이터베이스 : 대규모 장면 인식이 원시에서 동물원으로 이어진다. 컴퓨터 비전 및 패턴 인식(CVPR)에 관한 IEEE 컨퍼런스에서는 PP가 열렸다. 3485e280933492입니다. 2010년 IEEE. 시, Q, 다이, Z, 호비, E, 룽, M, 레, Q. V. 운수 등 데이터 공개가 이뤄지고 있다. 2019년 ARXIV Print ArXiv 1904.12848이다. 예, M, 장, X, 윤, P. C., 창, S. F. 불초격 배출 학습 등을 투명하고 확산하는 연습 기능이 특징이다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스가 진행된다. 2019년 6210e280936219년이다. 당신, Y., 기트만, I., 진스부르크, B. 대규모 분해 네트워크 훈련 2017년 ARXIV Print ArXive 17708.03888이다. 자하이, X, 올리버, A, 콜리니코프, A., 비어, L. S4l: 자기감독 준초등학습이다. 2019년 10월 IEEE 국제 컴퓨터 비전 컨퍼런스(ICCV)에서 열렸다. 장, R, 이슬라, P, 에브로스, A 등이 참여했다. 에이 색채 있는 이미지 컬러이자 티온이다 컴퓨터 비전에 관한 유럽 콘퍼런스에서, pp. 649e28093666입니다 스프링거, 2016년. 주앙, C., 자이, A. L., 야민스, D. 지역 집계 등이 시각적 배출물질에 대한 초보 학습을 위한 것으로 나타났다. 컴퓨터 비전에 관한 IEEE 국제 콘퍼런스가 진행되고 있다. 2019년 6002e280936012년이다. 0cA 간단한 시각대표 관제학습 기본 A. 데이터 8월 세부사항은 우리의 디폴트레이닝 설정(우리 최고의 모델을 훈련하는 데 사용된다)에서 무작위 작물(리사이즈와 랜덤 스파크82ipp), 무작정 컬러 왜곡, 데이터의 흐릿함이 자료 조작으로 활용된다. 이 세 가지 구체적인 내용이 아래에 제공된다. 랜덤 작물 224x224까지 재활용해 표준 인셉션 스타일의 무작위 크로핑(2015년 제지 et al)을 사용한다. 원래 크기의 무작위 작물(면적 0.08에서 1.0으로 일률적)과 원래 측면 비율(3.4∼4/3의 기하)의 임의 측면비율이 만들어진다. 이 작물은 원래 크기로 전환된 스파크81나리다 이는 텐소리파크 82w에서 e2809c 슬림프로세싱으로 도입되어 왔다. 인스턴트_proprocession.distort_bing_box_clust_creprosed_criproraced_supplesseding.dessorch_back_crupprojection_ 여기에 무작위 작물(리사이즈)은 항상 수평,좌우 스파크82ip이 50% 확률로 뒤를 이었다. 이것은 도움이 되지만 필수적이지는 않다. 이를 우리의 디폴트 조작 정책에서 제거함으로써 상위 1위 라인 평가는 64.5%에서 63.4%로 100도에서 훈련된 우리 RNet-50 모델이 떨어진다. 색채 왜곡 색채는 색채 불안과 색채가 떨어지는 방식으로 구성된다. 우리는 일반적으로 더 강력한 색채 불안감이 도움이 되기 때문에 강도 파라미터를 설정한다. 텐서플로우를 이용한 컬러 왜곡을 위한 가명코드가 다음과 같다. tf defe colle_distature(image, s=1.0): # 이미지는 가치 범위가 있는 수십 가지다. #s는 색채 왜곡의 강점이다 디스플레이 _ jitter(x) : # 적용할 때마다 이를 따르는 질서를 풀어줄 수 있다. x = tf.image. random_brights (x, max, delta=0.8*s) x  = tf . mange. 컬러_jitter = 트랜스포메이션.ColorJiter(0.8*s, 0.8 *s,  0.2*s) rnnd_collor_jinter=trandomApply([coler_juter], p=0.8) Rnd_gray=trons.compose([cland_destort], 12Our colder 및 결과는 Tensorefac82w를 기반으로 한다. 0cA 간단한 비주얼 대표 컨트라스틱 러닝 프레임워크 _ 디스토르트 가우시아 모호함 이런 반사는 우리의 디폴트 정책에 있다. 유네트-50 훈련이 63.2%에서 64.5%로 100명에 달하는 훈련을 개선하는 등 도움이 되었다고 한다. 가우시아 핵심을 이용해 당시의 50% 이미지가 흐려진다. 무작위 샘플 cf83 e28888[0.1, 2.0]으로 이미지 높이/위드 10% 규모의 핵심 크기가 설정됐다. B 추가 실험 결과 B.1. 배치 크기와 트레이닝 스텝  Figure B.1은 크기가 다른 배트 크기, 훈련 크기로 훈련했을 때 라인 평가에서 상위 5 정확도를 보여준다. 이번 결론은 이전에 보여준 상위 1 정확도와 매우 유사한 결론인데, 여기서 다른 배트 크기와 훈련 단계의 차이가 조금 작아 보인다는 점을 제외하고는 수치 9과 피지 B.1 모두 다양한 배트 크기의 훈련을 할 때 (2017년 충성도 1등)과 비슷한 학습률의 라인 스케일링을 사용한다. 라인러 학습률 스케일링이 SGD/M순간 최적화로 인기를 끌고 있지만, 우리는 EPAC81을 기대하는 것이 더욱 바람직한 것으로 나타났다. 근본학습률 스케일링을 통해 라인 스케줄 케이스에서 배팅레이트 0.3c397 배치Size/256 대신 배치 사이즈가 있지만, 배트 크기 4096 (당분간 디폴트 부츠 크기) 모두 스케치 방식으로 학습률이 동일하다. 사소한 배트 크기로 훈련된 모델의 성능을 향상시키고 작은 배트량으로 실력을 개선한다는 관측이 나오는 탁 B.1에서 비교가 제시된다. 학습률 = 0.075c397e2889a 배치 크기 \\Epochs 100 256 512 10248 4096 8192 57.5/62.8 63.8 / 66.6 64.6 / 서로 다른 배트 크기와 훈련 크기 아래 선별평가(톱1)가 이뤄진다. 슬래시 신호 왼쪽에는 라인어 LR 스케일링으로 훈련된 모델이 있으며, 오른쪽에서는 스퀘어 뿌리 LR을 스케줄링하며 모델을 훈련하고 있다. 결과는 0.5% 이상이면 과감하게 나타난다. 스퀘어 뿌리 LR 스케일링은 작은 배트 크기에 더 좋은 효과가 있다. 우리는 또 대량(최대 32K) 규모가 크고 길게(최고 3200km) 훈련을 하며 사각형 뿌리 학습률이 스케일링된다. 수치 B.2에서 보여주는 연주는 8192개의 배트 크기에 정확하게 포함된 것으로 보이며, 훈련은 여전히 더 오래 지속적으로 신호 81개의 성능을 개선할 수 있다. 그림 B.1. 리니어 평가(Top-5)는 리넷-50의 크기와 크기가 다른 트레이닝을 받아 훈련을 받았다. 각 술집은 스쳐 지나가는 단 한 점이다 상위 1-1 정확성을 위해 9번 수치를 보세요 그림 B.2. 리니어 평가(Top-1)는 Resnet-50이 다른 배트 크기와 더 길게 훈련을 받았다. 여기에는 라인어 하나가 아닌 사각형 뿌리 학습율이 활용된다. 1002030040050060080090010016003200Trance 6062646687072Top 1Batch 2565120242420484096896816384327680cA 간단한 시각대표 학습 기준 B.2. 데이터 조작의 방향성이 더욱 향상되고 본문(테이블 6,7)에서 우리가 가장 좋은 성과를 낼 수 있다. 라인어 평가 의전을 위해서는 데이터 보다 넓은 기준으로 훈련된 RNet-50 모델(1c397, 2c398, 4c3997)이 각각 70.0(+0.7), 74.4(+0.2), 66.8(60.3)을 달성한다. 테이블 B.2에는 EPAC81ne tuning SimCLR 모델(Explace B.5를 보면 EPac81n-tung 시술의 세부 내용을 살펴보자)이 확보한 이미지넷 정확도가 나 흥미롭게도 이미지넷 트레이닝 세트(100%)에서 EPAC81ne tuning 할 때 우리의 RNet(4c397) 모델은 상위 1/95.4% 상위-513을 달성하는데, 이는 같은 세트의 스트레이션(i.e.)을 활용해 훈련을 하는 것보다 신호 811위(상위 78.4%-1/94 무작위 작물과 수평적인 스파크82ipp)이 있다. RENET-50(2c397)의 경우 미리 훈련한 Resnet-50 (2c297)을 스크랩(77.8% 상위 1/93.9% 정상 5)으로 훈련하는 것도 좋다. UNET-50을 위한 스파크81ntuning에서는 개선이 없다. 건축가 Renet-50 ResNet-50 (2c397) ReNet -50(4c398) 1% Label 10% 100%, Top 1 TOp 5 TOP 5 POP 1 POT 549.4 93.194.4 88.1 91.2 92.8 76.1 86.74.8 TB 2. 이미지넷이 1%, 10%, 가득 차 있는 SIMCLR(더 폭넓은 데이터 조작으로 미리 훈련된 것)을 스파크81ne 튜닝으로 획득한 클래스피어 81도 정확도를 기준으로 한다. 참고로 우리 UNET-50(4c397)은 100% 라벨에서 스크래치에서 훈련을 받아 상위 1,94.2% 정상 5위를 달성했다. B.3. 이곳에서는 훈련 단계와 강력한 데이터 교육이 감독 훈련에 어떤 영향을 미치는지 살펴보기 위해 '감독 모델을 위한 장기 훈련'의 효과가 나타난다. 같은 세트의 데이터 조작(랜덤 작물, 컬러 왜곡, 50% 가우시아 모래)에서 사용되지 않은 초대형 모델에서도 리넷-50과 RSNet-50(4c397)을 테스트한다. 그림 B.3은 상위 1 정확도를 보여준다 우리는 이미지넷에서 더 이상 감독된 모델을 훈련하면서 신호탄 81cant Therefac81t이 존재하지 않는다고 관측한다. 더 강력한 데이터 조작이 유엔넷-50(4c397)의 정확도를 조금 향상시키지만 유네트-50에는 도움이 되지 않는다. 더 강력한 데이터 추출이 적용되면 유네트-50은 일반적으로 더 긴 훈련(e.g.)이 필요하다. 최적의 결과를 얻기 위해서는 1450명이 14명이고, RNet-50명(4c397명)은 장기 훈련에서 81t을 기록하지 않는다. 모델 트레이닝 리넷-50 ReNet-50(4c397), 9000 Top 1 + Color 75.6 76.5 77.77 78.77.9 + 콜러 + Blur 72.777.57.7 79.3 테이블 B.3. 다양한 데이터 조작 절차 하에서 더 오래 훈련된 감독 모델의 톱 1 정확도(대조적 학습을 위한 동일한 세트의 자료 조작)가 가능하다. B.4 비라이나 프로젝트 헤드 피겨 B.3을 파악하면 라인 전망 매트릭스 W e28888 R2048c397248이 Z = W h를 계산하는 데 사용되는 이긴 가치 유통이 상대적으로 거의 없음을 보여주는 것으로 나타났다. 그림 B.4에는 우리 최고의 RNET-50(top-1 라인어 평가 69.3%)이 무작위로 선정한 10개 클래스를 대상으로 h와 Z = g(h)의 t-SNE(2008년 마텐앤해튼) 시각화를 보여준다. h로 대표되는 클래스는 Z에 비해 더 잘 분리된다. 상위 1위 / 95.2% 상위 5위 80.1%에 해당하는 것은 심클러를 자랑하는 데 대한 보다 넓은 반감이 없다. 오토아쿠먼트(2019년 큐브크 등 14위)를 기록하면 최적 시험 정확도는 900∼500도 사이에 달성할 수 있다. 0cA 간단한 시각대표 컨트라스틱 러닝 프레임워크(a) Y-axis가 일률적으로 유지된다. (b) Y-axis가 로그 스케일로 진행된다. 그림 B.3. 라인어 프로젝트 매트릭스 We2888 R2048c3972048은 G(h) = Wh.(a) h(b) Z = g(h)의 그림 B.4을 계산하기 위해 사용되었다. 인증 세트에서 무작위로 선정된 10개 클래스에서 이미지의 숨겨진 Vector의 t-SNE 시각화가 진행된다. B.5 네스터프 모멘텀 최적화기를 이용한 미세훈련 미세 훈련 프로세스 위스파크81ntune을 통한 반 초등학습, 배트 크기 4096, 기세 0.9, 학습률 0.8(학습 후 0.05c397 배치사이즈/256) 등을 따뜻하지 않게 한다. 무작위 크로핑(무작위로 왼쪽 오른쪽 스파크82pp, 리사이즈 224x224)만 사용해 준비 작업을 하고 있다. 우리는 정규화(체중 부패 포함)를 사용하지 않는다 1%가 표시된 데이터에 대해서는 60개의 스파크81ntune을 대상으로 표시하고, 10% 표시한 자료에서는 30개 스파이크81ne트une에 표시되어 있다. 1880년을 위해서는 주어진 이미지를 256x256으로 재정비하고, 224x224의 한 센터 작물을 가지고 있다. 테이블 B.4는 준초과 학습을 위한 다양한 방법에 대한 상위 1 정확도의 비교를 보여준다. 우리 모델들은 최첨단 모델을 개선하는 것이 특징이다 방법 건축 레이블 1% 10%, 톱1 25.4 ResNet-50 Ronet-50 SNet-50감독 기본 방법 : UDA(w.RandAug) FixMatch(w.randag) S4L(Rot+VAT+Ent. 민) 자체 초과 대표 학습만을 이용한 ResNet-50(4c397) 방식: CPC v2 SimCLR(시간) SiMCLR (시간대) SIMCLLR 52.78.3 58.53.0 - 56.4 68.8 71.5 73.1 63.7 74.4 TB 4. 이미지넷은 라벨이 거의 없이 훈련된 모델의 상위 1 정확도가 높다. 상위 5 정확성을 위해 7번 테이블을 살펴보세요 B.6 라인어 평가를 위해서는 1.6(학습률 0.1c397 배치,256)의 학습률이 높고 90개의 교육이 길어진다는 것을 제외하고는 스파크81ntuning( Appendix B.5)과 유사한 절차를 밟는다. 대안적으로 프리트레이닝 초파라미터와 함께 PRS 최적화기를 사용하는 것도 비슷한 성과를 내는 것이다. 나아가 베이스 앙코더 위에 라인어 클래스피어 81대(입력 중 정지_그라디언트가 있는 라인러 클래식 81기를 탑재해 라벨 정보가 암호화기 내 수비 82u를 막는다)를 동시에 훈련하고 비슷한 성과를 거둔다는 것이 웹81일이다. B.7. 여기서 라인어 평가와 미세터닝 사이의 교정은 훈련 단계와 네트워크 건축의 다른 설정 아래 라인 평가, 스파크81ne-tuning 간의 상관관계를 연구한다. 그림 B.5는 UNET-50(주사량 4096) 훈련 기준이 50에서 3200으로 다양할 때 라인 평가 대비 EPAC81ne tuning을 보여준다. 이들은 거의 라인 상관관계가 있는 반면 라벨 100e2892110e 2889292910e27892710e-2888992310e102101S Quarden Eigen Value 05001,0002,1501116S quared egenvaluue0cA 간편 프레임워크는 연수 기간보다 길어 보이는 것으로 보인다. 그림 B.5. 라인어 평가와 EPAC81ne tuning을 거쳐 다른 라인어로 훈련된 모델들의 최상위 1 정확도가 나타나고 있다. 그림 B.6은 선택의 다른 건축가를 위한 라인별 평가 대비 스파크81ntuning을 보여준다. 그림 B.6. 라인별 평가와 스파크81ntuning 아래 서로 다른 건축가들의 톱1 정확도가 높다. B.8. 이동학습 우리는 이미지넷에서 배운 자체 초과 대표성을 기반으로 한 새로운 데이터셋을 분류하기 위해 물류 감소 교실 81명이 훈련 중 모든 무게가 다르게 변화할 수 있는 라인 평가, EPAC81ne tuning 등 두 가지 설정으로 이전학습을 위한 자체감독 성과를 두 경우 모두 코르블리트  et 알이 묘사한 접근법을 따른다. (2019년) 우리의 준비 처리가 조금은 다르지만 말이다. B.8.1. METHODS 데이터셋(2009년 크리제프스키 앤 해튼), 버그 앤 알(2014년), SUN397 스테이지(Xiao et, 2010), 스탠퍼드 카르스(Krause Et All,2013년 알), PASCAL BORC 2007 클래스 ATH 81 TAST(에버햄 et an 2010), DTD(Cimpoie Ent Art, 2014), ASCIT Pets(파크하이 Ether Archart), FGVC Arich Alict(Magie Ant, 13013), CIFARC 등을 대상으로 이전 학습 공연을 조사했다. 이러한 데이터를 도입한 논문에서 평가 프로토콜을 따르면 FGVC 에어크래프트, 옥스퍼드-IIT Pets, 칼텍-101, CIFAR-100, 씨파르-100, 버드나프, SUN397, 스탠퍼드 자동차, DTD 등에 대한 상위 1위 정확도를 보고하고 있다. 에버햄 등에서는 11점 MAP 메트릭을 102점으로 나타냈다. 2007년 PASCAL VORC(2010)를 대상으로 했다. DTD와 SUN397의 경우 데이터셋 크리에이터들이 디팩81인 복수열차/테스트 스플릿을 제거하고 있다. 우리는 스파크81rst 분할에만 결과를 보고한다. 칼테크-101 데파크81은 기차/테스트 분할이 없어 기존 작품(2014년 도나후 등)과 공정한 비교를 위해 수업당 30개의 이미지를 무작위로 선택하고 나머지 시험을 선택했다. 데이터셋 크리에이터가 제작한 인증 세트 스펙81을 사용해 FGVC 항공기 초퍼파라미터를 1%6264666870Liner 평가 35.037.540.042.545.047.550.0Fine-tuning 10% 505606570linar 평점 30336394245454454Fnetung (1%) Width1x2x4xDept18345015255560570LInar 다른 데이터에 대해서는 초파라미터 튜닝을 하면서 인증을 위한 훈련 세트의 지하 세트를 실시했다. 인증 세트에서 최적의 초파라미터를 선택한 뒤 모든 훈련과 인증 이미지를 활용해 선정된 파라메터를 이용한 모델을 재교육했다. 우리는 시험 세트에 대한 정확성을 보고한다 '라인나 클래스피어 81'을 통한 이동학습을 통해 '동결된 프리랜서 네트워크'에서 추출된 기능에 대해 '(시드 196) 2 규모의 다자 물류 퇴출 교실'을 훈련했다. 우리는 L-BFGS를 이용해 소프트맥스 크로스엔트로피 객관성을 최적화했고, 데이터 조작을 적용하지 않았다. 미리 처리하면서 모든 영상이 224c39724센트 작물을 촬영한 뒤 바이큐브 리펌플링을 이용해 짧은 측면을 따라 2204픽셀로 재편됐다. 우리는 10e288926∼105개 사이에 45개의 로가리즘이 분산된 범위에서 2개의 정규화 패러머를 선택했다. 파인 튜닝 웨이 스파크81ne을 통해 이동학습을 시작으로 미숙한 네트워크의 무게를 초기화로 활용해 전체 네크워크를 조율했다. 모멘텀 파라미터 0.9으로 네스테로프 모멘텀을 갖춘 SGD를 이용해 256개의 배트 크기에서 2만 단계를 훈련했다. 정상화 통계를 위한 모멘텀 파라미터를 1인당 단계 수준인 최대(1e28892 10/s, 0.9)로 설정했다. 스파크81ntuning 기간 동안 데이터 조작이 이루어지면서 리사이즈와 스파카82pp으로 무작위 작물만 공연했는데, 자존감과는 대조적으로 색채조작이나 모호한 작물을 수행하지 않았다. 시험 시간에는 짧은 측면을 따라 256픽셀로 이미지를 재편하고 224c397 223센티미터 작물을 섭취했다. (특히 CIFAR-10과 시파-100 데이터에 대한 추가적인 정확도 개선이 가능할 수 있다.) 우리는 학습률과 체중 부패를 선택했으며, 무게 부패는 물론 10∼288926∼10e28923 사이의 무게감 부패 가치가 0.0001∼0.1∼7가지 로고리즘적으로 분산된 학습율은 7가지였다. 우리는 이러한 체중 부패의 가치를 학습률에 따라 나눈다 우리는 랜덤 초기화 훈련을 통해 네트워크를 무작위 초기 단계화로부터 스파크81netuning과 같은 절차를 이용해 훈련했지만 오래 걸렸고, 초고파라미터 그리드가 바뀌었다. 우리는 로고리즘 7개 그리드에서 초과파라미터를 선택했는데 0.001∼1.0∼8개의 로가리즘적으로 체중 부패의 가치가 10e288925∼10e128921.5 사이에 분산됐다. 중요한 것은 우리의 무작위 초기화 기본기준이 4만 단계로 훈련되어 있는데, 이는 코른블리트 등 8단계에서 보여준 것처럼 최대 수치의 정확도를 달성하기 위해 수파 81시간으로 긴 단계이다. (2019) 나프에서는 방법 가운데 통계적으로 신호 81cant 차이가 없고, 푸드-101, 스탠퍼드 자동차, FGVC 항공기 데이터 등에서도 무작위 초기화 훈련을 통해 스파크81ntuning이 작은 장점을 제공한다. 다만 나머지 8개 데이터에서는 프리트레이닝이 뚜렷한 장점이 있다. 우리는 이미지넷에서 훈련된 동일한 유네트 모델과 표준적인 크로스엔트로피 손실로 비교한다. 이들 모델들은 우리 자체 슈퍼 모델(크로프트, 강력한 컬러 변형, 모호함)과 같은 데이터 변형으로 훈련을 받고 있으며, 1000여 개의 스피커를 위해 훈련도 받는다. 더 강력한 데이터 조작과 더 긴 훈련 시간이 이미지넷에서 정확도를 파악하지는 못하지만 이들 모델들은 90개의 시그니처와 일반적인 자료 조작을 위해 라인별 평가를 위한 감독 기본 훈련보다 신호 81개를 더 잘했다는 사실을 감독한 RNet-50 기본선은 자체 감독 카운터파트 대비 69.3%인 이미지넷에서 상위 1위 정확도를 달성하는 반면 RNET-50(4c397) 기준은 78.3%로 자체감독 모델은 76.5%에 달한다. 통계 시그니처ac81cance Testing We the Signerace 81cencess of Moder 간 차이가 있는 신호등급을 위해 테스트한다. 두 가지 모델에 대한 예측을 감안하면 이번 무작위화를 실시한 뒤 각 모델별 예측과 정확도 차이를 계산해 무작정 분배에서 10만 개의 표본을 생산한다. 이어 예측 차이보다 극단적인 무분배에서 표본 비율을 계산한다. 상위 1 정확성을 위해서는 이 시술이 정확한 맥네마 시험과 같은 결과를 얻는다. 무효 가설에 따른 교환 가능성을 가정하는 것도 1등급 정확도를 의미하는데 유효하지만 평균 정밀 곡선을 계산할 때가 아니다. 따라서 우리는 MAP의 차이가 아닌 VOC 2007에서 정확도 차이를 보여주기 위해 신호 81cance 테스트를 한다. 이 시술의 동굴은 평가를 위해 이미지의 스파크81나이트 샘플을 사용해 발생하는 변동성만으로 모델을 훈련할 때 런투런 변수를 고려하지 않는다는 것이다. B.8.2. STANDARD REWRTS THE RESNETS 50(4c397) 결과 텍스트 8에서 감독 또는 자체 초과 모델에 대한 명확한 장점이 없는 것으로 나타났다. 그러나 이번 RNET-50 건축을 통해 감독된 학습은 자기 초등학습에 대한 명확한 장점을 유지하고 있다. 감독한 RNET-50 모델은 라인 평가가 있는 모든 데이터 시스템에서 자체 감독 모델을 뛰어나게 하고, EPAC81ne tuning으로 가장 많은 (12개 중 10개) 자료를 만들어낸다. 비주얼 대표 식품 CIFA10 CIFAR100 조류나프 SUN397 자동차 항공기 VORC2007 DTD Petech-101 꽃 라인 평가: SIMCLR(ours) 68.472.3 슈퍼 파인트 90.63.6 97.9 71.68.3 85.9 86.4 80.2 37.7 75.8 76.1 63.5 53.6 자연 이미지 데이터 12개 전반에 걸쳐 감독된 기본 기준으로 우리 자체 감독 접근법의 이전 학습 성능을 비교해 비디오넷 해석 Resnet 모델을 활용했다. 유네트(4c397) 건축물로 결과를 얻을 수 있는 수치 8도 살펴보자. 모델은 이미지넷에서 감독된 모델과 자체감독 모델의 정확성 격차가 연관될 수 있다. 자체 초과 RNet은 절대적인 측면에서 감독 모델보다 6.8% 나쁜 상위 1위 69.3%, 자체 감독된 RNET(4c397) 모델은 76.5%로 감리 모델에 비해 1.8%나 나쁘다. B.9 이미지넷을 주요 데이터셋으로 활용하는 데 초점을 맞추지 못하는 모델을 선점하는 동안 우리의 방식도 다른 데이타셋과 함께 작동한다. 우리는 CIFAR-10을 다음과 같이 테스트해 시연한다 우리의 목표가 CIFAR-10 성능을 최적화하는 것이 아니라 이미지넷에 대한 관측의 추가적인 개념 81:1을 제공하기 위해 같은 건축물(ResNet-50)을 사용하고 있다. CIFAR-10 영상이 이미지넷 영상보다 훨씬 작기 때문에 3x3 Conv 1의 EPAC81rt 7x7 CONv를 대체하고, EPac81rst MOX Poolling 운영도 제거한다. 데이터 조작을 위해서는 이미지넷, 15, 컬러 왜곡(강도=0.5)과 같은 인셉션 작물(디팩82ip, 리사이즈 32x32)을 사용해 가우시아 모래를 벗어나게 된다. 우리는 학습률이 50.5,1.0,1.5,2048,4096,512,1024,408,0096:00에서 기온이 0.1,0.5.1,10.0,256의 배트 크기를 기준으로 하고 있다. 나머지 설정(최적화, 체중 부패 등 포함) 우리 이미지 네트 훈련과 똑같다 같은 건축물과 배트 크기를 이용한 감독 기본선에서 95.1%로 나타나는 것에 비해 1024배트로 훈련된 우리 최고의 모델은 94.0%의 라인 평가 정확도를 달성할 수 있다 라인어 평가 결과를 CIFAR-10에 보고하는 최고의 자체 감시 모델은 AMDIM(바흐만 et 2019)으로 우리보다 25c397이 큰 모델로 91.2%를 달성했다. 보다 적합한 기반 네트워크를 활용하는 것은 물론 추가 데이터 변형을 접목해 우리 모델이 개선될 수 있다는 점에 주목한다. 각기 다른 배트 크기와 훈련 단계인 피규어 B.7에 따른 퍼포먼스는 다양한 배치 크기, 교육 단계 아래 라인별 평가 성과를 보여준다. 이미지넷에 대한 우리의 관측과 일치하는 결과는 4096개의 배트 규모가 가장 많지만 CIFAR-10에서 실적 저하가 작은 것으로 보인다. 그림 B.7. 라인어 평가는 CIFAR-10 데이터셋에서 다양한 배트 크기와 편집력을 갖춘 훈련을 받았다. 각 술집은 학습률(0.5,1.0,1.5)과 온도 cf84 = 0.5로 평균 3개 이상이 운영된다. 오류바는 표준 일탈을 표시한다 이미지넷 영상보다 CIFAR-10 영상이 훨씬 작고 이미지 크기가 사례 가운데 다르지 않지만 리사이즈와 함께 크로핑하는 것은 여전히 대조적인 학습을 위한 매우 효과적인 성찰이라는 점을 1002003004006007008009001000 Trance 8082848688909294Top 1Batch 규모 2565121024204840960cA 시각대표 컨트라스틱 프레임워크 B.8 이하의 최적 온도는 다양한 부분 크기 아래 3가지 온도로 훈련된 모델의 라인 평가를 보여준다. 우리는 융합을 위한 훈련을 할 때(e.g.) EPAC81을 한다. 300개 이상의 훈련비율을 기준으로 하면 0.1, 0.5, 1.0 이하의 최적 온도인 '트레이닝비율'이  0.5로 대량 크기와 상관없이 일관된 것으로 보인다. 다만 크기가 커지면서 cf84 = 0.1로 성능이 개선돼 최적 온도 0.0을 향한 작은 변화를 제시할 수 있다. 289a4 300(b) 트레이닝 코트 >300 피규어 B.8. 모델(ResNet-50)에 대한 라인어 평가는 CIFAR-10에서 세 가지 온도가 다른 크기로 훈련된 것이다. 각 술집은 다양한 학습률과 전체 열차 운행량이 다른 복수 운행 평균을 기록한다. 오류바는 표준 일탈을 표시한다 B.10 다른 손실 재능을 위한 터닝은 NT-Xent 손실에 최선을 다하는 학습률이 다른 실손 기능에 대한 좋은 학습율이 아닐 수 있다. 공정한 비교를 보장하기 위해서는 마진 손실과 물류 손실 모두에 대한 초파라미터도 조율한다. 스피스텍 81cally에서는 두 손실 기능을 모두 대상으로 ' {0.01', 0.1, ' 0.3', '0.5', 1.0) 등에서 학습률을 조절한다. 마진 손실에 대해서는 0.4, 0.8, 1.6.1의 마진을 추가로 조정하고, 물류 손실의 경우 기온은  {0.1,0.2,0.5,1.0.1.로 나타났다. 단순히 실적을 약간 훼손하면서도 공정한 비교를 보장하는 한 방향(양쪽 대신)에서 부정적인 부분만 고려한다. 주요 텍스트에서 언급한 바와 같이 관련 방식에 대한 C. 추가 비교는 심클R의 개별 부품 대부분이 이전 작업에 등장했고, 실적 개선은 이러한 디자인 선택이 결합된 결과다. 테이블 C.1은 우리 방식의 디자인 선택을 기존 방식과 높은 수준의 비교를 제공한다. 이전 작업과 비교하면 우리의 디자인 선택이 대체로 간단하다. 데이터 8분기 세관 모델 CPC v2 AMDIM 패스트 오토액이다. CMC 패스트 오토업 Crop + Color MoCo POR CROP + 컬러 SimCLR Crob + Blur Base Encoder Resnet-161 (모드리퍼ac81ed) Comp Renet RNet-50 (2c397, L+ab) RNet-50(4c398) Ross Xent Project PixelCNNN Non-liner 각 방식별로 디자인 선택과 트레이닝 설정(이미지넷에서 가장 좋은 결과를 얻기 위한)을 높은 수준으로 비교한다. 여기서 제공된 설명이 일반적이라는 점에서 두 가지 방법에 맞춰도 형식과 시행이 다를 수 있다(e.g. 색채 조화를 위한) 원본 논문을 좀 더 자세히 보세요. #표본은 여러 패치로 나뉘어 효과적인 배트 크기를 나타낸다. 이28897A 메모리은행이 취업하고 있다. 아래에서는 최근 제시된 대조적 대표 학습 방식에 대한 심도 있는 비교를 제공하고 있다: e280a2 DIM/AMDIM(Hjelm et, 2018; Bachman et.,2019)이 콘비넷의 중층을 예측해 글로벌,현지,현지 동네 예측을 달성한다. 콘비넷은 네트워크의 수용성 스파크81elds(e.g.)에 대한 시그니퍼 81cant 제약을 배치하기 위해 디펜 모디퍼팩81ed를 구축한 리스넷이다. 많은 3x3 CONV를 1x1 Convs로 교체한다. 우리의 틀에서는 예측 과제와 암호화를 무작위로 설정하고, 256512102420484096Batch 규모 75.077.580.082.585.087.590.092.595.0Top 1TOP 0.10.51.025120424240496BAtth 909192939495TOp 1top 0.051.5100cA 간단한 프레임워크를 시각 표준 및 보다 강력한 리넷을 사용할 수 있다. 우리의 NT-Xent 손실 기능은 비슷한 점수 범위를 제한하기 위해 정상화와 온도를 이용하는 반면 정규화로 탄h 기능을 사용한다. 우리는 단순한 데이터 조작 정책을 사용하고 있으며, 패스트 어투어그먼트는 최선의 결과를 위해 사용한다. e280a2 CPC v1 및 v2(Oord et al, 2018; Hc3a9naf et, 19) defec81은 결정적인 전략을 이용해 패치를 패치로 나누는 맥락 예측 과제와 이러한 패치들을 집계하기 위해 패키지 네트워크(PixelCNN)를 분리 베이스 암호화 네트워크는 원래 이미지보다 상당히 작은 패치만 보고 있다. 예측 과제와 암호화 건축을 해결하기 때문에 맥락 네트워크를 필요로 하지 않고, 우리 암호가 더 넓은 결의의 스펙트럼의 이미지를 살펴볼 수 있다. 여기에 정상화와 온도를 지렛대로 하는 NT-Xent 손실 기능을 활용하는 반면 이들은 정상적인 크로스엔트로피 기반 객관적으로 활용한다. 우리는 간단한 데이터 조작을 사용한다 e280a2 InstDic, MoCo, PART(2018, 2019, 미사라 & Van Der Maten, 19)는 도소비츠키  et 알이 원래 제안한 Experar 접근법을 일반화한다. (2014년) 그리고 노골적인 메모리 은행의 레버리지가 있다. 메모리 뱅크를 사용하지 않는다. 더 큰 배트 크기로 마이너스 샘플 수피 81개가 사용된다. 비라인 프로젝션 헤드도 활용하고, 전시 헤드 앞에서 대표성을 활용한다. 비슷한 유형의 조각품(e.g., 무작위 작물, 색채 왜곡)을 사용하지만, 사양 81c 파라미터가 다를 수 있을 것으로 기대한다. e280a2 CMC(Tian et al, 2019)는 각 시야별로 분리된 네트워크를 사용하는 반면, 우리는 단순히 모든 무작위 조회에 공유된 단일 네트웍을 사용한다. 데이터 조작, 프로젝트 헤드, 손실 기능도 다르다. 우리는 메모리 은행 대신 더 큰 부분을 사용한다 E280a2, Yee et al. (2019년) 같은 이미지의 비유와 비유 없는 복사본 사이에서 극대화하면서 우리 프레임의 두 지점(Figuer 2) 모두에게 데이터 조작을 적용한다. 기지 기능망 생산에 비라인 프로젝트를 적용하고, 프러젝트 전에 대표성을 사용하는 것도 예 et알이다. (2019년) 라인별로 예측된 스파크81 110개의 숨겨진 베이커를 대표로 활용한다. 다중 액셀러레이터를 활용한 대량 크기의 트레이닝을 할 때 글로벌 BN을 활용해 대표성을 크게 떨어뜨릴 수 있는 짧은 코트를 피한다. 0c'\n"
     ]
    }
   ],
   "source": [
    "print(text_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "educational-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summ = Summarizer(text_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "metallic-secretary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"b'A 간단한 시각대표 컨트라스트 학습 기본 Ting Chen 1 Simon Kornblith 1 Mohammad Noruzi 1 George Hunton 1,02 2 luj 1 1) GL'이다. sc [3 대 9 0 7 5 0] 2002: viXr a bustruct is SimCLR: 시각적 표현에 대한 대조적 학습을 위한 간단한 틀을 제시한다. 최근 제안된 대조적 자기감독 알고리즘을 전문 건축가나 메모리 뱅크가 필요 없이 간소화한다. 대조적인 예측 과제가 유용한 논평을 배울 수 있는 것을 이해하기 위해 우리는 체계적으로 우리 틀의 주요 부품을 연구한다. 데이터 조작의 구성(1)이 효과적인 예측 과제인 데파크 81회에서 중요한 역할을 하고, 보고서 송환과 대조적 손실 사이에서 학습 가능한 비라인어 변신을 도입하는 것을 보여주고, (3) 대조 학습에 비해 더 큰 부분 크기와 더 많은 훈련 단계에서 비교적 학습 이러한 스파크81회를 결합해 이미지넷에서 자기초과,준초과 학습을 위한 기존 방식을 상당히 뛰어넘을 수 있다. 심클R이 배운 자체 감시 표현에 대해 훈련한 라인어 클래스피어 81명은 이전 최첨단보다 7% 상대적으로 개선된 상위 1위 정확도 76.5%를 달성해 감독한 RNet-50의 성과와 일치한다. 라벨의 1%에 불과한 스파크81n을 탑재하면 85.8%의 상위 5 정확도를 달성해 100c397개의 라벨이 적은 알렉스넷을 형성하는 퍼포먼스가 뛰어나다. 1. 인간의 감독 없이 효과적인 시각적 표현을 배우는 것은 오랜 문제다. 대부분의 주류 접근법이 세대적이거나 불범죄적인 두 가지 수업 중 하나에 빠진다. 세대적인 접근법은 입력 공간(2006년, 킹마 앤 웰링, 2013년, 굿 라운지 등 2014년)에서 모델 픽셀을 생성하거나 발생시키는 방법을 배운다. 1구글 리서치, 브레인 팀이다. 기준: 팅첸 <아이밍첸@google.com>. 2020년 PMLR 119 오스트리아 빈에서 열린 제37회 기계학습 국제회의 진행이 이뤄졌다. 저자(작가)가 2020년 저작권을 발행했다. https://gittub.com/google-research/simclr에서 1코드가 가능하다. 그림 1. 라인어 클래스피어 81명의 이미지Net Top-1 정확도는 서로 다른 자체 초음속 메스트악(이미지넷에서 해석)으로 학습한 것이다. 그레이 크로스는 UNET-50을 감독했다 우리의 방식인 심클루가 과감하게 보여준다. 다만 픽셀 수준의 세대는 계산적으로 수출이 확대되고 있어 대표 학습에 필요한 것은 아닐 수 있다. 차별적인 접근법은 감독 학습에 사용되는 기능과 유사한 비집 티브 기능을 활용해 배우지만 열차 네트워크는 무표준 데이터넷에서 인테리어와 라벨이 모두 파생되는 핑계적인 과제를 수행하고 있다. 그런 다수의 접근법은 그동안 패류에 의존해 학습된 표현의 불가능성을 제한할 수 있는 명목(2015년 도르치  et 알, 2016년 장 등, 노루지 & 파바로, 지도리 등)을 설계했다. 최근 잠재적 공간에서의 대조적 학습을 바탕으로 한 차별적 접근법이 큰 약속을 보이면서 최첨단 예술 결과(2006년 하드셀  et 알, 2014년 도소비츠키  et 아르, 오르트  et 르,2018년, 바흐만 등)를 달성하고 있다. 이 작품에서는 시각적 표현에 대한 콘트라스티브 학습을 위한 간단한 틀을 도입해 심클R이라고 부른다. 기존 작품(Figuer 1)은 SimCLR이 성과를 낼 뿐만 아니라 특화된 건축가(Bachman the Al., 2019;Hc3a9naffer, Ar., 19)도 필요 없고 메모리 뱅크(2018; Tian et., 그리고 2019; He an., Misra&Van Der Maa 우리는 어떤 좋은 대조적 리퍼레이션 학습을 가능하게 하는지 이해하기 위해 조직적으로 우리 프레임의 주요 코믹스를 연구하고 있으며, 2550102040626번 파라미터(Millions) 5560657075ImageNet Top-1 Accuracy(%) InstDicRotation BigBi LACPCv2CPCV2-LCMCAMDIMMoCoCo(2x) 그 외에도 감독 학습보다는 강력한 데이터 인터랙션으로부터 초과 대조적인 학습이 불가능하다. 배울 수 있는 비라인 변신을 선보이는 e280a2는 대표성과 대조적인 손실을 조직적으로 개선하는 방식으로 배운 표현의 질을 향상시킨다. 이280a2 대표 학습은 대조적인 크로스 기업 손실이 발생하면서 정상화된 배출 가능성과 전기 조절 온도 파라미터에서 발생한다. 이280a2 컨트라스티브 학습은 감독 대상자에 비해 더 큰 부분의 크기와 더 긴 훈련에서 비교하여 281t이다. 감독된 학습과 마찬가지로 더 깊고 넓은 네트워크에서 대조적인 학습 비공개 81t이 나온다. 우리는 이들 스파크81회를 결합해 이마-지넷 ILSVRC-2012(러시아코프스키 등 2015년)에서 자체 초,준 초등학습에 새로운 첨단학습을 달성했다. 라인어 평가 프로토콜에 따르면 심클R은 상위 1위 정확도 76.5%를 달성해 이전 첨단(2019년 Hc3a9나프 등)보다 상대적으로 7% 개선된 것이다. 이미지넷 라벨의 1%에 불과한 스파크81ntune을 달성하면 심클R은 상대적으로 10%(Hc3a9naff et, 2019) 개선이 가능한 85.8% 상위 5 정확도를 달성한다. 다른 자연 이미지 클래스피어 클래식 81ca tion 데이터에서 EPAC81n을 조회하면 심클R이 12개 데이타시트 중 10개에 대해 강력한 감독 기본선(2019년 Kornblith the All)보다 파트 이상 2. 방법 2.1. 최근 대조적인 학습 알고리즘(전시 시스템 7)이 주제로 한 컨트라스티브 러닝 프레임 인스페이스는 잠재적 공간에서 대비적인 손실을 통해 같은 데이터 사례의 다양한 견해를 극대화하는 방식으로 SIMCLR이 발휘한다. 그림 2에서 보여준 것처럼 이 틀은 다음 네 가지 주요 부품으로 구성된다. 이280a2 A 스토커틱 데이터 변형 모듈은 주어진 데ー터 사례를 무작정 변형하여 같은 사례의 두 가지 코르 관련 조회를 가져오게 하는데, 우리가 긍정적인 쌍으로 여기는 cb9cxi와 cB9cxj를 조회 이 작품에서 우리는 순차적으로 3가지 간단한 문구를 적용하고 있으며, 무작정 크로핑을 하고 원작 크기로 다시 리사이즈, 주거 컬러 왜곡, 랜덤 가우시아 모호 등을 적용한 3부에서 보여준 것처럼 좋은 성과를 거두기 위해서는 무작위 작물과 색채 왜곡이 결합되는 것이 결정적이다. e280a2 A 신경망 기반 암호화 f(c2b7)가 데이터 사례를 보여주는 레퍼레이션 베이커를 추출한다. 우리의 틀은 어떤 제약도 없이 네트워크 아키텍터의 다양한 선택이 가능하다. 우리는 일반적으로 사용되는 RNet(He et., 2016) Zig(c2b7) Hif(c2B7) cb9cxi 최대 합의 e28690e28892 대표 2892e287692 the 288bcT x T e298bc(cid.48) t zjg(c 2b7) 시각적 표현에 대한 대조적 학습을 위한 단순한 틀이다 동일한 가족(e288bc T, t(cid 48)e289bc t)과 별도의 데이터 조회 사업자 두 명이 표본으로 나눠 관련 조회를 얻기 위해 각각의 자료 사례에 적용됐다. 대조적 손실을 이용한 합의를 극대화하기 위해 기지 암호화 네트워크 F(c2b7)와 프로젝트 헤드 G(c2B7)가 훈련된다. 훈련이 끝난 후 프로젝트 헤드 G(c2b7)를 버리고 다운스트림 과제에 암호화 F(c2B7)와 대표 H를 활용한다. 하이 e2888 Rd가 평균 수영층 이후 생산량인 하이  = F(cb9cxi)  = ResNet (Cb9csi)를 얻기 위해서다. 이280a2 작은 신경망 헤드 G(c2b7)는 대조적 손실이 적용되는 공간에 지도를 표현한다. 숨겨진 층이 하나 있는 MLP를 이용해 cf83가 RELU 비선을 이루는 지 = g(i) = W(2)cp83(W(1)hi)를 얻을 수 있다. 4구간에서 보여준 바와 같이 우리는 28099년대가 아닌 지 28,099대에서 대조적인 손실을 배제하기 위해 스파크81을 제거했다. 이280a2 대조적인 손실 기능 디파크81이 대비적인 사전 과제로 예정되었다. cb9xi와 cb9cxj를 포함한 세트  { cb9xk}를 감안하면 주어진 Cb9ccxi에서 CB9cxk;k(cid54)=i를 파악하는 것이 대조적인 과제다. 우리는 미니바치에서 파생된 시험 플레이 쌍에 대한 대조적인 예측 과제를 N 사례의 마이니바치와 데파크81을 무작위로 표본으로 삼아 2N 데이터 포인트가 나온다. 우리는 명시적으로 부정적인 사례를 표시하지 않는다 대신 긍정적인 쌍(2017년 Chen et al)을 보면, 나머지 2(Ne28892 1)가 미니바치 안에서 부정적 사례로 나타나는 사례를 치료한다. Let Sim(u, v) = u(cid : 62)v/(시드 1007)u(cid : 107)v(cyd 1017) de - 2 정상화된 U와 v(i.e 사이의 점수를 주목한다. 화장품 유사성) 그 다음으로 손실 기능은 (i, j) e28892 로그 수출(sim(zi, zj)/cf84) 1 [k(cid 54) = i] 수출 (심(zim, zk)/cp84), (1) 1(k(cid 554)=i] e 2888  {0, 1}이 1 이상의 기능을 평가하는 기능이다. 스파크81 나올 손실은 모든 긍정적인 쌍(i, J)과 (j, i) 모두 미니 경기에서 계산된다. 이번 손실은 기존 작품(손, 2016, 우에트알, 오르드 등)에서 사용됐으며, 편의를 위해서는 NT-Xent(정상화된 온도 규모의 십자가 손실)라고 말한다. 2N k=1 0cA 간단한 시각대표 알고리즘 1 심클레이28099s 메인 학습  al고리즘을 위한 기본 학습이 가능하다. 입력: 배트 크기 N, 끊임없는 cf84, F, G, T 구조 등이다. 모든 ke2888  { 1에 대해서는 샘플이 나온 미니바치  { xk } N이다. …. …. N: do k=1 down the divertation te288bcT, t(cid 48)e289bcT #스파크81rstive cb9cx2 key288921 = t(xk) h2ke287921 …. …. 2N, Je28888  { 1. …. …. 2N} do, j = z(cid 1007) izj / (cid 1017) zj (시드 1077) (cyd 1900) N k = 1 [(cit 10:06) (2ke 288921, 2k) + (ciid 110) 2N deefac81ne (i, J) 2.2. 간단하게 유지하기 위해 대형 배치 사이즈와 함께 훈련하는 것은 메모리 뱅크(2018년 우에트, 2019년 헤이트, 알, 알)로 모델을 교육하지 않는다. 대신 우리는 256개에서 8192개로 훈련 배트 크기가 달라진다. 8192개의 배트 크기는 양쪽 측면에서 긍정적인 한 쌍당 1632건의 마이너스 사례가 나온다. 라인러닝 스케일링(2017년 충성도, 17일)으로 표준 SGD/M 순간을 활용할 때 대량 배트 크기의 훈련이 불안정할 수 있다. 훈련을 안정시키기 위해서는 모든 부분 크기를 위해 US 최적화기(You et al, 2017)를 사용한다. 우리는 클라우드 TPU와 함께 모델을 훈련하며 배트 크기 2.2 글로벌 BN에 따라 32개에서 128개의 코어를 사용하고 있다. 스탠더드 리사넷은 배트 정상화 티온(2015년 이오프앤세지)을 사용한다. 데이터 평행성으로 배포된 훈련에서는 BN의 의미와 발음이 일반적으로 장치당 현지에서 집계된다. 대조적인 학습에서는 같은 기기에서 긍정적인 쌍을 산출하기 때문에 모델이 지역 정보 유출을 활용해 표현을 개선하지 않고 사전 정확도를 개선할 수 있다. 우리는 이 문제를 훈련 중 모든 기기에 BN의 의미를 집계해 광고 드레스를 하고 있다. 다른 접근법에는 기기(Hee et al, 2019)를 넘나드는 데이터 사례(Shuffec82ing), 또는 레이어 규범(Hc3a9naf the All, 2019)으로 BN을 교체하는 방 2W TPU v3 코어가 128bc1.5시간 소요되는데, 100 코어에 4096대 규모의 UNET-50을 훈련하는 데 시간이 걸린다. B D C(a) 글로벌 및 지역 관점이 있다. (b) 부적절한 견해 그림 3. 솔직한 직사각형은 이미지이며, 직경이 달린 작물이다. 무작위로 꾸미는 이미지를 표본적으로 보면 글로벌 전망(Be28692 A)이나 인접한 뷰(De28792 C) 예측 등이 포함된 대조적인 과제를 샘플로 한다. 2.3. 평가 의전은 여기서 우리가 실험 연구를 위한 의전을 마련하고 있는데, 이는 우리의 틀에서 다른 디자인 선택을 이해하기 위해서다. 데이터셋과 메트릭스 우리의 대부분의 미초보전 연구(라벨이 없는 암호화 네트워크 F)는 이미지넷 ILSVRC-2012 데이터셋(러스-색소프스키 등 2015년)을 활용해 진행된다. CIFAR-10(Krizhevsky & Hinton, 2009)에서 추가로 체험하는 체험용 물질을 추가 검출할 수 있다. 이전학습을 위해 광범위한 데이터에 대한 미리 설명된 결과를 테스트하기도 한다. 배운 것을 평가하기 위해서는 냉동기지 네트워크 위에서 라인어 클래스피어 81명이 훈련되는 라인러 평가 프로토콜(2016년 장 등, 2018년 오르드 등,2019년 바흐만 등)을 따르고 있으며, 대표 품질의 대리로 시험 정확도가 활용된다. 라인별 평가를 넘어 준초,이적 학습에서도 첨단 학습과 비교한다. 결함 설정 그렇지 않으면 데이터 조작 멘토링을 위해 무작위 작물을 사용하고 (무작정 스파크82ip과 함께) 컬러 왜곡, 가우시아 모호함(세부적인 내용을 보면 A)을 재활용하는 것이 아니다. 리넷-50을 기지 암호화 네트워크로, 2단계 MLP 프로젝션 헤드로 활용해 128차원 잠재적 공간으로 대표성을 사업한다. 손실로는 NT-Xent를 활용해 학습률 4.8(0.3c397 배치사이즈/256), 체중 부패 10e288926으로 최적화했다. 우리는 10018.3 Furmer Hutter를 위해 4096 부분으로 훈련을 하고, EPAC81rt 10번지에 대한 라인어 온난화를 이용하며, 리시트 없이 화장품 부패 일정(2016년 Loshchilov & 3번. 컨트라스티브 대표학습 데이터 분석 데스크래프트 81억 개의 예측 과제가 있다. 그동안 감독과 미초보 대표 학습(크리제프스키 등)에서 데이터 분석이 널리 활용됐지만 최대 성과는 100도에 이르지 못했지만 재분석 결과는 달성돼 공정하고 스페프 81개의 과학적 발견이 가능하다. 0cA 간단한 시각대표 컨트라스틱 러닝 프레임워크(a) 오리지널(b) 크롭과 리모델링(c)크롭, 리조즈(and Evac82ip)(d) 컬러 왜곡. (drop) (e) 컬러 왜곡이다. (jitter) (f) 로테이트  {90e297a6, 180e 297A6, 270e287a6}(g) 커트(h) 가우시아 소음(i)가우시안 모호(j) 소벨 스파크81 터치 4. 연구된 데이터 조작 사업자들의 수가 많았다 각 변형은 일부 내부 파라미터(eg)로 데이터를 안정적으로 변형할 수 있다. 회전 정도, 소음 수준) 우리가 이들 사업자들을 발굴에서만 테스트한다는 점, 우리 모델을 훈련하던 발화 정책에는 무작위 작물(EPAC82ip과 리사이즈), 컬러 왜곡, 가우시아 모호 등이 포함돼 있다. (오리지널 이미지 cc-by: Von.grzanka) 2012; Hc3a9naffe et,2019; Bachman et al, 20 19) 콘트라스틱 예측 과제를 제거하는 체계적인 방법으로는 고려되지 않고 있다. 기존의 많은 사람들이 건축물을 바꿔 데파크81n 대비 예측 과제에 접근한다. 예를 들어, 헬름 에트 알이다. (2018); 바흐만 이트 알 (2019년) 네트워크 건축물에서 수용성 스파크81eld를 억제하는 것을 통해 글로벌 현지 시각 예측을 달성하는 한편 오르트 et 알 수 있다. (2018); Hc3a9naff et al. (2019) 인근 시야 예측을 통해 EPAC815 이미지 분리 절차와 콘 텍스트 집계 네트워크를 거쳐 달성한다. 이러한 복잡성을 피할 수 있다는 것을 보여주는 것은 지적 3에서 보여준 것처럼 위의 두 가지를 가리키는 단순한 태도의 가족을 만들어내는 대상 이미지의 무작위 크로핑(리사이즈)을 이러한 간단한 디자인 선택은 신경망 건축 등 다른 부품의 예측 과제를 편리하게 해결한다. 방향성 대비 예측 과제는 가족을 연장하고 이를 안정적으로 구현하는 방식으로 탈피하는 것이 가능하다. 3.1이요. 데이터 분석 작업의 구성이 좋은 논평을 배우는 데 결정적으로 필요한 것으로, 데이터의 분석이 미치는 영향을 체계적 연구하기 위해서는 여러 가지 공통된 논쟁을 한 종류의 변형은 크로핑과 리사이징(수평 스파크82pp), 로테이션(Gidaris et 2018), 컷아웃(2017년 디베리&테일러) 등 데이터의 공간/지형 변형이 포함된다. 다른 유형의 변형에는 컬러 왜곡(컬러 떨어지기, 밝기, 대조, 포화, 화려함)(2013년, 2015년), 가우시아 모래, 소벨 스파크81ling 등의 모습이 포함된다. 이 작품에서 우리가 공부하는 비전을 시각적으로 4화한다. 그림 5. 데이터 분류 또는 구성 아래 라인어 평가(ImageNet Top-1 정확도)가 한 지점에만 적용됐다. 모든 칼럼을 제외하고는 마지막으로 대각 출품이 단일 변신에 발목을 잡았고, 오프디각형은 두 개의 변형(응용 순서대로)의 구성에 해당한다. 마지막 암초 82호가 연속 평균을 연결한다. 개별 데이터 조작의 효과와 조작 구성의 중요성을 이해하기 위해 개별적으로나 쌍으로 조각을 적용할 때 틀의 성능을 조절한다. 이미지넷 영상은 크기가 다르기 때문에 우리는 항상 작물과 재규모 영상(2012년 크리제프스키 등, 2015년 제지 등)을 적용해 크로핑이 없는 상황에서 다른 작품을 연구하는 데 생명공학 81문화가 된다. 이런 불안을 없애기 위해서는 이번 불안정성을 위한 비대칭 데이터 전환 설정을 고려한다. 스피스텍81cally 우리는 항상 무작위로 수확량이 불가능한 나이에 수확하고 같은 해결책으로 거듭나고, 그 다음에는 표적 변신(speced trans)을 그 틀의 한 지점에만 적용하면서 다른 지점을 정체성(i.e.) t(xi) = 1818) Cropcuttor Color SobelNovel BlurRotate Apactorte 2차 TropCutute ColorsoberNoviel Nowelnowsblurrote 1차 변신 33.133.956.346.039.35.035.339.25.22.42.455.835.821.01.416.525.746.240.620.94.09.36.28.838.87.57.69.89.6535.15.26.6526.265.04040.2625.5188.16.46.516.250.725.39.765.52.72.7. (b) 색채 왜곡으로 말이다. 그림 6. 두 개의 다양한 이미지(i.e.)의 작물을 위한 픽셀 강도(모든 채널 전체) 역사가 다르다. 두 줄) 스파크81위 이미지는 그림 4에서 나온다. 모든 축은 같은 범위를 가지고 있다 방법 심클R 감독 1/8 69.6.0 컬러 왜곡 강도 1/4 61.0 76.7 62.6 63.2 7 7 1 64.5 7 4 1 (+ 블루) 오토액 6 1.1 77.1  Table 1. 라인별 평가를 활용한 미초보 RNet-50의 1위 정확도와 RNET-505를 감독했는데, 다양한 색상의 왜곡 tion 강도(추천 A)와 기타 데이터 변형이 가능하다. 강도 1(+Blur)가 우리의 디폴트 데이터 조작 정책이다. 리치 데이터 분석이 성능을 해치고 있다. 그럼에도 불구하고 이번 설정은 개별 데이터 조작이나 그 조작의 영향을 실질적으로 바꾸어서는 안 된다. 그림 5는 개별적이고 변형적인 구성으로 라인별 평가 결과를 보여준다. 대조적인 과제에서 모델이 긍정적인 쌍을 거의 완벽하게 파악할 수 있음에도 불구하고 단 한 개의 변신 Suffac81은 좋은 의미를 배울 수 없다는 관측이다. 반대 멘토를 구현할 때는 대조적인 예측 과제가 더 어려워지지만 대표성의 질은 극적으로 향상된다. 에픽스 B.2는 보다 넓은 세트의 편견에 대한 추가 연구를 제공한다. 무작위 작물 핑과 무작위적인 색채 왜곡 등 한 구성이 돋보인다. 이미지에서 나온 대부분의 패치가 비슷한 컬러 유통을 공유하는 것은 무작위 크로핑만을 데이터 조작으로 사용할 때 한 가지 심각한 문제라고 본다. 이미지를 구분하기 위해 자신의 토그램만으로도 수피 81ce의 색을 보여준다. 신경망은 예측 과제를 해결하기 위해 이 짧은 길을 이용할 수 있다 일반적으로 눈에 띄는 특징을 배우기 위해서는 색채 왜곡으로 꾸미는 것이 비판적이다. 3.2. 컨트라스틱 학습은 감독 학습보다 더 강력한 데이터 교육이 필요하며, 5S 초과 모델이 90개의 교육을 받아 훈련되고, e288bc 0.5% 강화된 교육 성능을 향상시키기 때문에 더 중요한 컬러 교육의 강도를 조절한다. 그림 7. 깊이와 폭이 다양한 모델에 대한 라인어 평가가 이뤄졌다. 파란색 점수의 모델은 우리 모델이 100점으로 훈련되어 있으며, 붉은색 스타 모델들은 1,000점을 대상으로 우리가 훈련하고 있고, 그린 십자가 모델 모델도 90점7점(2016년 하이 테이블 1에 나타났다 강력한 색채 변형이 초대형 비주얼 모델에 대한 라인 평가를 시기적으로 개선한다. 이런 맥락에서 초음속 학습을 이용해 발견된 세련된 정교화 정책인 오토아쿠먼트(2019년 큐브쿠엔트 알)가 간단한 크로핑+(강력한) 컬러 왜곡보다 더 잘 작동하지 않는다. 같은 세트의 조각을 가진 모듈 엘들을 훈련을 감독했을 때, 더 강한 색채 조화가 개선되지 않거나 실적까지 아프다는 것을 관찰한다. 따라서 우리의 실험에서는 감독된 학습보다 강력한(색깔) 데이터 분석에서 비대조적인 학습이 불가능하다는 것을 보여준다. 종교 전 작업에서는 자체 초과학습(2015년 도르치 등, 2019년 바흐만  et 알, Hc3a9naffer, 2919년, 아사노 등)에 데이터 전파가 유용하다고 보고했지만, 감독학습을 위해 정확성을 얻지 못하는 자료 전파는 여전히 대조적인 학습에 상당히 도움이 될 수 있다 4번. 엔코더와 헤드를 위한 건축가 4.1이다. 더 큰 모델인 'Phote7'에서 초과 대조적인 학습 'Persefac81t(이상)'은 어쩌면 심도가 높아지고 성능이 모두 개선되는 것은 놀랍지 않은 것으로 나타났다. 비슷한 스파크81회가 감독학습(그리고 2016년 등)을 실시하고 있는 가운데, 감독 모델과 라인라인 클래스피어 81명 사이의 격차는 모델 크기가 늘어나면서 미초보 모델을 훈련한 것으로 나타나 감독된 상대방보다 더 큰 모델에서 초보 학습 스파이 7교육이 더 이상 감독된 리넷(추가 B.3)을 개선하지 않는다. 050100150200250300350400450번 파라미터(조달) 50556065707580Top 1R101R101(2x) R152R52(2x,2x)R18R18(2x R18 (4x)R34R34(2x-R34) R50R50(2x.4x) 섭니다. R50Sup R50(2x) 업 R50(4x)R50*R50(2x)*R50 (4x)*0cA 간편 학습 기본 NT-Xent NT -Logital Margin Tripple uT v+/cf84 e28892 Log(cid 190) 부정적 손실 기능 V2888{v+,v2892} 수출(uT  v/cF84) log cf83(ut v+ u)/cf84 v+e28892(cid:0) Z(u)(1e2892 ex(uT v+/cp84 ) Ve28792 Exple(uTeve28692/cf 84) (cf83(e28392uT  v+/cF84) / cf85 v+ e282 cf 892 (uT ve28842 / Cf84) 부정적인 손실 기능과 그들의 졸업생들 모든 입력 자동차, I.e. U, v+, VE28892는 (시드 196) 2가 정상화됐다. NT-Xent는 e2809cNomal 온도 규모의 크로스 인트로피 28009d에 대한 약식이다. 각기 다른 손실 기능이 긍정적이고 부정적인 사례에 대한 다양한 가중치를 부여한다. 무엇을 예측할 것인가? 컬러 대 그레이스케일 로테이션 오리지 오리그 대 파손되었다 대 소벨 스파크81 http://www.andom 추측 g(h) 97.4 25.6 56.3 h 99.3 67.6 96.6 80 50 high 8. 친보호 헤드 G(c2b7)와 다양한 차원의 Z = g(h)에 대한 라인어 평가가 다르다. 이곳에서는 2048 차원의 표현 ht(예측)가 있다. 4.2입니다. 비라인어 프로젝션 헤드가 그 전에 층의 표현 질을 향상시킨 뒤 프러젝트 헤드를 포함한 중요성을 연구한다. 그(h) 헤드에 대한 세 가지 다른 건축물을 활용한 라인별 평가 결과를 보여준다. (1) 아이덴티티 맵핑, (2) 라인 프로젝트는 기존 여러 접근법(2018년 우에트 알)에서 사용했던 것처럼, (3) 바흐만 엔트 알과 비슷한 하나의 추가 숨겨진 층(RE (2019) 비라인 프로젝트가 라인 전망(+3%)보다 낫고, 전망이 없는(10% 이상) 것보다 훨씬 좋다는 관측이 나온다. 친절감 머리를 쓰면 출력 차원에 상관없이 비슷한 결과가 관측된다. 나아가 비라인어를 사용하더라도 머리 앞의 층은 그 뒤의 층보다 훨씬 더 좋은(10% 이상) 것으로 나타나는데, 그 이후에는 프로젝트 헤드 이전의 숨겨진 층이 층 이후보다 더 나은 표현임을 보여준다. 비라인 프로젝트 이전에 대표 티온을 사용하는 것이 중요하다는 것은 대조적 손실로 인한 알리타 티온의 손실 때문이라고 단언한다. 특히 Z=g(h)는 데이터 전환에 불투명하도록 훈련을 받는다. 따라서 G는 물체의 색깔이나 지향 등 다운스트림 과제에 유용할 수 있는 정보를 제거할 수도 있다. 비라인 변신 G(c2b7)를 레버그로 인해 더 많은 정보가 형성되고 유지될 수 있다. 이러한 가설을 검증하기 위해서는 H(h) 또는 g(h)를 사용하는 실험을 실시해 자존감 속에서 적용된 변신을 예측 여기서 G(h) = W(2)cf83(W(1)h)를 설정하는데, 입력과 출력 차원(i.e.)이 동일하다. 2048) 테이블3에는 h가 적용된 변신에 대한 정보를 훨씬 더 많이 담고 있고, g(h)는 정보가 없어진다. 추가 분석은 테이블 3이 가능하다. 신청된 변신을 예측하기 위해 서로 다른 송환에 대한 MLP 추가 훈련이 가능하다. 작물과 색채 조화를 제외하고는 지난 3개 줄을 선점하는 과정에서 독립적으로 로테이션(1:  {0e297a6, 90e2297A6, 180e 2907a6), 가우시아 소음, 소 블랙81로테이너 변신 등)을 추가로 추가하고 있다. h와 g(h) 모두 같은 차원의 차원이다. 2048. And B.4에서 발견된다 5번. 손실 재미와 배치 규모는 5.1이다. 대안보다 조절 가능한 온도로 정상화된 크로스트로피 손실이 좋아 물류적 손실(2013년 미콜로프 등 다른 일반적으로 사용되는 대조적인 손실 기능에 비해 NT-Xent 손실을 비교한다). 손실 기능 입력 졸업자는 물론 객관적인 기능을 테이블2에서 보여준다. 졸업생을 보면 1)(시드 6)2 정상화(이하 1)를 관찰한다. 온도가 효율적으로 다른 사례와 함께 화장품 유사성이 있으며, 적절한 온도는 크로스엔트로피와 달리 모델이 하드 부정에서 배우는 데 도움이 될 수 있다. 그리고 2) 기타 비젝티브 기능은 상대적으로 결과적으로 이러한 손실 기능에 대해서는 반하드 마이너스 채굴(2015년 기준, 스크로프 등)을 적용해야 하는데, 전체 손실조건에서 졸업자를 계산하는 기능이 있어야 하며, 준하드 부정적인 용어(i.e., 손실마진과 거리가 가장 가깝지만 긍정적 박람회를 만들기 위해서는 모든 손실 기능을 위해 동일한 (시드 1966) 2 정상화를 사용하고, 초파라미터를 조율해 최고의 성과를 보고하는 한편 (세미하드) 마이너스 채굴이 도움이 되는 반면 최상의 결과는 여전히 NT-Xent 손실보다 8개의 디테일은 B.10  Appendix에서 확인할 수 있다 단순히 우리는 단 한 가지 시각에서 부정적인 부분만 고려한다 326412825651210242048 프로젝트 출력 차원성 3040506070Top 1 Prosult LinearNon-linerNornone0cA 간편학습 마르긴 NT-Logi. 마르진(sh) NT-Logi이다. 57.5 57.9 63.9 51.6 테이블 4. 손실 기능이 다른 모델을 대상으로 훈련한 라인어 평가(톱1)다. 이2809ch28009d는 반하드 마이너스 채굴을 이용한다는 뜻이다. (시드 6) 2 규모? 예, Cf84 0.05 0.1 0.51 100 Etropy Contrastive Ac. 1.0 4.5 8.2 8.3  0.5 90.5 68.8 29.1 91.7 92.1 톱1 59.7 64.4 60.7 58.0 57.2 7.0  Table 5. NT-Xent 손실을 위해 2 규범과 온도 cf84의 다른 선택을 가지고 훈련된 모델에 대한 선별평가가 이뤄졌다. 대조적인 유통은 4096건이 넘는 예다 그림 9. 라인어 평가 모델(ResNet-50)은 차별화된 배트 크기와 부드러움으로 훈련을 받았다. 각 술집은 처음부터 단 한 번씩 달린다.10 우리는 다음으로 (시드 6) 2 정상화(i.e.)의 중요성을 테스트한다. 우리의 디폴트 NT-Xent 손실에서 화장품 대 제품)과 온도 Cf84가 유사하다. 테이블 5는 정상화와 적절한 온도 스케일링 없이 성능이 시그니프 나이프 81을 중심으로 더 나쁘다는 것을 보여준다. (Ciders6) 2 정상화 없이는 대조적 과제 정확도가 높지만 결과적인 대표성은 라인 평가 하에서 더 나빠진다. 5.2. 컨트라스티브 학습 코드파크 81t(이상)는 더 큰 배트 크기와 더 길게 훈련되는데, 그림 9에서는 모델들이 다른 배트의 숫자를 위해 훈련을 받을 때 배트량 크기의 영향을 우리는 훈련 시간이 적은 곳(eg)이라는 것을 알파크81일 말한다. 크기가 커진 100개 크기는 작은 크기에 비해 신호탄 81센티미터의 장점이 있다. 훈련 단계, 에포치가 더 많아지면서 크기가 다르게 줄어들거나 사라지는 격차가 발생하면서 프로 측면의 타구가 무작위로 재탄생하고 있다. 이곳에서는 10A 라인어 학습률 스케일링이 사용되는 것과 대조적이다. 사각형 근본학습률 스케일링을 활용한 수치 B.1은 작은 배트 크기의 성능을 향상시킬 수 있다. ResNet-50을 활용한 건축 방식 : RESNeT-50 지역 Agg. Renet-50 MoCo ResNet-50 PINET-50 CPC v2 SimCLR (시간) Rennet - 50 The Arters : Revente-50(4c397) BigBi AMDIM Cust-ResNet Risnets-50 이미지 네트워크는 다양한 자기 감시 방식으로 배운 레퍼레이션에서 훈련된 라인어 클래스피어 81명의 이미지를 배웠다. 방식 건축 ResNet-50 감독 기본 방식의 다른 레이블 프로그래밍: RESNET-50PICO-labelle ReSNet - 50 VAT+Entropy Min. UDA(w.RandAug) ResNet-50 FixMatch(w. Landag) Lessnet-50 S4L(Rot+VAT+En)이다. M.) ResNet-50(4c397) 방식만 사용하는 대표 학습 방식: IntDisc RONET-50 RESNet-161(e28897)CPC v2 RROSNet - 50 SIMCLR (ORS) RNESNeT-50 ROWNET - 10% TOP5 48.4 81.6 47.0 - 39.25.2 57.2 77.9 75.8 82. 라벨이 거의 없이 훈련된 모델들에 대한 이미지넷 정확도가 높다. 감독된 학습(2017년 기준)은 대조적으로 학습하기 위해 더 큰 부분의 크기가 더 부정적인 사례를 제공해 융합을 촉진(i.e.)한다. 주어진 정확도를 위해 적은 단계와 조치를 취하고 있다) 훈련이 더 길어지는 것도 더 부정적인 사례를 제공해 결과를 개선한다. 추가 B.1에서는 훈련 단계가 더 길어진 결과가 제공된다. 6번. 이번 서브섹션에서 스테이트 아웃과 비교하면 콜스니코프 에트 알과 비슷하다. (2019); 그는 알 것이다 (2019) 3개의 다른 숨겨진 층(1c397, 2c398, 4C397)에서 유네트-50을 사용한다. 더 잘 융합하기 위해서는 이곳의 우리 모델들이 1,000개의 시대를 위해 훈련을 받고 있다 선별 평가 88.97 CIFA100 SUN397 이미지넷 50(4c397) 모델을 대상으로 12개 자연이미지 클래스피어 81개 데이터 전반에 걸쳐 감독한 기본 기준으로 우리 자체감독 접근법의 이전학습 성과를 비교한다. 결과는 가장 좋은(p > 0.05, 복식 테스트)보다 신호 81이 더 나쁘지 않은 결과가 과감하게 나타난다. 실험적인 내용과 표준 UNET-50으로 결과를 보여주는 B.8 보기가 있다. 2019; 선별평가 설정(추천 B.6)에서 천 이트 알(2018)이 나왔다. 테이블 1은 서로 다른 방식 중에서 수치 비교가 더 많이 나타난다. 표준 네트워크를 활용해 기존 방식에 비해 스피스텍81cally 설계된 아키 테크놀러지가 필요한 것과 비교해 훨씬 더 좋은 결과를 얻을 수 있다. 우리의 RNET-50(4c397)으로 얻은 최고의 결과는 감독 미숙한 RNet-50과 일치할 수 있다. 반 초과 학습 우리는 지하이에 알을 따른다 (2019)와 표기된 ILSVRC-12 훈련 데이터 1% 또는 10%의 표본을 수업 균형 방식으로 표기(각 수업별 e288bc128 이미지)한다. 11 우리는 단순히 정규화 없이 표시된 데이터에 기지 네트워크 전체를 스파크 81ne으로 구분하는 것만 보면 된다. 테이블 7에서는 최근 방식(2019년 Zhay et, 2018년 Xie et al, 손에트 2020년, 우에트 Al, Donahue & Simonyan, 2919년, Misra & Van Der Maten, Hc3a9naf 등)에 대한 비교 결과가 초파라미터(조작 포함)를 집중적으로 검색해 감독한 기본선(2019년 Zhie Et All)이 강하다. 다시 한번 우리의 접근방식은 라벨의 1%와 10% 모두 첨단보다 시그니처 81가지로 개선된다. 흥미롭게도 전체 이미지넷에 대한 우리의 미리 설명한 리넷-50(2c397,4C397)을 조절하는 것도 시그니처 81을 중심으로 더 나은 훈련(최대 2%,  Appendix B.2)이다. 이전 학습 라인어 평가 아이온(Fefac81x 기능 추출기)과 스파크81ntuning 설정 모두 12개의 천연 이미지 데이터를 통해 이적학습 공연 매뉴얼을 평가한다. 사람들이 눈에 띄는 코르블리트 등이 있다 (2019) 우리는 모델 데이터넷 조합별 초파라미터 튜닝을 수행하고 인증 세트에서 최고의 초퍼라미터를 선택한다. 유네트-50(4c397) 모델로 결과가 나온다. 스파크81원을 조작할 때 우리 자체 초과 모델 시그니어팩81이 5개 데이터 시스템에서 초기 기준을 초과하는 반면 감독 기본선은 2개(i.e)에 불과하다. 애완동물과 꽃) 나머지 5개 데이터에서는 모델이 통계적으로 동결된다. 기준 UNET-50 건축물과 함께 실험적인 내용은 물론이고 결과가 나온 결과는 B8  Appendix에서 제공된다. 샘플 및 정확한 지하철 세부 내용은 https://www.tensorefac82ow.org/datasett/cataget/imagent 2012_subett에서 확인할 수 있다. 7번. 관련 작업은 작은 변신 속에서 이미지를 표현하는 것이 서로 동의한다는 생각이 베커앤혼턴(1992년)으로 돌아간다. 최근 데이터 조작, 네트워크 건축, 콘트라스티브 손실에 대한 광고 균형을 이용해 연장한다. 비슷한 일관성 아이디어지만 반 감독 학습(Xie et all, 2019; Bert Hellot teal, 19) 등 다른 맥락에서 반 레이블 예측이 탐구됐다. 수작된 구실 과제들 최근 자체 감독 학습 재조사는 상대적 패치 예측(도르치 등, 2015), 지그사 퍼즐(2016년 노루지 & 파바로, 색채 조각(20만6000년), 회전예측(Gidariza tion,2018년 Chen et al, 2919년) 등 아트리피어 81개의 명목으로 시작됐다. 더 큰 네트워크를 통해 좋은 결과를 얻을 수 있고 더 긴 열차 수용(2019년 콜리니코프 등)이 가능하지만, 이러한 핑계는 다소 광고체류에 의존해 학습된 발언의 불가능성을 제한한다. 컨트라스틱 비주얼 표현 학습 다시 하드셀 에트 알로 데이트하고 있다 (2006년) 이러한 접근법들은 부정적인 쌍에 대해 긍정적으로 짝을 맞추는 방식으로 오렌지 테이션을 배운다. 이 라인들을 따라 도소비츠키 에트 알이 있다. (2014년) 각 학원을 특징 베이터(패러메트릭 형태)가 대표하는 수업으로 취급하는 방안을 제시한다. 우에트알 (2018) 메모리뱅크를 이용해 최근 여러 논문에서 채택,연장된 접근 방식인 렌스 클래스 대표 베이터를 저장할 수 있도록 제안하고 있다. 다른 작품들은 메모리뱅크(2017년 도르치앤지서만, 2019년 예 et al, 지아 등) 대신 음성 샘플을 위한 인바치 샘플 사용을 탐구한다. 최근 문학은 자신들의 방식이 잠재적 정보를 극대화하기 위한 성공을 시도해 왔다(2018년 오르드 알, 2019년 Hc3a9naff the Al, Hjelm et al, 2918년, 바흐만 등). 다만 대조적 접근의 성공이 상호 정보에 의해 결정되거나, 대비적 손실(2019년 스크린엔트 알) 형태의 스펙 81c 형태로 결정될 경우는 명확하지 않다. 시각대표 컨트라스틱 학습을 위한 0cA 간편 프레임워크 우리는 스펙 81c 순간이 다를 수 있지만 우리의 틀 작품의 거의 모든 개별 부품이 이전 작품에 등장했음을 주목한다. 기존 작품과 관련된 우리의 프레임 작품의 우위는 단 한 가지 디자인 선택이 아니라 그들의 구성으로 설명된다. 우리는 기존 작품 C. 8에서 디자인 선택을 만화 선점 비교한다. 이 작품에서는 단순한 틀과 대조적인 시각 표현 학습을 위한 입장을 제시한다. 부품을 꼼꼼히 공부하고 다른 디자인 선택의 효과를 보여준다. 우리의 스파크81회를 결합해 기존의 자기감독, 준초과, 이적학습 방식을 놓고 상당히 개선했다. 데이터 조작 선택에만 이미지넷에 대한 표준 감독 학습, 네트워크 끝에 비라인 헤드 사용, 손실 재미 tion 등과 다른 접근법이 있다. 이 단순한 틀의 강점은 최근 관심이 급증하고 있음에도 불구하고 자체 초과 학습은 여전히 저평가되고 있다는 것을 시사한다. 우리는 샤오와 자하이, 라파엘 맥3bcler, 야니 이오누에게 드래프트에 대한 피드백에 감사드립니다. 토론토 등에서 구글 연구팀의 일반적인 지원에도 감사하다. 아사노, Y.M., 로프레흐트, C., 베달디 등을 참고한다. 자기감시에 대한 비판적인 분석, 또는 단 한 가지 이미지로 배울 수 있는 것이 무엇인지 등이다. 2019년 ARXIV Print ArXive 1904.1332가 출시됐다. 바흐만, P., Hjelm, R. D., 부치월터는 상호 정보를 조회 전반에 걸쳐 극대화하는 방식으로 레퍼런스를 배우고 있다. 신경정보처리시스템의 고도화에서는 pp가 있다. 2019년 15509e2809315519년이다. Becker, S., Hinton, G. E. 셀프가 무작위 고정관념으로 표면을 드러내는 신경망을 조직하고 있다. 자연은 1992년 355년(6356년) 1e28093163년이다. 버그, T, 류, J, 이, S. W, 알렉산더, M. L., 제이콥스, D. W., 벨롬우르, P.N. N. 나프: 대규모 스파크81ne급 조류의 시각적 범주화다. 컴퓨터 비전 및 패턴 인식(CVPR)에 관한 IEEE 컨퍼런스에서는 PP가 열렸다. 2019e280932026. 2014년 IEEE. 버트헬로트, D., 칼리니, N., Good Collini, I., Peoper Not, N, 올리버, A., 라벨, C. A. 믹스매치: 준감독 학습에 대한 홀로틱한 접근법이다. 신경정보 검출 시스템의 진보에서는 pp가 있다. 2019년 5050e 280935060년이다. 보사드, L., 구라민, M., 반 구울, L 푸드-101e28093 차별화 성분이 무작위 숲과 함께 있다. 컴퓨터 비전에 관한 유럽 콘퍼런스에서, pp. 446e28093461입니다 스프링거, 2014년. 천,T,선,Y,시,Y,홍 등이 신경망을 기반으로 한 협업 스파크81 입력을 위한 샘플 전략에 나섰다. 제23회 ACM SIGKDD 국제 지식발굴 및 데이터 마이닝 컨퍼런스 진행 기간 PP가 열렸다. 2017년 767e2809376이다. 천, 티, 자이, 엑스, 리터, 미, 루시크, 엠, 휴슬비, N. 셀프가 보조회전 손실을 통해 간을 감독했다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스가 진행된다. 2019년 12154e2809312163년이다. 심포이, M, 마이니, S, 코키노스, I, 모하메드, S., 베달디, A. 야생 속에서 교과서를 다듬었다 컴퓨터 비전 및 패턴 인식(CVPR)에 관한 IEEE 컨퍼런스에서는 PP가 열렸다. 3606e28093 3613입니다 2014년 IEEE. 쿠바쿠, ED, Zoff, B., 마네, D., 바스데반, V., 레, Q. V. 오토오플레이: 데이터로부터의 학습 전략이다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스가 진행되고 있다. 2019년 113e 28093123입니다 디브리, T., 테일러, G. W.는 컷아웃과 함께 소모적인 신경망의 정규화를 개선했다. 2017년 ARXIV Print ArXive 17708.04522가 출시됐다. 도어치, C., 지저만, A. 멀티태스크 자체 초등 시각학습이 진행된다. 컴퓨터 비전에 관한 IEEE 국제 콘퍼런스가 진행되고 있다. 2017년 2051e280932060년이다. 도어치, C, 괌타, A, 에브로스, A 등이 참여했다. 맥락 예측을 통해 A. 미초보 시각 표현 학습이 이뤄진다. 컴퓨터 비전에 관한 IEEE 국제 콘퍼런스가 진행되고 있다. 2015년 1422e280931430년이다. 도나휴, J, 시모니안, K. 스케일의 대형 비타리얼이 tion 학습을 대표한다. 신경정보처리시스템의 고도화에서는 pp가 있다. 2019년 10541e2809310551년이다. 도나휴, J, 지아, Y, 비닐스, O, 호프만, J., 장, N., 땡, E., 다렐, T. 데카프: 세대적인 시각적 인식을 위한 심화적 활성화 기능이 특징이다. 기계학습에 관한 국제 컨퍼런스, pp. 2014년 647e28093655년이다. 도소비츠키, A, 스프링크, J. T., 리드밀러, M., 브록스, T. 차별적 미초보 특징은 소모적인 신경망으로 학습하는 것이 특징이다. 신경정보처리 시스템의 발전에 있어서는 pp이다. 2014년 766e2809374년이다. 에버햄, M, 반 골, L., 윌리엄스, C. K., 캐슬링, J., 지저만, A. 패스컬 비주얼 오브젝트 클래스(변호사) 도전이 이뤄졌다. 2010년 국제 컴퓨터 비전, 88(2):303e2809338. 페이-페이, L., 포르투갈, R., 페로나, P. 러닝 세대적인 비주얼 모델 몇 가지 훈련 사례에서 나온다. 101개 항목 부문에서 점점 더 많은 베이니시아 접근법이 시험되었다. 2004년 비전을 기반으로 한 세대모델에서 컴퓨터 비전 및 패턴 인식(CVPR) 워크숍에서 열린 IEEE 컨퍼런스에서다. 기드라리스, S, 싱, P, 코모다키스, N. Unsuperviden tatement 학습은 이미지 로테이션을 예측해 학습한다. 2018년 ARXIV Print ArXive 1983.0728이다. 좋은 선생님, I., Pouget-Abadie, J., 미르자, M., Xu, B., Warde-Farley, D., Ozair, S., 쿠르빌, A., 그리고 바르시오, Y. 세대적인 네트워크가 있다. 신경정보처리 시스템의 발전에 있어서는 pp이다. 2014년 2672e280932680년이다. 0cA 간단한 비주얼 대표 성실함, P., 달러카3a1r, P, 길릭, R., 노르데시, 피, 위솔로스키, L., 교로라, A., 툴로치, A, 지아, Y., 그리고 K. 정확하고 대형 미니바치 sgd: 1시간 만에 트레이닝 모습이다. 2017년 ARXIV Print ArXive 17706.02677이다. 하드셀, R., 초프라, S., 레쿤, Y. 디펜션 감축 등이 불안정한 맵핑을 배우는 방식으로 이뤄졌다. 2006년 컴퓨터 비전 및 패턴 인식(CVPRe2809906)에 대한 IEEE 전산 소비자 불안 콘퍼런스에서 2, ppp. 1735e280931742입니다. 2006년 IEEE 그, K, 장, X, 렌, S, 선, J 등이 있다. 이미지 인식을 위한 잔류학습이 깊다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스가 진행되고 있다. 2016년 770e2809378년이다. 그, K, 판, H, 우, Y, 시, 에스, 기적, R. 모멘텀 대조는 초대형 비주얼 표현 학습을 위한 것이다. 2019년 ARXIV Print ArXive 1911.05722가 출시됐다. Hc3a9naff, O.J., Rasavi, A., Doersch, C., Eslami, S., Oord, A v. D. 데이터 방식 81 지적 이미지 인식은 대조적인 예측 코딩으로 가능하다. 아엑시브 프린트 아크시브 1905.09272, 2019년이다. 힌튼, G.E., 오산도, S., 텔레, Y.W.는 깊은 신념망을 위한 빠른 학습 알고리즘이다. 신경계산, 2006년 18(7)1527e28093년 1554년이다. Hjelm, R. D., Fedorov, A., Lavoi-Marchildon, S., C., K., Bachman, P., Trischler, A, Buncio, Y. 상호 정보 추정과 극대화로 깊은 반송을 배운다. 2018년 ARXIV Print ArXive 1988.06670이 출시됐다. Howard, A. G. 일부는 깊은 신경망을 기반으로 한 이미지 클래스피어 81 분야에 대한 개선이 있다. 2013년 ARXIV Print ArXive 13312.5402가 출시됐다. 이오프, S, 제지, C매치 정상화: 내부 코바리트 전환을 줄여 딥네트워크 훈련을 가속화한다. 2015년 ARXIV Print ArXive 1502.03167이다. 지, 엑스, 헨리, J. F. 그리고 베달디, A. 인바라이언트 정보는 초대형 이미지 클래스피어 81분위기와 세분위기 등을 대상으로 한다. 컴퓨터 비전에 관한 IEEE 국제 콘퍼런스가 진행되고 있다. 2019년 9865e280939874년이다. 킹마, D. P. 그리고 웰링, M. 오토 앤코딩 변주만이 있다. 2013년 ARXIV Print ArXive 13312.614가 출시됐다. 코르스니코프, A., 자이, X., 비어, L. 리베이스 등이 IEEE 비주얼 대표 학습을 진행하고 있다. 컴퓨터 비전과 패턴 인식에 관한 컨퍼런스, pp. 2019년 1920년 280931929년이다. Kornblith, S., Shlens, J., Le, Q. V. 더 나은 이적에 대한 IEEE 컨퍼런스 진행에 더 좋은 이미지넷 모델을 할 수 있을까? 컴퓨터 비전과 패턴 인식, pp. 2019년 2661e280932671년이다. 크라우스, J, 덩, J., 스타크, M., 페이-페이는 스파크81ne 교육 차량의 대규모 데이터셋을 수집하고 있다. 2013년 미세훈련 비주얼 지정 2차 워크숍에서 열렸다. 크리셰프스키, A, 혼턴, G. 등이 작은 이미지에서 여러 겹의 특징을 배우고 있다. 2009년 토론토대학교 기술보고서가 나왔다. URL https://www.ch.toronto.edu/~kriz/ 학습-features-2009-TR.pdf. 로시칠로프, 나, 허터, F Sgdr: 스토커틱 졸업생이 따뜻한 리조트로 내려간다. 2016년 ARXIV Print ArXive 10:08.03983이다. 마텐, L. v. D., 혼턴, G. 비주얼화 데이터가 t-sne을 활용한 것이다. 2008년 기계학습 연구 NAL, 9(Nov) 2579e280932605. 마이지, S, 칸발라, J, 라투, E, 블라시코, M, 베달디, A. 미세한 훈련을 받은 비주얼 클래스피어 81 분위기의 항공기 운항이 이뤄졌다. 2013년 기술 보고서 미콜로프, T, 첸, K, 코로도, G, 딘, J. 에스파크81 과학적 에스티메이션 단어가 베이터 공간에서 표현된다. 2013년 ARXIV Print ArXive 1301.3781이 출시됐다. 미스라, 나, 반데르 마텐, L. 등이 구실 없는 발언을 했다. 2019년 ARXIV 1912.0191이다. 자기 초등학습 - ARXIV 프린트 닐스백, M.-E., 지저만, A. 자동화된 EPAC82WAR 클래스피어 81 분위기가 많은 수업을 둘러싸고 있다. 컴퓨터 비전에서는 2008년 그래픽 & 이미지 프로세싱이 진행된다. ICVGIPe 2809908입니다 6차 인도 회의, pp. 722e28093729입니다 2008년 IEEE. 노루지, M, 파바로, P. 미스터디한 학습은 지스aw 퍼즐을 풀어 시각적 리프레이션을 배운다. 유럽 컴퓨터 비전 컨퍼런스에서는 PP가 열렸다. 69e2809384 스프링거, 2016년. 오드, A v. D., 리, Y., 비닐스, O. 대조적인 예측 코딩으로 대표 학습을 한다 2018년 ARXIV Print ArXive 1987.03748이다. 박시, 오엠, 베달디, A, 지저만, A., 자와하르, C. 고양이, 개 등이 있다. 컴퓨터 비전 및 패턴 인식(CVPR)에 관한 IEEE 컨퍼런스에서는 PP가 열렸다. 3498e280933505입니다 2012년 IEEE. 러시아프스키, O., 덩, J., 수, H., 크라우스, 제이, 시아쉬, S., 마, 에스, 황, Z., 카르파시, 에이, 코슬라, A., 버른슈타인, M., 등이다. 대규모 시각 인식 도전이 불가피하다 국제 컴퓨터 비전 저널인 2015년 115(3) 211e28093252년이다. 슈로프, F., 칼레니첸코, D., 필빈, J. 파크리에르: 안드리에이드 81ed In Proveding Everidgent 등이 얼굴 인식과 조롱을 위해 배출되었다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스의 결과, pp. 2015년 815e28093823입니다. 시모니안, K., 지저만, A씨 등이 참여했다. 대규모 이미지 인식을 위한 매우 심층적인 네트워크가 있다. 2014년 ARXIV 프린트 아크시브 09.1556이 출시됐다. 손씨, K씨는 다학년 N페어 손실 객관적으로 심화된 메트릭 학습을 개선했다. 신경정보처리 시스템의 발전에 있어서는 pp이다. 2016년 11865년 186e 28093년이다. 손, K, 헬로트, D, 리, C, L, 장, Z, 칼리니, N, 쿠바쿠, E. D., 쿠라킨, A, 창, H., 라펠, C. 픽스매치: 심플리 파이프 준 초과 학습 일관성과 콘셉트 81dence가 있다. 2020년 ARXIV Print ArXiv 2001.07685. 스지디, C, 류, W, 지아, Y, 세르만넷, P, 리드, S, 앙겔로프, D, 에르한, D., 반저우케, V., 라비노비치, A. 소용돌이로 더 깊이 들어갔다 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스가 진행되고 있다. 2015년 1e280939입니다. 텐, Y, 크리쉬난, D, 이슬라, P 컨트라스티브 멀티뷰 코딩이다. 2019년 ARXIV 프린트 아르엑시브 1906.05849이다. 크리셰프스키, A., 솔츠케버, I., 혼턴, G. E. 유전자 클래스피어 81 세이션은 깊은 복합 신경망을 갖추고 있다. 신경정보처리 시스템의 발전에 있어서는 pp이다. 2012년 1097e280931105년이다. 티샤넨, M, 돌, J, 루벤스턴, P. K., 겔리, S., 루시크 등에서는 대표학습을 위한 상호 정보 극대화에 대한 논의가 이뤄졌다. 2019년 ARXIV 프린트 아르엑시브 1907.13625이다. 시각대표 우, Z, Xiong, Y, 유, S.X., 린, D. 불초등학습 기본 0cA 단순 학습 프레임워크는 비초등학교 차별을 통해 학습한다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스의 진행 - pp. 2018년 3733e280933742년이다. 샤오, J, 헤이스, 제이, 잉거, K. A., 올리바, A. 그리고 에살바 A. 태양 데이터베이스 : 대규모 장면 인식이 원시에서 동물원으로 이어진다. 컴퓨터 비전 및 패턴 인식(CVPR)에 관한 IEEE 컨퍼런스에서는 PP가 열렸다. 3485e280933492입니다. 2010년 IEEE. 시, Q, 다이, Z, 호비, E, 룽, M, 레, Q. V. 운수 등 데이터 공개가 이뤄지고 있다. 2019년 ARXIV Print ArXiv 1904.12848이다. 예, M, 장, X, 윤, P. C., 창, S. F. 불초격 배출 학습 등을 투명하고 확산하는 연습 기능이 특징이다. 컴퓨터 비전과 패턴 인식에 관한 IEEE 컨퍼런스가 진행된다. 2019년 6210e280936219년이다. 당신, Y., 기트만, I., 진스부르크, B. 대규모 분해 네트워크 훈련 2017년 ARXIV Print ArXive 17708.03888이다. 자하이, X, 올리버, A, 콜리니코프, A., 비어, L. S4l: 자기감독 준초등학습이다. 2019년 10월 IEEE 국제 컴퓨터 비전 컨퍼런스(ICCV)에서 열렸다. 장, R, 이슬라, P, 에브로스, A 등이 참여했다. 에이 색채 있는 이미지 컬러이자 티온이다 컴퓨터 비전에 관한 유럽 콘퍼런스에서, pp. 649e28093666입니다 스프링거, 2016년. 주앙, C., 자이, A. L., 야민스, D. 지역 집계 등이 시각적 배출물질에 대한 초보 학습을 위한 것으로 나타났다. 컴퓨터 비전에 관한 IEEE 국제 콘퍼런스가 진행되고 있다. 2019년 6002e280936012년이다. 0cA 간단한 시각대표 관제학습 기본 A. 데이터 8월 세부사항은 우리의 디폴트레이닝 설정(우리 최고의 모델을 훈련하는 데 사용된다)에서 무작위 작물(리사이즈와 랜덤 스파크82ipp), 무작정 컬러 왜곡, 데이터의 흐릿함이 자료 조작으로 활용된다. 이 세 가지 구체적인 내용이 아래에 제공된다. 랜덤 작물 224x224까지 재활용해 표준 인셉션 스타일의 무작위 크로핑(2015년 제지 et al)을 사용한다. 원래 크기의 무작위 작물(면적 0.08에서 1.0으로 일률적)과 원래 측면 비율(3.4∼4/3의 기하)의 임의 측면비율이 만들어진다. 이 작물은 원래 크기로 전환된 스파크81나리다 이는 텐소리파크 82w에서 e2809c 슬림프로세싱으로 도입되어 왔다. 인스턴트_proprocession.distort_bing_box_clust_creprosed_criproraced_supplesseding.dessorch_back_crupprojection_ 여기에 무작위 작물(리사이즈)은 항상 수평,좌우 스파크82ip이 50% 확률로 뒤를 이었다. 이것은 도움이 되지만 필수적이지는 않다. 이를 우리의 디폴트 조작 정책에서 제거함으로써 상위 1위 라인 평가는 64.5%에서 63.4%로 100도에서 훈련된 우리 RNet-50 모델이 떨어진다. 색채 왜곡 색채는 색채 불안과 색채가 떨어지는 방식으로 구성된다. 우리는 일반적으로 더 강력한 색채 불안감이 도움이 되기 때문에 강도 파라미터를 설정한다. 텐서플로우를 이용한 컬러 왜곡을 위한 가명코드가 다음과 같다. tf defe colle_distature(image, s=1.0): # 이미지는 가치 범위가 있는 수십 가지다. #s는 색채 왜곡의 강점이다 디스플레이 _ jitter(x) : # 적용할 때마다 이를 따르는 질서를 풀어줄 수 있다. x = tf.image. random_brights (x, max, delta=0.8*s) x  = tf . mange. 컬러_jitter = 트랜스포메이션.ColorJiter(0.8*s, 0.8 *s,  0.2*s) rnnd_collor_jinter=trandomApply([coler_juter], p=0.8) Rnd_gray=trons.compose([cland_destort], 12Our colder 및 결과는 Tensorefac82w를 기반으로 한다. 0cA 간단한 비주얼 대표 컨트라스틱 러닝 프레임워크 _ 디스토르트 가우시아 모호함 이런 반사는 우리의 디폴트 정책에 있다. 유네트-50 훈련이 63.2%에서 64.5%로 100명에 달하는 훈련을 개선하는 등 도움이 되었다고 한다. 가우시아 핵심을 이용해 당시의 50% 이미지가 흐려진다. 무작위 샘플 cf83 e28888[0.1, 2.0]으로 이미지 높이/위드 10% 규모의 핵심 크기가 설정됐다. B 추가 실험 결과 B.1. 배치 크기와 트레이닝 스텝  Figure B.1은 크기가 다른 배트 크기, 훈련 크기로 훈련했을 때 라인 평가에서 상위 5 정확도를 보여준다. 이번 결론은 이전에 보여준 상위 1 정확도와 매우 유사한 결론인데, 여기서 다른 배트 크기와 훈련 단계의 차이가 조금 작아 보인다는 점을 제외하고는 수치 9과 피지 B.1 모두 다양한 배트 크기의 훈련을 할 때 (2017년 충성도 1등)과 비슷한 학습률의 라인 스케일링을 사용한다. 라인러 학습률 스케일링이 SGD/M순간 최적화로 인기를 끌고 있지만, 우리는 EPAC81을 기대하는 것이 더욱 바람직한 것으로 나타났다. 근본학습률 스케일링을 통해 라인 스케줄 케이스에서 배팅레이트 0.3c397 배치Size/256 대신 배치 사이즈가 있지만, 배트 크기 4096 (당분간 디폴트 부츠 크기) 모두 스케치 방식으로 학습률이 동일하다. 사소한 배트 크기로 훈련된 모델의 성능을 향상시키고 작은 배트량으로 실력을 개선한다는 관측이 나오는 탁 B.1에서 비교가 제시된다. 학습률 = 0.075c397e2889a 배치 크기 \\\\Epochs 100 256 512 10248 4096 8192 57.5/62.8 63.8 / 66.6 64.6 / 서로 다른 배트 크기와 훈련 크기 아래 선별평가(톱1)가 이뤄진다. 슬래시 신호 왼쪽에는 라인어 LR 스케일링으로 훈련된 모델이 있으며, 오른쪽에서는 스퀘어 뿌리 LR을 스케줄링하며 모델을 훈련하고 있다. 결과는 0.5% 이상이면 과감하게 나타난다. 스퀘어 뿌리 LR 스케일링은 작은 배트 크기에 더 좋은 효과가 있다. 우리는 또 대량(최대 32K) 규모가 크고 길게(최고 3200km) 훈련을 하며 사각형 뿌리 학습률이 스케일링된다. 수치 B.2에서 보여주는 연주는 8192개의 배트 크기에 정확하게 포함된 것으로 보이며, 훈련은 여전히 더 오래 지속적으로 신호 81개의 성능을 개선할 수 있다. 그림 B.1. 리니어 평가(Top-5)는 리넷-50의 크기와 크기가 다른 트레이닝을 받아 훈련을 받았다. 각 술집은 스쳐 지나가는 단 한 점이다 상위 1-1 정확성을 위해 9번 수치를 보세요 그림 B.2. 리니어 평가(Top-1)는 Resnet-50이 다른 배트 크기와 더 길게 훈련을 받았다. 여기에는 라인어 하나가 아닌 사각형 뿌리 학습율이 활용된다. 1002030040050060080090010016003200Trance 6062646687072Top 1Batch 2565120242420484096896816384327680cA 간단한 시각대표 학습 기준 B.2. 데이터 조작의 방향성이 더욱 향상되고 본문(테이블 6,7)에서 우리가 가장 좋은 성과를 낼 수 있다. 라인어 평가 의전을 위해서는 데이터 보다 넓은 기준으로 훈련된 RNet-50 모델(1c397, 2c398, 4c3997)이 각각 70.0(+0.7), 74.4(+0.2), 66.8(60.3)을 달성한다. 테이블 B.2에는 EPAC81ne tuning SimCLR 모델(Explace B.5를 보면 EPac81n-tung 시술의 세부 내용을 살펴보자)이 확보한 이미지넷 정확도가 나 흥미롭게도 이미지넷 트레이닝 세트(100%)에서 EPAC81ne tuning 할 때 우리의 RNet(4c397) 모델은 상위 1/95.4% 상위-513을 달성하는데, 이는 같은 세트의 스트레이션(i.e.)을 활용해 훈련을 하는 것보다 신호 811위(상위 78.4%-1/94 무작위 작물과 수평적인 스파크82ipp)이 있다. RENET-50(2c397)의 경우 미리 훈련한 Resnet-50 (2c297)을 스크랩(77.8% 상위 1/93.9% 정상 5)으로 훈련하는 것도 좋다. UNET-50을 위한 스파크81ntuning에서는 개선이 없다. 건축가 Renet-50 ResNet-50 (2c397) ReNet -50(4c398) 1% Label 10% 100%, Top 1 TOp 5 TOP 5 POP 1 POT 549.4 93.194.4 88.1 91.2 92.8 76.1 86.74.8 TB 2. 이미지넷이 1%, 10%, 가득 차 있는 SIMCLR(더 폭넓은 데이터 조작으로 미리 훈련된 것)을 스파크81ne 튜닝으로 획득한 클래스피어 81도 정확도를 기준으로 한다. 참고로 우리 UNET-50(4c397)은 100% 라벨에서 스크래치에서 훈련을 받아 상위 1,94.2% 정상 5위를 달성했다. B.3. 이곳에서는 훈련 단계와 강력한 데이터 교육이 감독 훈련에 어떤 영향을 미치는지 살펴보기 위해 '감독 모델을 위한 장기 훈련'의 효과가 나타난다. 같은 세트의 데이터 조작(랜덤 작물, 컬러 왜곡, 50% 가우시아 모래)에서 사용되지 않은 초대형 모델에서도 리넷-50과 RSNet-50(4c397)을 테스트한다. 그림 B.3은 상위 1 정확도를 보여준다 우리는 이미지넷에서 더 이상 감독된 모델을 훈련하면서 신호탄 81cant Therefac81t이 존재하지 않는다고 관측한다. 더 강력한 데이터 조작이 유엔넷-50(4c397)의 정확도를 조금 향상시키지만 유네트-50에는 도움이 되지 않는다. 더 강력한 데이터 추출이 적용되면 유네트-50은 일반적으로 더 긴 훈련(e.g.)이 필요하다. 최적의 결과를 얻기 위해서는 1450명이 14명이고, RNet-50명(4c397명)은 장기 훈련에서 81t을 기록하지 않는다. 모델 트레이닝 리넷-50 ReNet-50(4c397), 9000 Top 1 + Color 75.6 76.5 77.77 78.77.9 + 콜러 + Blur 72.777.57.7 79.3 테이블 B.3. 다양한 데이터 조작 절차 하에서 더 오래 훈련된 감독 모델의 톱 1 정확도(대조적 학습을 위한 동일한 세트의 자료 조작)가 가능하다. B.4 비라이나 프로젝트 헤드 피겨 B.3을 파악하면 라인 전망 매트릭스 W e28888 R2048c397248이 Z = W h를 계산하는 데 사용되는 이긴 가치 유통이 상대적으로 거의 없음을 보여주는 것으로 나타났다. 그림 B.4에는 우리 최고의 RNET-50(top-1 라인어 평가 69.3%)이 무작위로 선정한 10개 클래스를 대상으로 h와 Z = g(h)의 t-SNE(2008년 마텐앤해튼) 시각화를 보여준다. h로 대표되는 클래스는 Z에 비해 더 잘 분리된다. 상위 1위 / 95.2% 상위 5위 80.1%에 해당하는 것은 심클러를 자랑하는 데 대한 보다 넓은 반감이 없다. 오토아쿠먼트(2019년 큐브크 등 14위)를 기록하면 최적 시험 정확도는 900∼500도 사이에 달성할 수 있다. 0cA 간단한 시각대표 컨트라스틱 러닝 프레임워크(a) Y-axis가 일률적으로 유지된다. (b) Y-axis가 로그 스케일로 진행된다. 그림 B.3. 라인어 프로젝트 매트릭스 We2888 R2048c3972048은 G(h) = Wh.(a) h(b) Z = g(h)의 그림 B.4을 계산하기 위해 사용되었다. 인증 세트에서 무작위로 선정된 10개 클래스에서 이미지의 숨겨진 Vector의 t-SNE 시각화가 진행된다. B.5 네스터프 모멘텀 최적화기를 이용한 미세훈련 미세 훈련 프로세스 위스파크81ntune을 통한 반 초등학습, 배트 크기 4096, 기세 0.9, 학습률 0.8(학습 후 0.05c397 배치사이즈/256) 등을 따뜻하지 않게 한다. 무작위 크로핑(무작위로 왼쪽 오른쪽 스파크82pp, 리사이즈 224x224)만 사용해 준비 작업을 하고 있다. 우리는 정규화(체중 부패 포함)를 사용하지 않는다 1%가 표시된 데이터에 대해서는 60개의 스파크81ntune을 대상으로 표시하고, 10% 표시한 자료에서는 30개 스파이크81ne트une에 표시되어 있다. 1880년을 위해서는 주어진 이미지를 256x256으로 재정비하고, 224x224의 한 센터 작물을 가지고 있다. 테이블 B.4는 준초과 학습을 위한 다양한 방법에 대한 상위 1 정확도의 비교를 보여준다. 우리 모델들은 최첨단 모델을 개선하는 것이 특징이다 방법 건축 레이블 1% 10%, 톱1 25.4 ResNet-50 Ronet-50 SNet-50감독 기본 방법 : UDA(w.RandAug) FixMatch(w.randag) S4L(Rot+VAT+Ent. 민) 자체 초과 대표 학습만을 이용한 ResNet-50(4c397) 방식: CPC v2 SimCLR(시간) SiMCLR (시간대) SIMCLLR 52.78.3 58.53.0 - 56.4 68.8 71.5 73.1 63.7 74.4 TB 4. 이미지넷은 라벨이 거의 없이 훈련된 모델의 상위 1 정확도가 높다. 상위 5 정확성을 위해 7번 테이블을 살펴보세요 B.6 라인어 평가를 위해서는 1.6(학습률 0.1c397 배치,256)의 학습률이 높고 90개의 교육이 길어진다는 것을 제외하고는 스파크81ntuning( Appendix B.5)과 유사한 절차를 밟는다. 대안적으로 프리트레이닝 초파라미터와 함께 PRS 최적화기를 사용하는 것도 비슷한 성과를 내는 것이다. 나아가 베이스 앙코더 위에 라인어 클래스피어 81대(입력 중 정지_그라디언트가 있는 라인러 클래식 81기를 탑재해 라벨 정보가 암호화기 내 수비 82u를 막는다)를 동시에 훈련하고 비슷한 성과를 거둔다는 것이 웹81일이다. B.7. 여기서 라인어 평가와 미세터닝 사이의 교정은 훈련 단계와 네트워크 건축의 다른 설정 아래 라인 평가, 스파크81ne-tuning 간의 상관관계를 연구한다. 그림 B.5는 UNET-50(주사량 4096) 훈련 기준이 50에서 3200으로 다양할 때 라인 평가 대비 EPAC81ne tuning을 보여준다. 이들은 거의 라인 상관관계가 있는 반면 라벨 100e2892110e 2889292910e27892710e-2888992310e102101S Quarden Eigen Value 05001,0002,1501116S quared egenvaluue0cA 간편 프레임워크는 연수 기간보다 길어 보이는 것으로 보인다. 그림 B.5. 라인어 평가와 EPAC81ne tuning을 거쳐 다른 라인어로 훈련된 모델들의 최상위 1 정확도가 나타나고 있다. 그림 B.6은 선택의 다른 건축가를 위한 라인별 평가 대비 스파크81ntuning을 보여준다. 그림 B.6. 라인별 평가와 스파크81ntuning 아래 서로 다른 건축가들의 톱1 정확도가 높다. B.8. 이동학습 우리는 이미지넷에서 배운 자체 초과 대표성을 기반으로 한 새로운 데이터셋을 분류하기 위해 물류 감소 교실 81명이 훈련 중 모든 무게가 다르게 변화할 수 있는 라인 평가, EPAC81ne tuning 등 두 가지 설정으로 이전학습을 위한 자체감독 성과를 두 경우 모두 코르블리트  et 알이 묘사한 접근법을 따른다. (2019년) 우리의 준비 처리가 조금은 다르지만 말이다. B.8.1. METHODS 데이터셋(2009년 크리제프스키 앤 해튼), 버그 앤 알(2014년), SUN397 스테이지(Xiao et, 2010), 스탠퍼드 카르스(Krause Et All,2013년 알), PASCAL BORC 2007 클래스 ATH 81 TAST(에버햄 et an 2010), DTD(Cimpoie Ent Art, 2014), ASCIT Pets(파크하이 Ether Archart), FGVC Arich Alict(Magie Ant, 13013), CIFARC 등을 대상으로 이전 학습 공연을 조사했다. 이러한 데이터를 도입한 논문에서 평가 프로토콜을 따르면 FGVC 에어크래프트, 옥스퍼드-IIT Pets, 칼텍-101, CIFAR-100, 씨파르-100, 버드나프, SUN397, 스탠퍼드 자동차, DTD 등에 대한 상위 1위 정확도를 보고하고 있다. 에버햄 등에서는 11점 MAP 메트릭을 102점으로 나타냈다. 2007년 PASCAL VORC(2010)를 대상으로 했다. DTD와 SUN397의 경우 데이터셋 크리에이터들이 디팩81인 복수열차/테스트 스플릿을 제거하고 있다. 우리는 스파크81rst 분할에만 결과를 보고한다. 칼테크-101 데파크81은 기차/테스트 분할이 없어 기존 작품(2014년 도나후 등)과 공정한 비교를 위해 수업당 30개의 이미지를 무작위로 선택하고 나머지 시험을 선택했다. 데이터셋 크리에이터가 제작한 인증 세트 스펙81을 사용해 FGVC 항공기 초퍼파라미터를 1%6264666870Liner 평가 35.037.540.042.545.047.550.0Fine-tuning 10% 505606570linar 평점 30336394245454454Fnetung (1%) Width1x2x4xDept18345015255560570LInar 다른 데이터에 대해서는 초파라미터 튜닝을 하면서 인증을 위한 훈련 세트의 지하 세트를 실시했다. 인증 세트에서 최적의 초파라미터를 선택한 뒤 모든 훈련과 인증 이미지를 활용해 선정된 파라메터를 이용한 모델을 재교육했다. 우리는 시험 세트에 대한 정확성을 보고한다 '라인나 클래스피어 81'을 통한 이동학습을 통해 '동결된 프리랜서 네트워크'에서 추출된 기능에 대해 '(시드 196) 2 규모의 다자 물류 퇴출 교실'을 훈련했다. 우리는 L-BFGS를 이용해 소프트맥스 크로스엔트로피 객관성을 최적화했고, 데이터 조작을 적용하지 않았다. 미리 처리하면서 모든 영상이 224c39724센트 작물을 촬영한 뒤 바이큐브 리펌플링을 이용해 짧은 측면을 따라 2204픽셀로 재편됐다. 우리는 10e288926∼105개 사이에 45개의 로가리즘이 분산된 범위에서 2개의 정규화 패러머를 선택했다. 파인 튜닝 웨이 스파크81ne을 통해 이동학습을 시작으로 미숙한 네트워크의 무게를 초기화로 활용해 전체 네크워크를 조율했다. 모멘텀 파라미터 0.9으로 네스테로프 모멘텀을 갖춘 SGD를 이용해 256개의 배트 크기에서 2만 단계를 훈련했다. 정상화 통계를 위한 모멘텀 파라미터를 1인당 단계 수준인 최대(1e28892 10/s, 0.9)로 설정했다. 스파크81ntuning 기간 동안 데이터 조작이 이루어지면서 리사이즈와 스파카82pp으로 무작위 작물만 공연했는데, 자존감과는 대조적으로 색채조작이나 모호한 작물을 수행하지 않았다. 시험 시간에는 짧은 측면을 따라 256픽셀로 이미지를 재편하고 224c397 223센티미터 작물을 섭취했다. (특히 CIFAR-10과 시파-100 데이터에 대한 추가적인 정확도 개선이 가능할 수 있다.) 우리는 학습률과 체중 부패를 선택했으며, 무게 부패는 물론 10∼288926∼10e28923 사이의 무게감 부패 가치가 0.0001∼0.1∼7가지 로고리즘적으로 분산된 학습율은 7가지였다. 우리는 이러한 체중 부패의 가치를 학습률에 따라 나눈다 우리는 랜덤 초기화 훈련을 통해 네트워크를 무작위 초기 단계화로부터 스파크81netuning과 같은 절차를 이용해 훈련했지만 오래 걸렸고, 초고파라미터 그리드가 바뀌었다. 우리는 로고리즘 7개 그리드에서 초과파라미터를 선택했는데 0.001∼1.0∼8개의 로가리즘적으로 체중 부패의 가치가 10e288925∼10e128921.5 사이에 분산됐다. 중요한 것은 우리의 무작위 초기화 기본기준이 4만 단계로 훈련되어 있는데, 이는 코른블리트 등 8단계에서 보여준 것처럼 최대 수치의 정확도를 달성하기 위해 수파 81시간으로 긴 단계이다. (2019) 나프에서는 방법 가운데 통계적으로 신호 81cant 차이가 없고, 푸드-101, 스탠퍼드 자동차, FGVC 항공기 데이터 등에서도 무작위 초기화 훈련을 통해 스파크81ntuning이 작은 장점을 제공한다. 다만 나머지 8개 데이터에서는 프리트레이닝이 뚜렷한 장점이 있다. 우리는 이미지넷에서 훈련된 동일한 유네트 모델과 표준적인 크로스엔트로피 손실로 비교한다. 이들 모델들은 우리 자체 슈퍼 모델(크로프트, 강력한 컬러 변형, 모호함)과 같은 데이터 변형으로 훈련을 받고 있으며, 1000여 개의 스피커를 위해 훈련도 받는다. 더 강력한 데이터 조작과 더 긴 훈련 시간이 이미지넷에서 정확도를 파악하지는 못하지만 이들 모델들은 90개의 시그니처와 일반적인 자료 조작을 위해 라인별 평가를 위한 감독 기본 훈련보다 신호 81개를 더 잘했다는 사실을 감독한 RNet-50 기본선은 자체 감독 카운터파트 대비 69.3%인 이미지넷에서 상위 1위 정확도를 달성하는 반면 RNET-50(4c397) 기준은 78.3%로 자체감독 모델은 76.5%에 달한다. 통계 시그니처ac81cance Testing We the Signerace 81cencess of Moder 간 차이가 있는 신호등급을 위해 테스트한다. 두 가지 모델에 대한 예측을 감안하면 이번 무작위화를 실시한 뒤 각 모델별 예측과 정확도 차이를 계산해 무작정 분배에서 10만 개의 표본을 생산한다. 이어 예측 차이보다 극단적인 무분배에서 표본 비율을 계산한다. 상위 1 정확성을 위해서는 이 시술이 정확한 맥네마 시험과 같은 결과를 얻는다. 무효 가설에 따른 교환 가능성을 가정하는 것도 1등급 정확도를 의미하는데 유효하지만 평균 정밀 곡선을 계산할 때가 아니다. 따라서 우리는 MAP의 차이가 아닌 VOC 2007에서 정확도 차이를 보여주기 위해 신호 81cance 테스트를 한다. 이 시술의 동굴은 평가를 위해 이미지의 스파크81나이트 샘플을 사용해 발생하는 변동성만으로 모델을 훈련할 때 런투런 변수를 고려하지 않는다는 것이다. B.8.2. STANDARD REWRTS THE RESNETS 50(4c397) 결과 텍스트 8에서 감독 또는 자체 초과 모델에 대한 명확한 장점이 없는 것으로 나타났다. 그러나 이번 RNET-50 건축을 통해 감독된 학습은 자기 초등학습에 대한 명확한 장점을 유지하고 있다. 감독한 RNET-50 모델은 라인 평가가 있는 모든 데이터 시스템에서 자체 감독 모델을 뛰어나게 하고, EPAC81ne tuning으로 가장 많은 (12개 중 10개) 자료를 만들어낸다. 비주얼 대표 식품 CIFA10 CIFAR100 조류나프 SUN397 자동차 항공기 VORC2007 DTD Petech-101 꽃 라인 평가: SIMCLR(ours) 68.472.3 슈퍼 파인트 90.63.6 97.9 71.68.3 85.9 86.4 80.2 37.7 75.8 76.1 63.5 53.6 자연 이미지 데이터 12개 전반에 걸쳐 감독된 기본 기준으로 우리 자체 감독 접근법의 이전 학습 성능을 비교해 비디오넷 해석 Resnet 모델을 활용했다. 유네트(4c397) 건축물로 결과를 얻을 수 있는 수치 8도 살펴보자. 모델은 이미지넷에서 감독된 모델과 자체감독 모델의 정확성 격차가 연관될 수 있다. 자체 초과 RNet은 절대적인 측면에서 감독 모델보다 6.8% 나쁜 상위 1위 69.3%, 자체 감독된 RNET(4c397) 모델은 76.5%로 감리 모델에 비해 1.8%나 나쁘다. B.9 이미지넷을 주요 데이터셋으로 활용하는 데 초점을 맞추지 못하는 모델을 선점하는 동안 우리의 방식도 다른 데이타셋과 함께 작동한다. 우리는 CIFAR-10을 다음과 같이 테스트해 시연한다 우리의 목표가 CIFAR-10 성능을 최적화하는 것이 아니라 이미지넷에 대한 관측의 추가적인 개념 81:1을 제공하기 위해 같은 건축물(ResNet-50)을 사용하고 있다. CIFAR-10 영상이 이미지넷 영상보다 훨씬 작기 때문에 3x3 Conv 1의 EPAC81rt 7x7 CONv를 대체하고, EPac81rst MOX Poolling 운영도 제거한다. 데이터 조작을 위해서는 이미지넷, 15, 컬러 왜곡(강도=0.5)과 같은 인셉션 작물(디팩82ip, 리사이즈 32x32)을 사용해 가우시아 모래를 벗어나게 된다. 우리는 학습률이 50.5,1.0,1.5,2048,4096,512,1024,408,0096:00에서 기온이 0.1,0.5.1,10.0,256의 배트 크기를 기준으로 하고 있다. 나머지 설정(최적화, 체중 부패 등 포함) 우리 이미지 네트 훈련과 똑같다 같은 건축물과 배트 크기를 이용한 감독 기본선에서 95.1%로 나타나는 것에 비해 1024배트로 훈련된 우리 최고의 모델은 94.0%의 라인 평가 정확도를 달성할 수 있다 라인어 평가 결과를 CIFAR-10에 보고하는 최고의 자체 감시 모델은 AMDIM(바흐만 et 2019)으로 우리보다 25c397이 큰 모델로 91.2%를 달성했다. 보다 적합한 기반 네트워크를 활용하는 것은 물론 추가 데이터 변형을 접목해 우리 모델이 개선될 수 있다는 점에 주목한다. 각기 다른 배트 크기와 훈련 단계인 피규어 B.7에 따른 퍼포먼스는 다양한 배치 크기, 교육 단계 아래 라인별 평가 성과를 보여준다. 이미지넷에 대한 우리의 관측과 일치하는 결과는 4096개의 배트 규모가 가장 많지만 CIFAR-10에서 실적 저하가 작은 것으로 보인다. 그림 B.7. 라인어 평가는 CIFAR-10 데이터셋에서 다양한 배트 크기와 편집력을 갖춘 훈련을 받았다. 각 술집은 학습률(0.5,1.0,1.5)과 온도 cf84 = 0.5로 평균 3개 이상이 운영된다. 오류바는 표준 일탈을 표시한다 이미지넷 영상보다 CIFAR-10 영상이 훨씬 작고 이미지 크기가 사례 가운데 다르지 않지만 리사이즈와 함께 크로핑하는 것은 여전히 대조적인 학습을 위한 매우 효과적인 성찰이라는 점을 1002003004006007008009001000 Trance 8082848688909294Top 1Batch 규모 2565121024204840960cA 시각대표 컨트라스틱 프레임워크 B.8 이하의 최적 온도는 다양한 부분 크기 아래 3가지 온도로 훈련된 모델의 라인 평가를 보여준다. 우리는 융합을 위한 훈련을 할 때(e.g.) EPAC81을 한다. 300개 이상의 훈련비율을 기준으로 하면 0.1, 0.5, 1.0 이하의 최적 온도인 '트레이닝비율'이  0.5로 대량 크기와 상관없이 일관된 것으로 보인다. 다만 크기가 커지면서 cf84 = 0.1로 성능이 개선돼 최적 온도 0.0을 향한 작은 변화를 제시할 수 있다. 289a4 300(b) 트레이닝 코트 >300 피규어 B.8. 모델(ResNet-50)에 대한 라인어 평가는 CIFAR-10에서 세 가지 온도가 다른 크기로 훈련된 것이다. 각 술집은 다양한 학습률과 전체 열차 운행량이 다른 복수 운행 평균을 기록한다. 오류바는 표준 일탈을 표시한다 B.10 다른 손실 재능을 위한 터닝은 NT-Xent 손실에 최선을 다하는 학습률이 다른 실손 기능에 대한 좋은 학습율이 아닐 수 있다. 공정한 비교를 보장하기 위해서는 마진 손실과 물류 손실 모두에 대한 초파라미터도 조율한다. 스피스텍 81cally에서는 두 손실 기능을 모두 대상으로 ' {0.01', 0.1, ' 0.3', '0.5', 1.0) 등에서 학습률을 조절한다. 마진 손실에 대해서는 0.4, 0.8, 1.6.1의 마진을 추가로 조정하고, 물류 손실의 경우 기온은  {0.1,0.2,0.5,1.0.1.로 나타났다. 단순히 실적을 약간 훼손하면서도 공정한 비교를 보장하는 한 방향(양쪽 대신)에서 부정적인 부분만 고려한다. 주요 텍스트에서 언급한 바와 같이 관련 방식에 대한 C. 추가 비교는 심클R의 개별 부품 대부분이 이전 작업에 등장했고, 실적 개선은 이러한 디자인 선택이 결합된 결과다. 테이블 C.1은 우리 방식의 디자인 선택을 기존 방식과 높은 수준의 비교를 제공한다. 이전 작업과 비교하면 우리의 디자인 선택이 대체로 간단하다. 데이터 8분기 세관 모델 CPC v2 AMDIM 패스트 오토액이다. CMC 패스트 오토업 Crop + Color MoCo POR CROP + 컬러 SimCLR Crob + Blur Base Encoder Resnet-161 (모드리퍼ac81ed) Comp Renet RNet-50 (2c397, L+ab) RNet-50(4c398) Ross Xent Project PixelCNNN Non-liner 각 방식별로 디자인 선택과 트레이닝 설정(이미지넷에서 가장 좋은 결과를 얻기 위한)을 높은 수준으로 비교한다. 여기서 제공된 설명이 일반적이라는 점에서 두 가지 방법에 맞춰도 형식과 시행이 다를 수 있다(e.g. 색채 조화를 위한) 원본 논문을 좀 더 자세히 보세요. #표본은 여러 패치로 나뉘어 효과적인 배트 크기를 나타낸다. 이28897A 메모리은행이 취업하고 있다. 아래에서는 최근 제시된 대조적 대표 학습 방식에 대한 심도 있는 비교를 제공하고 있다: e280a2 DIM/AMDIM(Hjelm et, 2018; Bachman et.,2019)이 콘비넷의 중층을 예측해 글로벌,현지,현지 동네 예측을 달성한다. 콘비넷은 네트워크의 수용성 스파크81elds(e.g.)에 대한 시그니퍼 81cant 제약을 배치하기 위해 디펜 모디퍼팩81ed를 구축한 리스넷이다. 많은 3x3 CONV를 1x1 Convs로 교체한다. 우리의 틀에서는 예측 과제와 암호화를 무작위로 설정하고, 256512102420484096Batch 규모 75.077.580.082.585.087.590.092.595.0Top 1TOP 0.10.51.025120424240496BAtth 909192939495TOp 1top 0.051.5100cA 간단한 프레임워크를 시각 표준 및 보다 강력한 리넷을 사용할 수 있다. 우리의 NT-Xent 손실 기능은 비슷한 점수 범위를 제한하기 위해 정상화와 온도를 이용하는 반면 정규화로 탄h 기능을 사용한다. 우리는 단순한 데이터 조작 정책을 사용하고 있으며, 패스트 어투어그먼트는 최선의 결과를 위해 사용한다. e280a2 CPC v1 및 v2(Oord et al, 2018; Hc3a9naf et, 19) defec81은 결정적인 전략을 이용해 패치를 패치로 나누는 맥락 예측 과제와 이러한 패치들을 집계하기 위해 패키지 네트워크(PixelCNN)를 분리 베이스 암호화 네트워크는 원래 이미지보다 상당히 작은 패치만 보고 있다. 예측 과제와 암호화 건축을 해결하기 때문에 맥락 네트워크를 필요로 하지 않고, 우리 암호가 더 넓은 결의의 스펙트럼의 이미지를 살펴볼 수 있다. 여기에 정상화와 온도를 지렛대로 하는 NT-Xent 손실 기능을 활용하는 반면 이들은 정상적인 크로스엔트로피 기반 객관적으로 활용한다. 우리는 간단한 데이터 조작을 사용한다 e280a2 InstDic, MoCo, PART(2018, 2019, 미사라 & Van Der Maten, 19)는 도소비츠키  et 알이 원래 제안한 Experar 접근법을 일반화한다. (2014년) 그리고 노골적인 메모리 은행의 레버리지가 있다. 메모리 뱅크를 사용하지 않는다. 더 큰 배트 크기로 마이너스 샘플 수피 81개가 사용된다. 비라인 프로젝션 헤드도 활용하고, 전시 헤드 앞에서 대표성을 활용한다. 비슷한 유형의 조각품(e.g., 무작위 작물, 색채 왜곡)을 사용하지만, 사양 81c 파라미터가 다를 수 있을 것으로 기대한다. e280a2 CMC(Tian et al, 2019)는 각 시야별로 분리된 네트워크를 사용하는 반면, 우리는 단순히 모든 무작위 조회에 공유된 단일 네트웍을 사용한다. 데이터 조작, 프로젝트 헤드, 손실 기능도 다르다. 우리는 메모리 은행 대신 더 큰 부분을 사용한다 E280a2, Yee et al. (2019년) 같은 이미지의 비유와 비유 없는 복사본 사이에서 극대화하면서 우리 프레임의 두 지점(Figuer 2) 모두에게 데이터 조작을 적용한다. 기지 기능망 생산에 비라인 프로젝트를 적용하고, 프러젝트 전에 대표성을 사용하는 것도 예 et알이다. (2019년) 라인별로 예측된 스파크81 110개의 숨겨진 베이커를 대표로 활용한다. 다중 액셀러레이터를 활용한 대량 크기의 트레이닝을 할 때 글로벌 BN을 활용해 대표성을 크게 떨어뜨릴 수 있는 짧은 코트를 피한다. 0c'\"]\n"
     ]
    }
   ],
   "source": [
    "print(text_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "immune-wales",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "간단하게 유지하기 위해 대형 배치 사이즈와 함께 훈련하는 것은 메모리 뱅크(2018년 우에트, 2019년 헤이트, 알, 알)로 모델을 교육하지 않는다. 훈련을 안정시키기 위해서는 모든 부분 크기를 위해 US 최적화기(You et al, 2017)를 사용한다. 데이터 평행성으로 배포된 훈련에서는 BN의 의미와 발음이 일반적으로 장치당 현지에서 집계된다.\n"
     ]
    }
   ],
   "source": [
    "txt_parsed = text_trans.split('.')\n",
    "\n",
    "print(Summarizer('. '.join(txt_parsed[80:120])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "gothic-speaker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40883"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-transaction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "optional-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(src, gap=10, space='\\n\\n'):\n",
    "    parsed = src.split('.')\n",
    "    for i in range(len(parsed)-1, 0, -1):\n",
    "        if len(parsed[i]) < 10:\n",
    "            del parsed[i]\n",
    "\n",
    "    sum_ = []\n",
    "    for i in range(0, len(parsed), gap):\n",
    "        sum_.append(Summarizer('. '.join(parsed[i:i+gap])))\n",
    "    sum__ = space.join(sum_)\n",
    "    return sum__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "collective-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum__ = summarize(text_trans, gap=10, space=' ')\n",
    "sum__ = summarize(sum__, gap=10, space=' ')\n",
    "sum__ = summarize(sum__, gap=10, space=' ')\n",
    "sum__ = summarize(sum__, gap=10, space=' ')\n",
    "sum__ = summarize(sum__, gap=10, space='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "armed-virgin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "심클R이 배운 자체 감시 표현에 대해 훈련한 라인어 클래스피어 81명은 이전 최첨단보다 7% 상대적으로 개선된 상위 1위 정확도 76.     5%를 달성해 감독한 RNet-50의 성과와 일치한다.      라벨의 1%에 불과한 스파크81n을 탑재하면 85.     8%의 상위 5 정확도를 달성해 100c397개의 라벨이 적은 알렉스넷을 형성하는 퍼포먼스가 뛰어나다. 이 작품에서는 시각적 표현에 대한 콘트라스티브 학습을 위한 간단한 틀을 도입해 심클R이라고 부른다. 우리는 이들 스파크81회를 결합해 이마-지넷 ILSVRC-2012(러시아코프스키 등 2015년)에서 자체 초,준 초등학습에 새로운 첨단학습을 달성했다.      라인어 평가 프로토콜에 따르면 심클R은 상위 1위 정확도 76 5%를 달성해 이전 첨단(2019년 Hc3a9나프 등)보다 상대적으로 7% 개선된 것이다.\n",
      "\n",
      "개별 데이터 조작의 효과와 조작 구성의 중요성을 이해하기 위해 개별적으로나 쌍으로 조각을 적용할 때 틀의 성능을 조절한다. 이런 불안을 없애기 위해서는 이번 불안정성을 위한 비대칭 데이터 전환 설정을 고려한다. 강도 1(+Blur 가 우리의 디폴트 데이터 조작 정책이다.  그럼에도 불구하고 이번 설정은 개별 데이터 조작이나 그 조작의 영향을 실질적으로 바꾸어서는 안 된다 일반적으로 눈에 띄는 특징을 배우기 위해서는 색채 왜곡으로 꾸미는 것이 비판적이다.  (1) 아이덴티티 맵핑, (2) 라인 프로젝트는 기존 여러 접근법(2018년 우에트 알)에서 사용했던 것처럼, (3) 바흐만 엔트 알과 비슷한 하나의 추가 숨겨진 층(RE (2019) 비라인 프로젝트가 라인 전망(+3%)보다 낫고, 전망이 없는(10% 이상 것보다 훨씬 좋다는 관측이 나온다.  나아가 비라인어를 사용하더라도 머리 앞의 층은 그 뒤의 층보다 훨씬 더 좋은(10% 이상) 것으로 나타나는데, 그 이후에는 프로젝트 헤드 이전의 숨겨진 층이 층 이후보다 더 나은 표현임을 보여준다.  비라인 프로젝트 이전에 대표 티온을 사용하는 것이 중요하다는 것은 대조적 손실로 인한 알리타 티온의 손실 때문이라고 단언한다.  단순히 우리는 단 한 가지 시각에서 부정적인 부분만 고려한다 326412825651210242048 프로젝트 출력 차원성 3040506070Top 1 Prosult LinearNon-linerNornone0cA 간편학습 마르긴 NT-Logi.      마르진(sh NT-Logi이다.\n",
      "\n",
      "Renet-50 MoCo ResNet-50 PINET-50 CPC v2 SimCLR (시간) Rennet - 50 The Arters : Revente-50(4c397) BigBi AMDIM Cust-ResNet Risnets-50 이미지 네트워크는 다양한 자기 감시 방식으로 배운 레퍼레이션에서 훈련된 라인어 클래스피어 81명의 이미지를 배웠다. 다시 한번 우리의 접근방식은 라벨의 1%와 10% 모두 첨단보다 시그니처 81가지로 개선된다. 유럽 컴퓨터 비전 컨퍼런스에서는 PP가 열렸다.\n",
      "\n",
      "5%로 100명에 달하는 훈련을 개선하는 등 도움이 되었다고 한다. 우리 모델들은 최첨단 모델을 개선하는 것이 특징이다 우리는 단순한 데이터 조작 정책을 사용하고 있으며, 패스트 어투어그먼트는 최선의 결과를 위해 사용한다\n"
     ]
    }
   ],
   "source": [
    "print(sum__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "metallic-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "gec = Pororo(task=\"gec\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "sophisticated-watch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>R<unk>. <unk> <unk>'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gec(sum__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "helpful-philippines",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"b'A Simple Framework for Contrastive Learning of Visual Representations Ting Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton 1 0 2 0 2 l u J 1 ] G L.\", 's c [ 3 v 9 0 7 5 0.', '2 0 0 2: v i X r a Abstract This paper presents SimCLR: a simple framework for contrastive learning of visual representations.', 'We simplify recently chosen contrastive self - supervised learning algorithms without requiring specialized architecture or a memory bank.', 'In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework.', 'We show that (1) composition of data augmentations plays a critical role in deefac81ning effective predictive tasks, (2) introducing a learn - able nonlinear transformation between repre- sentation and the contrastive loss substantially im- proves the quality of the learned representations, and (3) contrastive learning', 'By combining these efac81ndings, we are able to considerably outperform previous methods for self -upervised and semi-upervised learning on ImageNet.', 'A linear classiefac81er trained on self-upervised representations learned by Sim- CLR achieves 76.', '5% top-1 accuracy, which is a 7% relative improvement over previous state-of- the-art, matches the performance of a supervised ResNet-50.', 'When efac81ne-tuned on only 1% of the labels, we achieve 85.', '8% top-5 accuracy, outper- forming AlexNet with 100c397 fewer labels.', 'Introduction Learning effective visual representations without human supervision is a long-standing problem.', 'Most mainstream approaches fall into one of two classes: generative or dis- criminative.', 'Generative approaches learn to generate or otherwise model pixels in the input space (Hinton et al.', ', 2006; Kingma & Welling, 2013; Goodfellow et al.', '1Google Research, Brain Team.', 'Correspondence to: Ting Chen <iamtingchen@google.', 'Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020.', 'Copyright 2020 by the author(s).', '1Code available at https: //github.', 'com/google-research/simclr.', 'Figure 1.', 'ImageNet Top-1 accuracy of linear classiefac81ers trained on representations learned with different self -upervised meth- ods (pretrained on ImageNet).', 'Gray cross indicates supervised ResNet-50.', 'Our method, SimCLR, is shown in bold.', 'However, pixel-level generation is computationally expen- sive and may not be necessary for representation learning.', 'Discriminative approaches learn representations using objec- tive functions similar to those used for supervised learning, but train networks to perform pretext tasks where both the in - puts and labels are derived from an unlabeled dataset.', 'Many such approaches have relied on heuristics to design pretext tasks (Doersch et al.', ', 2015; Zhang et al.', '; 2016; Noroozi & Favaro, 2016; Gidaris et al.', ', 2018), which could limit the generality of the learned representations.', 'Discriminative approaches based on contrastive learning in the latent space have recently shown great promise, achieving state-of-the- art results (Hadsell et al.', ', 2006; Dosovitskiy et al.', ', 2014; Oord et al.', ', 2018; Bachman et al.', 'In this work, we introduce a simple framework for con- trastive learning of visual representations, which we call SimCLR.', 'Not only does SimCLR outperform previous work (Figure 1), but it is also simpler, requiring neither special- ized architecture (Bachman et al.', ', 2019; Hc3a9naff et al.', ', 2019) nor a memory bank (Wu et al.', ', 2018; Tian et al.', ', 2019; He et al.', ', 2019; Misra & van der Maaten, 2019).', 'In order to understand what enables good contrastive repre- sentation learning, we', 'Pč-c2xSimCLRSimCL ChampR (2x)Sim ruR (4x)Supervised0cA', 'In addition, unsupervised contrastive learning language from stronger data augmen- tation than supervised learning.', 'e280a2 Introducing a learnable nonlinear transformation be- tween the representation and the contrastive loss of substan- tially improves the quality of the learned representations.', 'e280a2 Representation learning with contrastive cross entropy loss, language from normalized embeddings and an appro- priately adjusted temperature parameter.', 'e280a2 Contrastive learning language from larger batch sizes and longer training compared to its supervised counterpart.', 'Like supervised learning, contrastive learning language from deeper and wider networks.', 'We combine these efac81ndings to achieve a new state-of-art in self-upervised and semi -upervised learning on Ima- geNet ILSVRC-2012 (Russakovsky et al.', 'Under the linear evaluation protocol, SimCLR achieves 76.', '5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art (Hc3a9naff et al.', 'When efac81ne-tuned with only 1% of the ImageNet labels, SimCLR achieves 85.', '8% top-5 accuracy, a relative improvement of 10% (Hc3a9naff et al.', 'When efac81ne-tuned on other natural image classiefac81ca- tion datasets, SimCLR performs on par with or better than a strong supervised baseline (Kornblith et al.', ', 2019) on 10 out of 12 datasets.', 'The Contrastive Learning Framework is Inspired by recent contrastive learning algorithms (see Sec- tion 7 for an overview), SimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space.', 'As illustrated in Figure 2, this framework comprises the following four major components.', 'e280a2 A stochastic data augmentation module that transforms any given data example randomly resulting in two cor- related views of the same example, denoted cb9cxi and cb9wxj, which we consider as a positive pair.', 'In this work, we sequentially apply three simple augmentations: random cropping followed by resizing back to the original size, run- dom color distortions, and random Gaussian blur.', 'As shown in Section 3, the combination of random crop and color distortion is crucial to achieve a good performance.', 'e280a2 A neural network based encoder f (c2b7) that extracts repre- sentation vectors from augmented data examples.', 'Our framework allows various choices of network archi- tecture without any constraints.', 'We opt for simplicity and adopt the commonly used ResNet (He et al.', ', 2016) zi g(c2b7) hi f (c2b4) cb9cxi Maximize agreement e28690e2892 Representatione2889288692 te288kbcT x T e288bc (cid: 48) t zj g(ckb7) hj f (ciab7) cbak9cxj Figure 2.', 'A simple framework for contrastive learning of visual representations.', 'Two separate data augmentation operators are sampled from the same family of augmentations (t e288bc T and t(cid: 48) e28kbc T ) and applied to each data example to obtain two correlated views.', 'A base encoder network f (c2b7) and a projection head g(c2b4) are trained to maximize agreement using a contrastive loss.', 'After training is completed, we throw away the projection head g(c2b7) and use encoder f (c2b4) and representation h for downstream tasks.', 'to obtain hi = f ( cb9cxi) = ResNet( cb9lxi) where hi e2888 Rd is the output of the average pooling layer.', 'e280a2 A small neural network projection head g(c2b7) that maps representations to the space where contrastive loss is applied.', 'We use a MLP with one hidden layer to obtain zi = g(hi) = W (2)cf83(W (1)hi) where cf83 is a ReLU non- linearity.', 'As shown in section 4, we efac81nd it', 'e280a2 A contrastive loss function deefac81ned for a contrastive pre- diction task.', 'Given a set { cb9cxk} including a positive pair of examples, cb9coxi and cb9lxj, the contrastive prediction task aims to identify cb9wxj in { cb4cxkčk(cid: 54)', 'We randomly sample a minibatch of N examples and deefac81ne, the contrastive prediction task on pairs of augmented exam- ples derived from the minibatch, resulting in 2N data points.', 'We do not sample negative examples explicitly.', 'Instead, given a positive pair, similar to (Chen et al.', ', 2017), we treat the other 2(N e28892 1) augmented examples within a minibatch as negative examples.', 'Let sim(u, v) = u(cid: 62)v/(cid', 'cosine similarity).', 'Then the loss of function for a positive pair of examples (i, j) i deefac81n as (cid: 96)i, j = e2892 log exp(sim(zi, zj)/cf84 ) 1[k(cid: 54)=i] exp(slim(zi ; zk)/c84 ), (1) where 1(k(cids: 54) rui] e2888 {0 ; 54:4i] eicator function evaluating to 1 iff k (cid ) 54)= i and cf84 denotes a temperature parameter.', 'The efac81- nal loss is computed across all positive pairs, both (i, j) and (j, i), in a mini-batch.', 'This loss has been used in previous works (Sohn, 2016; Wu et al.', ', 2018; Oord et al.', ', 2018); for convenience, we term it NT-Xent (the normalized temperature-scaled cross entropy loss).', '(cid: 80)2N k=1 0cA Simple Framework for Contrastive Learning of Visual Representations Algorithm 1 SimCLRe28099 main learning algorithm.', 'input: batch size N, constant cf84, structure of f, g, T,', 'for sampled minibatch {xk}N for all k e28888 {1,.', '; N} do k=1 do draw two augmentation functions te288bcT, t(cid: 48)e288bscT # the efac81rst augmentation cb9cx2ke28921 = t(xk) h2ke2D821 = f ( cb9cocx2889221) z2kk289921( g(h2ke2921 )1) # the second', ', 2N} and j e28888 {1.', \"; 2N} do si, j = z(cid: 62) i zj/((cid': 107)zi(cids: 107)(cor: 107)zj(cid! 107)) (cid: 80)N k=1 [(cid ) 96)(2ke288921 ; 2k) + (cid ) 12)(2k ; 2,000,000889921)] end for (cids: 80)2N deefac81ne (cids: 96))(i, j) as (cid a: 96)(i ; j)(e2892 log L = 1 update networks f and g to minimize L 2N k\", 'Training with Large Batch Size To keep it simple, we do not train the model with a memory bank (Wu et al.', ', 2018; He et al.', 'Instead, we vary the training batch size from 256 to 8192.', 'A batch size of 8192 gives us 16382 negative examples per positive pair of both augmentation views.', 'Training with large batch sizes may be unstable when using standard SGD/Momentum with linear learning rate scaling (Goyal et al.', 'To stabilize training, we use the language optimizer (You et al.', ', 2017) for all batch sizes.', 'We train our model with Cloud TPUs, using 32 to 128 cores depending on the batch size.', '2 Global BN.', 'Standard ResNets use batch normaliza- tion (Ioffe & Szegedy, 2015).', 'In distributed training with data parallelism, the BN mean and variance are typically aggregated locally per device.', 'In our contrastive learning, as positive pairs are computed into the same device, the model can exploit the local information leakage to improve pre - diction accuracy without improving representations.', 'We ad- dress this issue by aggregating BN means and variance over all devices during training.', 'Other approaches include shufefac82 data examples across devices (He et al.', ', 2019), or replacing BN with layer norm (Hc3a9naff et al.', '2With 128 TPU v3 cores, it takes e288bc1.', '5 hours to train our ResNet-50 with a batch size of 4096 to 100 epochs.', 'A B D C (a) Global and local views.', '(b) Adjacent views.', 'Figure 3.', 'Solid rectangles are images, dashed rectangles are run- dom crops.', 'By randomly cropping images, we sample contrastive prediction tasks that include global to local view (B e28692 A) or adjacent view (D e2842 C) prediction.', 'Evaluation Protocol Here we lay out the protocol for our empirical studies, which aim to understand different design choices in our framework.', 'Dataset and Metrics.', 'Most of our study for unsupervised pretraining (learning encoder network f without labels) is done using the ImageNet ILSVRC-2012 dataset (Rus- sakovsky et al.', 'Some additional pretraining experi- ments on CIFAR-10 (Krizhevsky & Hinton, 2009) can be found in Appendix B.', 'We also test the pretrained results on a wide range of datasets for transfer learning.', 'To evalu- eat the learned representations, we follow the widely used linear evaluation protocol (Zhang et al.', ', 2016; Oord et al.', ', 2018; Bachman et al.', ', 2019; Kolesnikov et al.', ', 2019), where a linear classiefac81er is trained on top of the frozen base net- work, and test accuracy is used as a proxy for representation quality.', 'Beyond linear evaluation, we also compare against state-of-arts on semi-upervised and transfer learning.', 'Default setting.', 'Unless otherwise speciefac81ed, for data aug- mentation we use random crops and resize (with random efac82ip), color distortions, and Gaussian blur (for details, see Appendix A).', 'We use ResNet-50 as the base encoder net- work, and a 2-layer MLP projection head to project the representation to a 128-dimensional latent space.', 'As a loss, we use NT-Xent, optimized using', '3 c397 BatchSize/256) and weight decay of 10e288926.', 'We train at batch size 4096 to 100 epochs.', '3 Fur- thermore, we use linear warmup for the efac81rst 10 epochs, and decay the learning rate with the cosine decay schedule without restarts (Loshchilov & Hutter, 2016).', 'Data Augmentation for Contrastive Representation Learning Data augmentation deefac81nes predictive tasks.', 'While data augmentation has been widely used in both supervised and unsupervised representations, learning (Krizhevsky et al.', ', 3Althern max performance is not reached in 100 epochs, rea- sonable results are achieved, allowing fair and efefac81cient ablations.', '0cA Simple Framework for Contrastive Learning of Visual Representations (a) Original (b) Crop and resize (c) Crop, resize (and efac82ip) (d) Color distort.', '(drop) (e) Color distort.', '(jitter) (f) Rotate {90e297a6, 180e294a6, 270e299a6} (g) Cutout (h) Gaussian noise (i) Gausian blur (j) Sobel efac81ltering Figure 4.', 'Illustrations of the studied data augmentation operators.', 'Each augmentation can transform data stochastically with some internal parameters (e.', 'rotation degree, noise level).', 'Note that we only test these operators in ablation ; the augmentation policy used to train our models only includes random crops (with efac82ip and resize), color distortion, and Gaussian blur.', '(Očal image cc-by: Von.', 'grzanka) 2012; Hc3a9naff et al.', ', 2019; Bachman et al.', ', 2019), it has not been considered as a systematic way to deefac81ne the con- trastive prediction task.', 'Many existing approaches deefac81ne contrastive prediction tasks by changing the architecture.', 'For example, Hjelm et al.', '(2018); Bachman et al.', '(2019) achieved global-to-local view prediction via constraining the receptive efac81ld in the network architecture, whereas Oord et al.', '(2018); Hc3a9naff et al.', '(2019) achieved neighboring view prediction via a efac81xed image splitting procedure and a con- text aggregation network.', 'We show that this complexity can be avoided by performing simple random cropping (with resizing) of target images, which creates a family ofč- tive tasks subsuming the above mentioned two, as shown in Figure 3.', 'This simple design choice conveniently decouples the predictive task of other components, such as the neural network architecture.', 'Broader contrastive prediction tasks can be deefac81ned by extending the family from augmentations and composing them stochastically.', 'Composition of data augmentation operations is crucial for learning good representations To systematically study the impact on data augmentation, we consider several common augmentations here.', 'One type of augmentation involves spatial/geometric transformation of data, such as cropping and resizing (with horizontal efac82ipping), rotation (Gidaris et al.', ', 2018) and cutout (De- Vries & Taylor, 2017).', 'The other type of augmentation involves appearance transformation, such as color distortion (including color dropping, brightness, contrast, saturation, hue) (Howard, 2013; Szegedy et al.', ', 2015), Gaussian blur, and Sobel efac81ltering.', 'Figure 4 visualizes the augmentations that we study in this work.', 'Figure 5.', 'Linear evaluation (ImageNet top-1 accuracy) under in- dividual or composition of data augmentations, applied only to one branch.', 'For all columns but the last, diagonal entries corre- spond to single transformation, and off-diagonals correspond to the composition of two transformations (applied sequentially).', 'The last column reefac82ects the average over the row.', 'To understand the effects of individual data augmentations and the importance of augmentation composition, we in- vestigate the performance of our framework when applying augmentations individually or in pairs.', 'Since ImageNet images are of different sizes, we always apply crop and re- size images (Krizhevsky et al.', ', 2012; Szegedy et al.', ', 2015), which makes it difefac81cult to study other augmentations in the absence of cropping.', 'To eliminate this, we consider an asymmetric data transformation setting for this ablation.', 'Speciefac81cally, we always efac81rst randomly crop im- ages and resize them to the same resolution, and we then apply the targeted transformation() only to one branch of the framework in Figure 2, while leaving the other branch as the identity (i.', 't(xi) = xi).', 'Note that this asymmet- CropCutoutColorSobelNoiseBlurRotateAverage2nd transformation is the asrop ofCutoutDolorS likelyelNisingRotate1st transformation33.', '810203040500cA Simple Framework for Contrastive Learning of Visual Representations (a) Without color distortion.', '(b) With color distortion.', 'Figure 6.', 'Histograms of pixel intensities (over all channels) for different crops of two different images (i.', 'two rows).', 'The image for the efac81rst row is from Figure 4.', 'All axes have the same range.', 'Methods SimCLR Supervised 1/8 59.', '0 Color distortion strength 1/4 61.', '4 1 (+Blur) AutoAug 61.', '1 Table 1.', 'Top-1 accuracy of unsupervised ResNet-50 using linear evaluation and supervised ResNets-505, under varied color distor- tion strength (see Appendix A) and other data transformations.', 'Strength 1 (+Blur) is our default data augmentation policy.', 'ric data augmentation hurts the performance.', 'Nonetheless, this setup should not substantively change the impact of individual data augmentations or their compositions.', 'Figure 5 shows linear evaluation results from individual and composition of transformations.', 'We observe that no single transformation sufefac81ces to learn good representations, even though the model can almost perfectly identify the positive pairs in the contrastive task.', 'When composing aug- mentations, the contrastive prediction task becomes harder, but the quality of representation improves dramatically.', 'Ap- pendix B.', '2 provides a further study on composing a broader set of augmentations.', 'One composition of augmentations stands out: random crop- ping and random color distortion.', 'We conjecture that one serious issue when using only random cropping as a data augmentation is that most patches of an image share a similar color distribution.', 'Figure 6 shows that color his- tograms alone sufefac81ce to distinguish images.', 'Neural nets may exploit this shortcut to solve the predictive task.', 'There- fore, it is critical to composing cropping with color distortion in order to learn generalizable features.', 'Contrastive learning needs stronger data augmentation than supervised learning. To further demonstrate the importance of the color aug- mentation, we adjust the strength of color augmentation as 5Supervised models are trained for 90 epochs; longer training improves the performance of stronger augmentation by e288bc 0.', 'Figure 7.', 'Linear evaluation of models with varied depth and width.', 'Models in blue dots are ours trained in 100 epochs, models in red stars are ours trained for 1000 epochs, and models in green crosses are supervised ResNets trained for 90 epochs7 (He et al.', 'shown in Table 1.', 'Stronger color augmentation substan- tially improves the linear evaluation of the learned unsuper- vised models.', 'In this context, AutoAugment (Cubuk et al.', ', 2019), a sophisticated augmentation policy found using su- pervised learning, does not work better than simple cropping + (čer) color distortion.', 'When training supervises mod- els with the same set of augmentations, we observe that stronger color augmentation does not improve or even hurt their performance.', 'Thus, our experiments show that unsu- pervised contrastive learningčefac81ts from stronger (color) data augmentation than supervised learning.', 'Although pre - vious work has reported that data augmentation is useful for self -upervised learning, (Doersch et al.', ', 2015; Bachman et al.', ', 2019; Hc3a9naff et al.', ', 2019; Asano et al.', ', 2019), we show that data augmentation that does not yield accuracy beneath because supervised learning can still help considerably with contrastive learning.', 'Architectures for Encoder and Head 4.', 'Unsupervised contrastive learning beneath (more) from bigger models Figure 7 shows, perhaps unsurprisingly, that increasing depth and width both improve performance.', 'While similar efac81ndings hold for supervised learning (He et al.', ', 2016), we efac81nd the gap between supervised models and linear classiefac81ers trained on unsupervised models shrinks as the model size increases, suggesting that unsupervises learning language more from bigger models than its supervised counterpart.', '7Training longer does not improve supervising ResNets (see Appendix B.', '050100150200250300350400450Number of Parameters (Millions)50556065707580Top 1R101R101(2x)R152R152(2 brown)R18R18(2we)R18(4x)R34R34(2x(R34IVE4x)00050R50(2x.1R50(4x.Sup.', 'R50(2x)Sup.', 'R50(4x)R50thernR50(2x)', 'u )/cf84 v+ e2892(cid: 80) Z(u) (1 e28892 exp(uT v+/cff84 ) ve2842 exp(usT ve2883/cfk4 ) (cf83(e2888222 v+/co884 ))/c ph84 vč e29292 else 0 Zf84(uT ve242/c(c84 )/c84 ve2898824444,000,000,000,00092,0009242 if uT v', 'Negative loss functions and their gradients.', 'All input vectors, i.', 'u, v+, ve28892, i (cid: 96)2 normalized.', 'NT-Xent is an abbreviation for e2809cNormalized', 'Different loss functions impose different weightings of positive and negative examples.', 'What to predict? Color vs grayscale Rotation Orig.', 'vs corrupted Orig.', 'vs Sobel efac81', '6 80 25 50 50 Figure 8.', 'Linear evaluation of representations with different pro- jection heads g(c2b7) and various dimensions of z = g(h).', 'The representation h (before projection) i 2048-dimensional here.', 'A nonlinear projection head improves the representation quality of the layer before it. We then study the importance of including a projection head, i.', 'Figure 8 shows linear evaluation results using three different architecture for the head: (1) identity mapping; (2) linear projection, as used by several previous approaches (Wu et al.', ', 2018); and (3) the default nonlinear projection with one additional hidden layer (and ReLU acti- vation), similar to Bachman et al.', 'We observe that a nonlinear projection is better than a linear projection (+3%), and much better than no projection (>10%).', 'When a pro- jection head is used, similar results are observed regardless of output dimension.', 'Furthermore, even when nonlinear projection is used, the layer before the projection head, h, is still much better (>10%) than the layer above, z = g(h), which shows that the hidden layer above the projection head is a better representation than the layer below.', 'We conjecture that the importance of using the representa- tion before the nonlinear projection is due to loss of informa - tion induced by the contrastive loss.', 'In particular, z = g(h) is trained to be invariant on data transformation.', 'Thus, g can remove information that may be useful for the downstream task, such as the color or orientation of objects.', 'By leverag- ing the nonlinear transformation g(c2b7), more information can be formed and maintained in h.', 'To verify this hypothesis, we conduct experiments that use either h or g(h) to learn to predict the transformation applied during the pretraining.', 'Here we set g(h) = W (2)cf83(W (1)h), with the same input and output dimensionality (i.', 'Table 3 shows h contains much more information about the transformation applied, while g(h) loses information.', 'Further analysis can Table 3.', 'Accuracy of training additional MLPs with different repre- sentations to predict the transformation applied.', 'Other than crop and color augmentation, we additionally and independently add rotation (one of {0e297a6, 90e294a6, 180e299a6, 270e2966}), Gaussian noise, and So- bel efac81ltering transformation during the pretraining of the last three rows.', 'Both h and g(h) are of the same dimensionality, i.', 'be found in Appendix B.', 'Loss Functions and Batch Size 5.', 'Normalized cross entropy loss with adjustable temperature works better than alternatives. We compare the NT-Xent loss against other commonly used contrastive loss functions, such as logistic loss (Mikolov et al.', ', 2013), and margin loss (Schroff et al.', 'Table 2 shows the objective function as well as the gradient to the input of the loss function.', 'Looking at the gradient, we observe 1) (cid: 96)2 normalization (i.', 'cosine similarities) along with temperature effectively, different examples, and an appropriate temperature can help the model learn from hard negatives; and 2) unlike cross-entropy, other objec- tive functions do not weigh negatives by their relative hardness.', 'As a result, one must apply semi -hard negative mining (Schroff et al.', ', 2015) for these loss functions: in- stead of computing the gradient over all loss terms, one can compute the gradient using semi -hard negative terms (i.', '; those that are within the loss margin and closest in distance, but farther than positive examples).', 'To make the comparison fair, we use the same (cid: 96)2 normaliza- tion for all loss functions, and we tune the hyperparameters, and report their best results.', '8 Table 4 shows that, while (semi-hard) negative mining helps, the best result is still much worse than our default NT-Xent loss.', '8Details can be found in Appendix B.', 'For simplicity, we only consider the negatives from one augmentation view.', '326412825651210242048Projection output dimensionality3040506070Top 1ProjectionLinearNon-linearNone0cA Simple Framework for Contrastive Learning of Visual Representations Margin NT-Logi.', 'Margin (sh) NT-Logi.', '(sh) NT-Xent 50.', '6 Table 4.', 'Linear evaluation (top-1) for models trained with different loss functions.', 'e2809cshe2D09d means using semi -hard negative mining.', '(cid: 96)2 norm? Yes No cf84 0.', '5 1 10 100 Entropy Contrastive acc.', '1 Top 1 59.', '0 Table 5.', 'Linear evaluation for models trained with different choices of (cid: 96)2 norm and temperature cf84 for NT-Xent loss.', 'The contrastive distribution is over 4096 examples.', 'Figure 9.', 'Linear evaluation models (ResNet-50) trained with differ- ent batch sizes and epochs.', 'Each bar is a single run from scratch.', '10 We next test the importance of the (cid: 96)2 normalization (i.', 'cosine similarity vs dot product) and temperature cf84 in our default NT-Xent loss.', 'Table 5 shows that without normal- ization and proper temperature scaling, performance is sig- niefac81cantly worse.', 'Without (cid: 96)2 normalization, the contrastive task accuracy is higher, but the resulting representation is worse under linear evaluation.', 'Contrastive learning language (more) from larger batch sizes and longer training Figure 9 shows the impact of batch size when models are trained for different numbers of epochs.', 'We efac81nd that, when the number of training epochs is small (e.', '100 epochs), larger batch sizes have a signiefac81cant advantage over the smaller ones.', 'With more training steps/epochs, the gaps between different batch sizes decrease or disappear, pro- vide the batches are randomly resampled.', 'In contrast to 10A, linear learning rate scaling is used here.', '1 shows using a square root learning rate scaling can improve performance of ones with small batch sizes.', 'Architecture Method Methods using ResNet-50: ResNet -50 Local Agg.', 'ResNet -50 MoCo ResNet-50 Pč ResNetet-50 CPC v2 SimCLR (ours) ResNet', '5 - - 85.', '0 - - 81.', '2 Table 6.', 'ImageNet accuracies of linear classiefac81ers trained on repre- sentations learned from different self -upervised methods.', 'Method Architecture ResNet-50 Supervised baseline Methods using other label-propagation: ResNet -50 P', 'RandAug) ResNet-50 FixMatch (w.', 'RandAug) ResNet-50 S4L (Rot+VAT+En.', '), ResNet-50 (4c397)', '0 - - 39.', '6 Table 7.', 'ImageNet accuracy of models trained with few labels.', 'supervised learning (Goyal et al.', ', 2017), in contrastive learn- ing, larger batch sizes provide more negative examples, facilitating convergence (i.', 'Take fewer epochs and steps for a given accuracy).', 'Training longer also provides more negative examples, improving the results.', 'In Appendix B.', '1, results with even longer training steps are provided.', 'Comparison with State-of-art In this subsection, similar to Kolesnikov et al.', '(2019); He et al.', '(2019), we use ResNet-50 in 3 different hidden layer widths (width multipliers of 1c397, 2c394, and 4c399).', 'For better convergence, our models here are trained for 1000 epochs.', 'Linear evaluation.', 'Table 6 compares our results with previ- ous approaches (Zhuang et al.', ', 2019; He et al.', '; 2019; Misra & van der Maaten, 2019; Hc3a9naff et al.', ', 2019; Kolesnikov et al.', '; 2019; Donahue & Simonyan, 2019; Bachman et al.', ', 1002003004005006007008009001000Training epochs50.', '0Top 1Batch size25651210242048409681920cA Simple Framework for Contrastive Learning of Visual Representations Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers Linear evaluation: SimCLR (ours) 76.', '9 Supervised 75.', '2 Fine-tuned: SimCLR (ours) 89.', '7 Supervised Random init 88.', '5 Table 8.', 'Comparison of transfer learning performance of our self-upervised approach with supervised baselines across 12 natural image classiefac81cation datasets, for ResNet-50 (4c397) models pretrained on ImageNet.', 'Results not signiefac81cantly worse than the best (p > 0.', '05, permutation test) is shown in bold.', 'See Appendix B.', '8 for experimental details and results with standard ResNet-50s.', '2019; Tian et al.', ', 2019) in the linear evaluation setting (see Appendix B.', 'Table 1 shows more numerical compar- isons among different methods.', 'We are able to use standard networks to obtain substantially better results compared to previous methods that require speciefac81cally designed archi- tectures.', 'The best result obtained with our ResNet-50 (4c397) can match the supervised pretrained ResNet -50.', 'Semi-upervised learning.', 'We follow Zhai et al.', '(2019) and sample 1% or 10% of the labeled ILSVRC-12 training datasets in a class-balanced way (e288bc12.', '8 and e288bc128 images per class respectively).', '11 We simply efac81ne-tune the whole base network on the labeled data without regularization (see Appendix B.', 'Table 7 shows the comparison of our results against recent methods (Zhai et al.', ', 2019; Xie et al.', ', 2019; Sohn et al.', ', 2020; Wu et al.', ', 2018; Donahue & Simonyan, 2019; Misra & van der Maaten, 2019 # Hc3a9naff et al.', 'The supervised baseline from (Zhai et al.', ', 2019) is strong due to intensive search of hyper-parameters (including augmentation).', 'Again, our approach signiefac81cantly improves over state-of-theart with both 1% and 10% of the labels.', 'Interestingly, efac81ne-tuning our pretrained ResNet-50 (2c397 ; 4c399) OF full ImageNet are also signiefac81cantly better, then training from scratch (up to 2%, see Appendix B.', 'Transfer learning.', 'We evaluate learning perfor- mance across 12 natural image datasets in both linear evalu- ation (efac81xed feature extractor) and efac81ne-tuning settings.', 'Fol- lowing Kornblith et al.', '(2019), we perform hyperparameter tuning for each model-daset combination and select the best hyperparameters on a validation set.', 'Table 8 shows results with the ResNet-50 (4c397) model.', 'When efac81ne-tuned, our self-upervised model signiefac81cantly outperforms the su- pervised baseline on 5 datasets, whereas the supervised baseline is superior to only 2 (i.', 'Pets and Flowers).', 'In the remaining 5 datasets, the models are statistically tied.', 'Full experimental details as well as results about the standard ResNet-50 architecture are provided in Appendix B.', '11The details of sampling and exact subsets can be found in https: //wwww.', 'tensorefac82ow.', 'org/dasets/catalog/imagenet2012<unk>subset.', 'Related Work The idea of making representations of an image agree with each other under small transformations dates back to Becker & Hinton (1992).', 'We extend it by leveraging recent ad- vances on data augmentation, network architecture and con- trastive losses.', 'A similar consistency idea, but for class label prediction, has been explored in other contexts such as semi - supervised learning (Xie et al.', ', 2019; Berthelot et al.', 'Handcrafted pretext tasks.', 'The recent renaissance of self - supervised learning began with artiefac81cially designed pretext tasks, such as relative patch prediction (Doersch et al.', ', 2015), solving jigsaw puzzles (Noroozi & Favaro, 2016), coloriza- tion (Zhang et al.', ', 2016) and rotation prediction (Gidaris et al.', ', 2018; Chen et al.', 'Although good results can be obtained with bigger networks and longer train - ing (Kolesnikov et al.', ', 2019), these pretext tasks rely on somewhat adhoc heuristics, which limits the generality of learned representations.', 'Contrastive visual representation learning.', 'Dating back to Hadsell et al.', '(2006), these approaches learnčen- tations by contrasting positive pairs against negative pairs.', 'Along these lines, Dosovitskiy et al.', '(2014) proposes to treat each instance as a class represented by a feature vector (in a parametric form).', '(2018) proposes to use a memory bank to store the instance class representation vector, an approach adopted and extended in several recent papers (Zhuang et al.', ', 2019; Tian et al.', ', 2019; He et al.', ', 2019; Misra & van der Maaten, 2019).', 'Other work explores the use of in-batch samples for negative sampling instead of a memory bank (Doersch & Zisserman, 2017; Ye et al.', ', 2019; Ji et al.', 'Recent literature has attempted to relate the success of their methods to maximization of mutual information between latent representations (Oord et al.', ', 2018; Hc3a9naff et al.', ', 2019; Hjelm et al.', ', 2018; Bachman et al.', 'However, it is not clear if the success of contrastive approaches is determined by the mutual information, or by the speciefac81c form of the contrastive loss (Tschannen et al.', '0cA Simple Framework for Contrastive Learning of Visual Representations We note that almost all individual components of our frame- work have appeared in previous works, although the speciefac81c instantiations may be different.', 'The superiority of our frame - work relative to previous work is not explained by any single design choice, but by their composition.', 'We provide a com- prehensive comparison of our design choices with those of previous work in Appendix C.', 'Unfortunately, in this work, we present a simple framework and its in- stantiation for contrastive visual representation learning.', 'We carefully study its components, and show the effects of different design choices.', 'By combining our efac81ndings, we improve considerably over previous methods for self - supervising, self -upervising, and transfer learning.', 'Our approach differs from standard supervised learning on ImageNet only in the choice of data augmentation, the use of a nonlinear head at the end of the network, and the loss of func- tion.', 'The strength of this simple framework suggests that, despite a recent surge in interest, self -upervised learning remains undervalued.', 'Ačledgements We would like to thank Xiaohua Zhai, Rafael Mc3bcller and Yani Ioannou for their feedback on the draft.', 'We are also grateful for general support from Google Research teams in Toronto and elsewhere.', 'References Asano, Y.', ', Rupprecht, C.', ', and Vedaldi, A.', 'A critical analysis of self -upervision, or what we can learn from a single image.', 'arXiv preprint arXiv: 1904.', '13132, 2019.', 'Bachman, P.', ', Hjelm, R.', ', and Buchwalter, W.', 'Learning rep- resentations by maximizing mutual information across views.', 'In Advances in Neural Information Processing Systems, pp.', '15509e2809315519, 2019.', 'Becker, S.', 'and Hinton, G.', 'Self-organizing neural network that discovers surfaces in random-dot stereograms.', 'Nature, 355 (6356): 161e28093163, 1992.', ', Alexander, M.', ', Jacobs, D.', ', and Belhumeur, P.', 'Birdsnap: Large-scale efac81ne-grained visual categorization of birds.', 'In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.', '2019e280932026.', 'IEEE, 2014.', 'Berthelot, D.', ', Carlini, N.', '; Goodfellow, I am.', ', Papernot, N.', ', Oliver, A.', ', and Raffel, C.', 'Mixmatch: A holistic approach to semi - supervised learning.', 'In Advances in Neural Information Pro-therning Systems, pp.', '5050e280935060, 2019.', 'Bossard, L.', ', Guillaumin, M.', ', and Van Gool, L.', 'Food-101e28093mining discriminative components with random forests.', 'In European conference on computer vision, pp.', '446e28093461.', 'Springer, 2014.', ', and Hong, L.', 'On sampling strategies for neural networks-based collaborative efac81ltering.', 'In Proceed-\"ings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.', '767e28093776, 2017.', ', Ritter, M.', ', Lucic, M.', ', and Houlsby, N.', 'Self - supervised gans via auxiliary rotation loss.', 'In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.', '12154e2809312163, 2019.', 'Cimpoi, M.', ', Kokkinos, I.', ', Mohamed, S.', ', and Vedaldi, A.', 'Describing textures in the wild.', 'In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.', '3606e28093 3613.', 'IEEE, 2014.', 'Cubuk, E.', ', Vasudevan, V.', ', and Le, Q.', 'Autoaugment: Learning augmentation strategies from data.', 'In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.', '113e28093123, 2019.', 'DeVries, T.', 'and Taylor, G.', 'Improved regularization of convolutional neural networks with cutouts.', 'arXiv preprint arXiv: 1708.', '04552, 2017.', 'Doersch, C.', 'and Zisserman, A.', 'Multi-task self-upervised visual learning.', 'In Proceedings of the IEEE International Conference on Computer Vision, pp.', '2051e280932060, 2017.', 'Doersch, C.', ', Gupta, A.', ', and Efros, A.', 'Unsupervised visual representation learning by context prediction.', 'In Proceedings of the IEEE International Conference on Computer Vision, pp.', '1422e280931430, 2015.', 'Donahue, J.', 'and Simonyan, K.', 'Large scale adversarial representa- tion learning.', 'In Advances in Neural Information Processing Systems, pp.', '10541e2809310551, 2019.', 'Donahue, J.', ', Vinyals, O.', ', Hoffman, J.', ', Zhang, N.', ', Tzeng, E.', ', and Darrell, T.', 'Decaves: A deep convolutional activation feature for generic visual recognition.', 'In International Conference on Machine Learning, pp.', '647e28093655, 2014.', 'Dosovitskiy, A.', ', Springenberg, J.', ', Riedmiller, M.', ', and Brox, T.', 'Discriminative unsupervised features learning about convolutional neural networks.', 'In Advances in neural information processing systems, pp.', '766e28093774, 2014.', 'Everingham, M.', ', Van Gool, L.', ', Williams, C.', ', and Zisserman, A.', 'The pascal visual object classes (', 'International Journal of Computer Vision, 88(2): 303e28093338, 2010.', 'Fei-Fei, L.', ', Fergus, R.', ', and Perona, P.', 'Learning generative visual models from a few training examples: An incremental bayesian approach tested on 101 object categories.', 'In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop on Generative-Model Based Vision, 2004.', 'Gidaris, S.', ', Singh, P.', ', and Komodakis, N.', 'Unsupervisedčen- tation learning by predicting image rotation.', 'arXiv preprint arXiv: 1803.', '07728, 2018.', 'Goodfellow, I am.', ', Pouget-Abadie, J.', ', Mirza, M.', ', Warde- Farley, D.', ', Ozair, S.', ', Courville, A.', ', and Bengio, Y.', 'Generative adversarial nets.', 'In Advances in neural information processing systems, pp.', '2672e280932680, 2014.', '0cA Simple Framework for Contrastive Learning of Visual Representations Goyal, P.', ', Dollc3a1r, P.', ', Girshick, R.', ', Noordhuis, P.', ', Wesolowski, L.', ', Kyrola, A.', ', Tulloch, A.', ', and He, K.', 'Accurate, large minibatch sgd: Training imagenet in 1 hour.', 'arXiv preprint arXiv: 1706.', '02677, 2017.', 'Hadsell, R.', ', Chopra, S.', ', and LeCun, Y.', 'Dimensionality reduction by learning an invariant mapping.', 'In 2006, IEEE Computer So- ciety Conference on Computer Vision and Pattern Recognition (CVPRe2809906), volume 2, pp.', '1735e280931742.', 'IEEE, 2006.', ', Zhang, X.', ', and Sun, J.', 'Deep residual learning for image recognition.', 'In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.', '770e28093778, 2016.', ', and Girshick, R.', 'Momentum contrast to unsupervised visual representation learning.', 'arXiv preprint arXiv: 1911.', '05722, 2019.', 'Hc3a9naff, O.', ', Razavi, A.', ', Doersch, C.', ', Eslami, S.', ', and Oord, A.', 'Data-efefac81cient image recognition with contrastive predictive coding.', 'arXiv preprint arXiv: 1905.', '09272, 2019.', 'Hinton, G.', ', Osindero, S.', ', and Teh, Y.', 'A fast learning al- gorithm for deep belief nets.', 'Neural computation, 18(7): 1527e28093 1554, 2006.', 'Hjelm, R.', ', Fedorov, A.', ', Lavoie-Marchildon, S.', ',čal, K.', ', Bachman, P.', ', Trischler, A.', ', and Bengio, Y.', 'Learning deep repre- sentations by mutual information estimation and maximization.', 'arXiv preprint arXiv: 1808.', '06670, 2018.', 'Howard, A.', 'Some improvements on deep convolutional neural networks based image classiefac81cation.', 'arXiv preprint arXiv: 1312.', '5402, 2013.', 'Ioffe, S.', 'and Szegedy, C.', 'Batch normalization: Accelerating deep network training by reducing internal covariate shift.', 'arXiv preprint arXiv: 1502.', '03167, 2015.', ', Henriques, J.', ', and Vedaldi, A.', 'Invariant information clustering for unsupervised images, classiefac81cation and segmenta- tion.', 'In Proceedings of the IEEE International Conference on Computer Vision, pp.', '9865e280939874, 2019.', 'Kingma, D.', 'and Welling, M.', 'Auto-encoding variational bayes.', 'arXiv preprint arXiv: 1312.', '6114, 2013.', 'Kolesnikov, A.', ', and Beyer, L.', 'Revisiting self -upervised In Proceedings of the IEEE visual representation learning.', 'conference on Computer Vision and Pattern Recognition, pp.', '1920e280931929, 2019.', 'Kornblith, S.', ', Shlens, J.', ', and Le, Q.', 'Do you better ImageNet models In Proceedings of the IEEE conference on transfer better? computer vision and pattern recognition, pp.', '2661e280932671, 2019.', 'Krause, J.', ', Stark, M.', ', and Fei-Fei, L.', 'Collecting a large-scale dataset ofad81ne-grained cars.', 'At Second Workshop on Fine-Grained Visual Categorization, 2013.', 'Krizhevsky, A.', 'and Hinton, G.', 'Learning multiple layers of features from tiny images.', 'Technical report, University of Toronto, 2009.', 'URL https: //wwww.', 'edu/~kriz/ learning-feature-2009-TR.', 'Loshchilov, I.', 'and Hutter, F.', 'Sgdr: Stochastic gradient descent with warm restarts.', 'arXiv preprint arXiv: 1608.', '03983, 2016.', 'Maaten, L.', 'and Hinton, G.', 'Visualizing data using t-ne.', 'Jour- nal of machine learning research, 9(Nov): 2579e28093205, 2008.', ', Kannala, J.', ', Rahtu, E.', ', Blaschko, M.', ', and Vedaldi, A.', 'Fine-grained visual classiefac81cation of aircraft.', 'Technical report, 2013.', 'Mikolov, T.', ', Corrado, G.', ', and Dean, J.', 'Efefac81cient esti- mation of word representations in vector space.', 'arXiv preprint arXiv: 1301.', '3781, 2013.', 'Misra, I.', 'and van der Maaten, L.', 'ing of pretext-invariant representations.', 'arXiv: 1912.', '01991, 2019.', 'Self-upervised learn- arXiv preprint Nilsback, M.', 'and Zisserman, A.', 'Automated efac82ower classiefac81cation over a large number of classes.', 'In Computer Vision, Graphics & Image Processing, 2008.', 'ICVGIPe2809908.', 'Sixth Indian Conference on, pp.', '722e28093729.', 'IEEE, 2008.', 'Noroozi, M.', 'and Favaro, P.', 'Unsupervised learning of visual repre- sentations by solving jigsaw puzzles.', 'In European Conference on Computer Vision, pp.', '69e2809384.', 'Springer, 2016.', ', and Vinyals, O.', 'Representation learning with contrastive predictive coding.', 'arXiv preprint arXiv: 1807.', '03748, 2018.', 'Parkhi, O.', ', Vedaldi, A.', ', Zisserman, A.', ', and Jawahar, C.', 'Cats and dogs.', 'In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.', '3498e280933505.', 'IEEE, 2012.', 'Russakovsky, O.', ', Krause, J.', ', Satheesh, S.', ', Huang, Z.', ', Karpathy, A.', ', Khosla, A.', ', Bernstein, M.', 'Imagenet large scale visual recognition challenge.', 'International journal of computer vision, 115(3): 211e28093252, 2015.', 'Schroff, F.', ', Kalenichenko, D.', ', and Philbin, J.', 'Facenet: A uniefac81 In Proceed- embedding for face recognition and clustering.', '\"ings of the IEE conference on computer vision and pattern recognition, pp.', '815e28093823, 2015.', 'Simonyan, K.', 'and Zisserman, A.', 'Very deep convolutional networks for large-scale image recognition.', 'arXiv preprint arXiv: 1409.', '1556, 2014.', 'Improved deep metric learning with multi-class n-pair loss objectives.', 'In Advances in neural information processing systems, pp.', '1857e280931865, 2016.', ', Berthelot, D.', ', Zhang, Z.', ', Carlini, N.', ', Cubuk, E.', ', Kurakin, A.', ', Zhang, H.', ', and Raffel, C.', 'Fixmatch: Simpli- fying semi-upervised learning with consistency and conefac81dence.', 'arXiv preprint arXiv: 2001.', '07685, 2020.', 'Szegedy, C.', ', Sermanet, P.', ', Anguelov, D.', ', Erhan, D.', ', Vanhoucke, V.', ', and Rabinovich, A.', 'Going deeper with convolutions.', 'In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.', '1e280939, 2015.', ', Krishnan, D.', ', and Isola, P.', 'Contrastive multiview coding.', 'arXiv preprint arXiv: 1906.', '05849, 2019.', 'Krizhevsky, A.', ', Sutskever, I.', ', and Hinton, G.', 'Imagenet classiefac81- cation with deep convolutional neural networks.', 'In Advances in neural information processing systems, pp.', '1097e280931105, 2012.', 'Tschannen, M.', ', Djolonga, J.', ', Rubenstein, P.', ', Gelly, S.', ', and Lu- cic, M.', 'On mutual information maximization for representation, learning.', 'arXiv preprint arXiv: 1907.', '13625, 2019.', '0cA Simple Framework for Contrastive Learning of Visual Representations Wu, Z.', ', Xiong, Y.', ', and Lin, D.', 'Unsupervised features learning via non -parametric instance discrimination.', 'In Proceed-\"iEEE Conference on Computer Vision and Pattern Recognition, pp.', '3733e280933742, 2018.', ', Ehinger, K.', ', Oliva, A.', ', and Torralba, A.', 'Sun database: Large-scale scene recognition from abbey to zoo.', 'In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.', '3485e28093392.', 'IEEE, 2010.', ', Luong, M.', ', and Le, Q.', 'Unsu- pervised data augmentation.', 'arXiv preprint arXiv: 1904.', '12848, 2019.', ', Zhang, X.', ', and Chang, S.', 'Unsupervised embedding, learning via invariant and spreading instance features.', 'In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.', '6210e280936219, 2019.', ', Gitman, I.', ', and Ginsburg, B.', 'Large batch training of con- volutional networks.', 'arXiv preprint arXiv: 1708.', '03888, 2017.', ', Oliver, A.', ', Kolesnikov, A.', ', and Beyer, L.', 'S4l: Self - supervised semi -upervised learning.', 'In the IEEE International Conference on Computer Vision (ICCV), October 2019.', 'Zhang, R.', ', Isola, P.', ', and Efros, A.', 'Colorful image coloriza - tion.', 'In European conference on computer vision, pp.', '649e28093666.', 'Springer, 2016.', 'Zhuang, C.', ', and Yamins, D.', 'Local aggregation for unsupervised learning of visual embeddings.', 'In Proceedings of the IEEE International Conference on Computer Vision, pp.', '6002e280936012, 2019.', '0cA Simple Framework for Contrastive Learning of Visual Representations A.', 'Data Augmentation Details In our default pretraining setting (which is used to train our best models), we utilize random crops (with resize and random efac82ip), random color distortion, and random Gaussian blur as the data augmentations.', 'The details of these three augmentations are provided below.', 'Random crops and resize 224x224 We use standard Inception-style random cropping (Szegedy et al.', 'The crop of random size (uniform from 0.', '0 in area) of the original size and a random aspect ratio (default: of 3', 'This crop is efac81nally resized to the original size.', 'This has been imple- mented in Tensorefac82ow as e2809cslim.', 'preprocessing.', 'inception<unk>preprocessing.', 'distorted<unk>bounding<unk>box<unk>crope2809d, or in Pytorch as e2805ctorchvision.', 'transforms.', 'RandomResizedCrope2809d.', 'Additionally, the random crop (with resize) is always followed by a random horizontal/left-to-right efac82ip with 50% probability.', 'This is helpful but not essential.', 'By removing this from our default augmentation policy, the top-1 linear evaluation drops from 64.', '4% of our ResNet-50 models trained in 100 epochs.', 'Color distortion and distorortion is composed of color jittering and color dropping.', 'We efac81nd stronger color jittering usually helps, so we set a strength parameter.', 'A pseudo-code for color distortion using TensorFlow is as follows.', 'import tensorflow as tf def color<unk>distortion(image, s=1.', '0): # image is a tensor with value range in [0, 1].', '# s is the strength of color distortion.', 'def color<unk>jitter(x): # one can also shuffle the order of following augmentations # each time they are applied.', 'random<unk>brightness(x, max<unk>delta=0.', '8therns) x = tf.', 'random<unk>contrast(x, lower=1-0.', '8therns, upper=1+0.', '8therns) x = tf.', 'random<unk>saturation(x, lower=1-0.', '8therns, upper=1+0.', '8therns) x = tf.', 'random<unk>hue(x, max<unk>delta=0.', '2therns) x = tf.', 'clip<unk>by<unk>value(x, 0, 1) return x def color<unk>drop(x): image = tf.', 'rgb<unk>to<unk>grayscale(image) image = tf.', 'tile(image, [1, 1, 3]) # randomly applied transformation with probability p.', 'image = random<unk>apply(color<unk>jitter, image, p=0.', '8) image = random<unk>apply(color<unk>drop, image, p=0.', '2) return image A pseudo-code for color distortion using Pytorch is as follows 12.', 'from torchvision import transforms def get<unk>color<unk>distortion(=1.', '0): # s is the strength of color distortion.', 'color<unk>jitter = transforms.', 'ColorJitter(0.', '2therns) rnd<unk>color<unk>jitter = transforms.', 'RandomApply([color<unk>jitter], p=0.', '8) rnd<unk>gray = transforms.', 'RandomGrayscale(p=0.', '2) color<unk>distort = transforms.', 'Compose([ rnd<unk>color<unk>jitter, rnd<unk>gray]) 12Our code and results are based on Tensorefac82ow. The Pytorch code here is a reference.', '0cA Simple Framework for Contrastive Learning of Visual Representations return color<unk>distort Gaussian blur This augmentation is in our default policy.', 'We efac81nd it helpful, as it improves our ResNet-50 trained for 100 epochs from 63.', 'We blur the image 50% of the time using a Gaussian kernel.', 'We randomly sample cf83 e2888 [0.', '0], and the kernel size is set to be 10% of the image height/width.', 'Additional Experimental Results B.', 'Batch Size and Training Steps Figure B.', '1 shows the top-5 accuracy on linear evaluation when trained with different batch sizes and training epochs.', 'The conclusion is very similar to top-1 accuracy shown before, except that the differences between different batch sizes and training steps seem slightly smaller here.', 'In both Figure 9 and Figure B.', '1, we use a linear scaling of learning rates similar to (Goyal et al.', ', 2017) when training with different batch sizes.', 'Although linear learning rate scaling is popular with SGD/Momentum optimizer, we efac81nd a square root learning rate, scaling is more desirable with', 'With square root learning rate scaling, we have BatchSize, instead of LearningRate = 0.', '3 c397 BatchSize/256 in the linear scaling case, but the learning rate is the same under both scaling methods when batch size of 4096 (our default batch size).', 'A comparison is presented in Table B.', '1, where we observe that square root learning rate scaling improves the performance for models trained in small batch sizes and in smaller numbers of epochs.', 'LearningRate = 0.', '075 c397 e2889a Batch sizečč Epochs 100 256 512 1024 2048 4096 8192 57.', '8 200 61.', '0 400 64.', '3 800 66.', '1 Table B.', 'Linear evaluation (top-1) under different batch sizes and training epochs.', 'On the left side of the slash sign are models trained with linear LR scaling, and on the right are models trained in square root LR scalings.', 'The result is bold if it is more than 0.', 'Square root LR scaling works better for smaller batch sizes trained in fewer epochs (', 'We also train with larger batch size (up to 32K) and longer (up to 2700 epochs), with the square root learning rate scaling.', 'A shown in Figure B.', '2, the performance seems to saturate with a batch size of 8192, while training longer can still signiefac81cantly improve the performance.', 'Figure B.', 'Linear evaluation (top-5) of ResNet-50 trained with different batch sizes and epochs.', 'Each bar is a single run from scratch.', 'See Figure 9 for top -1 accuracy.', 'Figure B.', 'Linear evaluation (top-1) of ResNet-50 trained with different batch sizes and longer epochs.', 'Here, a square root learn- ing rate, instead of a linear one, is utilized.', '1002003004005006007008009001000Training epochs70.', '0Top 5Batch size25651210220448409681925010020040080016003200Training epochs6062646687072Top 1Batch size1262242048468198193838427680cAč Framework for Contrastive Learning of Visual Representations B.', 'Broader composition of data augmentations further improves performance. Our best results in the main text (Table 6 and 7) can be further improved when expanding the default augmentation policy to include the following: (1) Sobel efac81ltering, (2) additional color distortion (equalized, solarize), and (3) motion blur.', 'For linear evaluation, the protocol, the ResNet-50 models (1c397, 2c337, 4c399) trained with broader data augmentations achieved 70.', '3), respectively.', '2 shows ImageNet accuracy obtained by efac81ne-tuning the SimCLR model (see Appendix B.', '5 for the details of efac81ne-tuning procedure).', 'Interestingly, when efac81ne-tuned on full (100%) ImageNet training set, our ResNet (4c397) model achieves 80.', '4% top-1 / 95.', '4% top-5 13, which is signiefac81cantly better than that (78.', '4% top-1 / 94.', '2% top-5) of training from scratch using the same set of augmentations (i.', 'random crops and horizontal efac82ip).', 'For ResNet-50 (2c397), efac81ne-tuning our pre-trained ResNet -50 (2w.97) is also better than training from scratch (77.', '8% top-1 / 93.', 'There is no improvement from efac81ne-tuning to ResNet-50.', 'Architecture ResNet-50 ResNetet-50 (2c397) ResNet -50 (4c347) 1% Label fraction 10% 100% Top 1 Top 5 Top 1 Top 19 Top 5 Top 5 49.', '8 Table B.', 'Classiefac81cation accuracy obtained by efac81ne-tuning the SimCLR (which is pretrained with broader data augmentations) at 1%, 10% and full of ImageNet.', 'As a reference, our ResNet-50 (4c397) trained from scratch on 100% labels achieves 78.', '4% top-1 / 94.', 'Effects of Longer Training for Supervised Models Here we perform experiments to see how training steps and stronger data augmentation affect supervised training.', 'We test ResNet -50 and ResNet-50 (4c397) under the same set of data augmentations (random crops, color distortion, 50% Gaussian blur) as used in our unsupervised models.', '3 shows the top-1 accuracy.', 'We observe that there is no signiefac81cant language from training supervised models longer on ImageNet.', 'Stronger data augmentation slightly improves the accuracy of ResNet-50 (4c397) but does not help with ResNet -50.', 'When stronger data augmentation is applied, ResNet-50 generally requires longer training (e.', '500 epochs 14) to obtain the optimal result, while ResNet-50 (4c397) does not', 'Model Training epochs ResNet-50 ResNet -50 (4c397) 90 500 1000 90 500 1000 Top 1 +Color 75.', '2 Crop 76.', '9 +Color+Blur 75.', '3 Table B.', 'Top-1 accuracy of supervised models trained longer under various data augmentation procedures ( Unfortunately the same set of data augmentations for contrastive learning).', 'Understanding The Non-Linear Projection Head Figure B.', '3 shows the eigenvalue distribution of linear projection matrix W e28888 R2048c3972048 used to compute z = W h.', 'This matrix has relatively few large eigenvalues, indicating that it is approximately low-rank.', '4 shows t-SNE (Maaten & Hinton, 2008) visualizations of h and z = g(h) for randomly selected 10 classes by our best ResNet-50 (top-1 linear evaluation 69.', 'Classes represented by h are better separated compared to z.', '13It is 80.', '1% top-1 / 95.', '2% top-5 without broader augmentations for pretraining SimCLR.', '14With AutoAugment (Cubuk et al.', ', 2019), optimal test accuracy can be achieved between 900 and 500 epochs.', '0cA Simple Framework for Contrastive Learning of Visual Representations (a) Y-axis in uniform scale.', '(b) Y-axis in log scale.', 'Figure B.', 'Squared real eigenvalue distribution of linear projection matrix W e28888 R2048c3972048 used to compute g(h) = W h.', '(a) h (b) z = g(h) Figure B.', 't-SNE visualization of hidden vectors of images from a randomly selected 10 classes in the validation set.', 'Semi-upervised Learning via Fine-Tuning Fine-tuning Procedure We efac81ne-tune using the Nesterov momentum optimizer with a batch size of 4096, momentum of 0.', '9, and a learning rate of 0.', '8 (following LearningRate = 0.', '05c397 BatchSize/256) without warmup.', 'Only random cropping (with random left-to-right efac82ipping and resizing to 224x24) is used for preprocessing.', 'We do not use any regularization (including weight decay).', 'For 1% labeled data, we efac81ne-tune for 60 epochs, and for 10% labelled data, weising for 30 epochs.', 'For the inference, we resize the given image to 256x256, and take a single center crop of 224x224.', '4 shows the comparison of top-1 accuracy to different methods of semi-upervised learning.', 'Our models signiefac81cantly improve state-of-the-art.', 'Method Architecture Label fraction 1% 10% Top 1 25.', '4 ResNet -50 ResNet-50 ResNets-50 Supervised baseline Methods using label-propagation: UDA (w.', 'RandAug) FixMatch (w.', 'RandAug) S4L (Rot+VAT+Ent.', \"), ResNet-50 (4c397) Methods using self -upervised representation learning only: CPC v2 SimCLR (ours) SimCLR's (ours), SimCLR - (ours) ResNet -161(e28897) ResNets-50,000,000 ResNetp50 (2c3947) ResVet-50 - (4c997) 52.\", '0 - - 56.', '4 Table B.', 'ImageNet top-1 accuracy of models trained with few labels.', 'See Table 7 for top -5 accuracy.', 'Linear Evaluation For linear evaluation, we follow similar procedures as efac81ne-tuning (described in Appendix B.', '5), except that a larger learning rate of 1.', '6 (following LearningRate = 0.', '1 c397 BatchSize/256) and longer training of 90 epochs.', 'Alternatively, using language optimizer with the pretraining hyper-parameters also yields similar results.', 'Furthermore, we, efac81nd that attach the linear classiefac81er on top of the base encoder (with a stop<unk>gradient on the input to linear - classiefack1er to prevent the label information from inefac82uencing the encoder) and training them simultaneously during the pretraining achieves similar performance.', 'Correlation Between Linear Evaluation and Fine-Tuning Here we study the correlation between linear evaluation and efac81ne-tuning under different settings of training step and network architecture.', '5 shows linear evaluation versus efac81ne-tuning when training epochs of a ResNet-50 (using batch size of 4096) are varied from 50 to 3200 as in Figure B.', 'While they are almost linearly correlated, it seems', 'Figure B.', 'Top-1 accuracy of models trained in different epochs (', '2), under linear evaluation and efac81ne-tuning.', 'Figure B.', '6 shows linear evaluation versus efac81ne-tuning for different architectures of choice.', 'Figure B.', 'Top-1 accuracy of different architectures under linear evaluation and efac81ne-tuning.', 'Transfer Learning We evaluated the performance of our self-upervised representations for transfer learning in two settings: linear evaluation, where a logistic regression classiefac81er is trained to classify a new dataset based on the self -upervised representation learned on ImageNet, and efac81ne-tuning, where we allow all weights to vary during training.', 'In both cases, we follow the approach described by Kornblith et al.', '(2019), although our preprocessing differs slightly.', 'METHODS Datasets We investigated transfer learning performance on the Food-101 dataset (Bossard et al.', ', 2014), CIFAR -10 and CIFAR-100 (Krizhevsky & Hinton, 2009), Birdsnap (Berg et al.', ', 2014), the SUN397 scene dataset (Xiao et al.', ', 2010), Stanford Cars (Krause et al.', ', 2013), FGVC Aircraft (Maji et al.', ', 2013), the PASCAL VOC 2007 classiefac81cation task (Everingham et al.', ', 2010), the Describable Textures Dataset (DTD) (Cimpoi et al.', ', 2014), Oxford-IIIT Pets (Parkhi et al.', ', 2012), Caltech-101 (Fei-Fei et al.', ', 2004), and Oxford 102 Flowers (Nilsback & Zisserman, 2008).', 'We follow the evaluation protocols in the papers introducing these datasets, i.', '; we report top-1 accuracy for Food-101, CIFAR-10, CinthAR-100, Birdsnap, SUN397, Stanford Cars, and DTD; mean per-class accuracy for FGVC Aircraft, Oxford-IIIT Pets, Caltech-101 ; and Oxford 102 Flowers; and the 11-point mAP metric as deefac81ned in Everingham et al.', '(2010) OF PASCAL VOC 2007.', 'For DTD and SUN397, the dataset creators deefac81 multiple trains/test splits; we report results only for the efac81rst split.', 'Caltech-101 deefac81nes no train/test split, so we randomly chose 30 images per class and tested on the remainder, for fair comparison with previous work (Donahue et al.', ', 2014; Simonyan & Zisserman, 2014).', 'We used the validation sets speciefac81ed by the dataset creators to select hyperparameters for FGVC Aircraft, PASCAL VOC 626466680Linear eval35.', '0Fine-tuning on 1%6264666870Linear eval6062666Fine-Tuning on 10%5055606570Linesar eval303336394245485154inetune (1%)Width1x2x4xDepth1834501011525056006500Linears eval5154560D, and Oxford 10269Finetune (10%)Widsth1x4x4x918341015202020cA', 'For other datasets, we held out a subset of the training set for validation while performing hyperparameter tuning.', 'After selecting the optimal hyperparameters on the validation set, we retrained the model using the selected parameters using all training and validation images.', 'We report accuracy on the test set.', 'Transfer Learning via a Linear Classiefac81er We trained an (cid: 96)2-regularized multinomial logistic regression classiefac82er on features extracted from the frozen pretrained network.', 'We used L-BFGS to optimize the softmax cross-entropy objectives and we did not apply data augmentation.', 'As preprocessing, all images were resized on 224 pixels along the shorter side using bicubic resampling, after which we took a 224 c397 224 center crop.', 'We selected the (cid: 96)2 regularization parameter from a range of 45 logarithmically spaced values between 10e288926 and 105.', 'Transfer Learning via Fine-Tuning We efac81ne-tuned the entire network using the weight of the pretrained network as initialization.', 'We trained for 20, 000 steps at a batch size of 256 using SGD with Nesterov momentum with a momentum parameter of 0.', 'We set the momentum parameter for the batch normalization statistics to max(1 e28892 10/, 0.', '9) where s is the number of steps per epoch?', 'As data augmentation during efac81ne-tuning, we performed only random crops with resize and efac82ips; in contrast to pretraining, we did not perform color augmentation or blurring.', 'At test time, we resized images of 256 pixels along the shorter side and took a 224 c397 224 center crop.', '(Additional accuracy improvements may be possible with further optimization of data augmentation, particularly on the CIFAR-10 and CIFAR -100 datasets.', '), We selected the learning rate and weight decay, with a grid of 7 logarithmically spaced learning rates between 0.', '0001 and 0.', '1 and 7 logarithmically spaced values of weight decay between 10e288926 and 10e2D8923, as well as no weight decay.', 'We divide these values of weight decay by the learning rate.', 'Training from Random Initialization, We trained the network from random initialization using the same procedure as for efac81ne-tuning, but for longer, and with an altered hyperparameter grid.', 'We chose hyperparameters from a grid of 7 logarithmically spaced learning rates between 0.', '0 and 8 logarithmically spaced values of weight decay between 10e288925 and 10e2D8921.', 'Importantly, our random initialization baselines are trained to 40 and 000 steps, which is sufefac81ciently long to achieve near-maximal accuracy, as demonstrated in Figure 8 of Kornblith et al.', 'On Birdsnap, there are no statistically signiefac81cant differences among methods, and on Food-101, Stanford Cars, and FGVC Aircraft datasets, efac81ne-tuning provides only a small advantage over training from random initialization.', 'However, on the remaining 8 datasets, pretraining has clear advantages.', 'Supervised Baselines We compare against architecturally identical ResNet models trained on ImageNet with standard cross-entropy loss.', 'These models are trained with the same data augmentation as our self -upervised models (crops, strong color augmentation, and blur) and are also trained in 1000 epochs.', 'We found that, although stronger data augmentation and longer training time do notčefac81t accuracy on ImageNet, these models performed signiefac81cantly better than a supervised baseline trained for 90 epochs and ordinary data augmentation for linear evaluation on a subset of transfer datasets.', 'The supervised ResNet-50 baseline achieves 76.', '3% top-1 accuracy on ImageNet, vs.', '3% for the self -upervised counterpart, while the ResNet-50 (4c397) baseline achieves 78.', '5% of the self -upervised model.', 'Statistical Signiefac81cance Testing We test for the signiefac82cance of differences between models with a permutation test.', 'Given predictions of two models, we generate 100, 000 samples from the null distribution by randomly exchanging predictions for each example and computing the difference in accuracy after performing this randomization.', 'We then compute the percentage of samples from the null distribution that are more extreme than the observed difference in predictions.', 'For top-1 accuracy, this procedure yields the same result as the exact McNemar test.', 'The assumption of exchangeability under the null hypothesis is also valid for mean per-class accuracy, but not when computing average precision curves.', 'Thus, we perform signiefac81cance testing for a difference in accuracy on VOC 2007 rather than a difference in mAP.', 'A caveat of this procedure is that it does not consider run-to-run variability when training the models, only variability arising from using a efac81nite sample of images for evaluation.', 'RES ruTS WITH ST', 'With the narrower ResNet-50 architecture, however, supervised learning maintains a clear advantage over self -upervised learning.', 'The supervised ResNet-50 model outperforms the self-upervised model on all datasets with linear evaluation, and most (10 of 12) dataset with efac81ne-tuning.', 'The weaker performance of the ResNet model compared to the ResNets (4c397) 0cA Simple Framework for Contrastive Learning of Visual Representations Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft', '3 Supervised Fine-tuned: SimCLR (ours) 88.', '3 Supervised Random init 86.', '0 Table B.', 'Comparison of transfer learning performance of our self-upervised approach with supervised baselines across 12 natural image datasets, using ImageNet-pretrained ResNet models.', 'See also Figure 8 for results with the ResNet (4c397) architecture.', 'The model may relate to the accuracy gap between the supervised and self-upervised models on ImageNet.', 'The self -upervised ResNet gets 69.', '3% top-1 accuracy, 6.', '8% worse than the supervised model in absolute terms, whereas the self -upervised ResNet (4c397) model gets 76.', '5%, which is only 1.', '8% worse than the supervised model.', 'CIFAR-10 While we focus on using ImageNet as the main dataset for pretraining our unsupervised model, our method also works with other datasets.', 'We demonstrate it by testing on CIFAR-10 as follows.', 'Setup As our goal is not to optimize CIFAR-10 performance, but rather to provide further conefac81rmation of our observations on ImageNet, we use the same architecture (ResNet-50) for CIFAR -10 experiments.', 'Because CIFAR-10 images are much smaller than ImageNet images, we replace the efac81rst 7x7 Conv of stride 2 with 3x3 Conv in stride 1, and also remove theising 131rst max pooling operation.', 'For data augmentation, we use the same Inception crop (efac82ip and resize to 32x32) as ImageNet, 15 and color distortion (strength=0.', '5), leaving out Gaussian blur.', 'We pretrain with learning rate in {0.', '5}, temperature in {0.', '0}, and batch size in {256, 512, 1024, 2048, 4096}.', 'The rest of the settings (including optimizer, weight decay, etc.', '), are the same as our ImageNet training.', 'Our best model trained with batch size 1024 can achieve a linear evaluation accuracy of 94.', '0%, compared to 95.', '1% of the supervised baselines using the same architecture and batch size.', 'The best self -upervised model that reports linear evaluation resulting in CIFAR-10 is AMDIM (Bachman et al.', ', 2019), which achieves 91.', '2% with a model 25c397 larger than ours.', 'We note that our model can be improved by incorporating extra data augmentations as well as using a more suitable base network.', 'Performance under different batch sizes and training steps, Figure B.', '7 shows the linear evaluation performance under different batch sizes and training steps.', 'The results are consistent with our observations on ImageNet, although the largest batch size of 4096 seems to cause a small degradation in performance on CIFAR-10.', 'Figure B.', 'Linear evaluation of ResNet-50 (with ad- just stem) trained with different batch sizes and epochs on CIFAR-10 dataset.', 'Each bar is averaged over 3 runs at different learning rates (0.', '5) and temperature cf84 = 0.', 'Error bar denotes standard deviation.', '15It is worth noting that, although CIFAR-10 images are much smaller than ImageNet images and image size does not differ among examples, cropping with resizing is still a very effective augmentation for contrastive learning.', '1002003004005006007008009001000Training epochs808284868909294Top 1Batch size256512102204840960cA', '8 shows the linear evaluation of models trained in three different temperatures under various batch sizes.', 'We efac81nd that when training to convergence (e.', 'training epochs > 300), the optimal temperature in {0.', '5 and seems consistent regardless of the batch sizes.', 'However, the performance with cf84 = 0.', '1 improves as batch size increases, which may suggest a small shift in optimal temperature towards 0.', '(a) Training eternal e289a4 300 (b) Training epochs > 300 Figure B.', 'Linear evaluation of the model (ResNet-50) trained with three temperatures on different batch sizes in CIFAR-10.', 'Each bar is averaged over multiple runs with different learning rates and total train epochs.', 'Error bar denotes standard deviation.', 'Tuning For Other Loss Functions, the learning rate that works best for NT-Xent losses may not be a good learning rate for other loss functions.', 'To ensure a fair comparison, we also tune hyperparameters for both margin loss and logistic loss.', 'Speciefac81cally, we tune learning rates in {0.', '0} for both loss functions.', 'We further tune the margin in {0, 0.', '6} for margin loss, the temperature in {0.', '0} for logistic loss.', 'For simplicity, we only consider the negatives from one augmentation view (instead of both sides), which slightly impairs performance but ensures fair comparison.', 'Further Comparison to Related Methods As we have noted in the main text, most individual components of SimCLR have appeared in previous works, and the improved performance is a result of a combination of these design choices.', '1 provides a high-level comparison of the design choices of our method with those of previous methods.', 'Compared with previous work, our design choices are generally simpler.', 'Data Augmentation Custom Model CPC v2 AMDIM Fast AutoAug.', 'CMC Fast AutoAug.', 'Crop+color MoCo P', 'A high-level comparison of design choices and training setup (for best result on ImageNet) for each method.', 'Note that descriptions provided here are general; even when they match for two methods, formulations and implementations may differ (e.', 'for color augmentation).', 'Refer to the original papers for more details.', '#Examples are split into multiple patches, which enlarges the effective batch size.', 'e28897A memory bank is employed.', 'In below, we provide an in-depth comparison of our method to the recently chosen contrastive representation learning methods: e280a2 DIM/AMDIM (Hjelm et al.', ', 2018; Bachman et al.', ', 2019) achieved global-to-local/local-to -neighbor prediction by predicting the middle layer of ConvNet.', 'The ConvNet is a ResNet that has bewitched modiefac8 to place signiefac81cant constraints on the receptive efac81elds of the network (e.', 'replacing many 3x3 Convs with 1x1 Convs).', 'In our framework, we decouple the prediction task and encoder architecture, by random cropping (with resizing) and using the efac81nal 256512102420484096Batch size75.', '0Top 1Temperature0.', '0256512102420484096Batch size909192939495Top 1Tčature0.', '00cA Simple Framework for Contrastive Learning of Visual Representations representations of two augmented views for prediction, so we can use standard and more powerful ResNets.', 'Our NT-Xent loss function leverages normalization and temperature to restrict the range of similarity scores, whereas they use a tanh function with regularization.', 'We use simpler data augmentation policies, while they use FastAutoAugment for their best results.', 'e280a2 CPC v1 and v2 (Oord et al.', ', 2018; Hc3a9naff et al.', ', 2019) deefac81ne the context prediction task using a deterministic strategy to split examples into patches, and a context aggregation network (a PixelCNN) to aggregate these patches.', 'The base encoder network sees only patches, which are considerably smaller than the original image.', 'We decouple the prediction task and the encoder architecture, so we do not require a context aggregation network, and our encoder can look at the images of the wider spectrum of resolutions.', 'In addition, we use the NT-Xent loss function, which leverages normalization and temperature, whereas they use an unthernalized cross-entropy-based objective.', 'We use simpler data augmentation.', 'e280a2 InstDisc, MoCo, Pč (Wu et al.', ', 2018; He et al.', ', 2019; Misra & van der Maaten, 2019) generalize the Exemplar approach originally proposed by Dosovitskiy et al.', '(2014) and leverage an explicit memory bank.', 'We do not use a memory bank; we efac81nd that, with a larger batch size, in-batch negative example sampling sufefac81ce.', 'We also utilize a nonlinear projection head, and use the representation before the projection head.', 'Although we use similar types of augmentations (e.', ', random crop and color distortion), we expect speciefac81c parameters may be different.', 'e280a2 CMC (Tian et al.', ', 2019) uses a separated network for each view, while we simply use a single network shared with all randomly augmented views.', 'The data augmentation, projection head and loss function are also different.', 'We use larger batch sizes instead of a memory bank.', 'e280a2 Whereas Ye et al.', '(2019) maximize similarities between augmented and unaugmented copies of the same image ; we apply data augmentation symmetrically to both branches of our framework (Figure 2).', 'We also apply a nonlinear projection on the output of the base feature network, and use the representation before the projection network, whereas Ye et al.', '(2019) use the linearly projected efac81nal hidden vector as the representation.', 'When training with large batch sizes using multiple accelerators, we use global BN to avoid shortcuts that can greatly decrease representation quality.']\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "gec = Pororo(task=\"gec\", lang=\"en\")\n",
    "corrected = []\n",
    "\n",
    "text_new_parsed = text_new.split('. ')\n",
    "\n",
    "for m in tqdm(text_new.split('.'), total=len(text_new_parsed)):\n",
    "    if len(m) < 10:\n",
    "        continue\n",
    "    m = m + '.'\n",
    "    corr = gec(m)\n",
    "    corrected.append(corr)\n",
    "    \n",
    "print(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "protecting-calcium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'A Simple Framework for Contrastive Learning of Visual Representations  Ting Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton 1  0 2 0 2    l u J    1      ]  G L . s c [      3 v 9 0 7 5 0  .  2 0 0 2 : v i X r a  Abstract  This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self- supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in deefac81ning effective predictive tasks, (2) introducing a learn- able nonlinear transformation between the repre- sentation and the contrastive loss substantially im- proves the quality of the learned representations, and (3) contrastive learning beneefac81ts from larger batch sizes and more training steps compared to supervised learning. By combining these efac81ndings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classiefac81er trained on self-supervised representations learned by Sim- CLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of- the-art, matching the performance of a supervised ResNet-50. When efac81ne-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outper- forming AlexNet with 100c397 fewer labels. 1  1. Introduction Learning effective visual representations without human supervision is a long-standing problem. Most mainstream approaches fall into one of two classes: generative or dis- criminative. Generative approaches learn to generate or otherwise model pixels in the input space (Hinton et al., 2006; Kingma & Welling, 2013; Goodfellow et al., 2014).  1Google Research, Brain Team. Correspondence to: Ting Chen  <iamtingchen@google.com>.  Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).  1Code available at https://github.com/google-research/simclr.  Figure 1. ImageNet Top-1 accuracy of linear classiefac81ers trained on representations learned with different self-supervised meth- ods (pretrained on ImageNet). Gray cross indicates supervised ResNet-50. Our method, SimCLR, is shown in bold.  However, pixel-level generation is computationally expen- sive and may not be necessary for representation learning. Discriminative approaches learn representations using objec- tive functions similar to those used for supervised learning, but train networks to perform pretext tasks where both the in- puts and labels are derived from an unlabeled dataset. Many such approaches have relied on heuristics to design pretext tasks (Doersch et al., 2015; Zhang et al., 2016; Noroozi & Favaro, 2016; Gidaris et al., 2018), which could limit the generality of the learned representations. Discriminative approaches based on contrastive learning in the latent space have recently shown great promise, achieving state-of-the- art results (Hadsell et al., 2006; Dosovitskiy et al., 2014; Oord et al., 2018; Bachman et al., 2019). In this work, we introduce a simple framework for con- trastive learning of visual representations, which we call SimCLR. Not only does SimCLR outperform previous work (Figure 1), but it is also simpler, requiring neither special- ized architectures (Bachman et al., 2019; Hc3a9naff et al., 2019) nor a memory bank (Wu et al., 2018; Tian et al., 2019; He et al., 2019; Misra & van der Maaten, 2019). In order to understand what enables good contrastive repre- sentation learning, we systematically study the major com- ponents of our framework and show that:  2550100200400626Number of Parameters (Millions)5560657075ImageNet Top-1 Accuracy (%)InstDiscRotationBigBiGANLACPCv2CPCv2-LCMCAMDIMMoCoMoCo (2x)MoCo (4x)PIRLPIRL-ens.PIRL-c2xSimCLRSimCLR (2x)SimCLR (4x)Supervised0cA Simple Framework for Contrastive Learning of Visual Representations  e280a2 Composition of multiple data augmentation operations is crucial in deefac81ning the contrastive prediction tasks that yield effective representations. In addition, unsupervised contrastive learning beneefac81ts from stronger data augmen- tation than supervised learning.  e280a2 Introducing a learnable nonlinear transformation be- tween the representation and the contrastive loss substan- tially improves the quality of the learned representations. e280a2 Representation learning with contrastive cross entropy loss beneefac81ts from normalized embeddings and an appro- priately adjusted temperature parameter.  e280a2 Contrastive learning beneefac81ts from larger batch sizes and longer training compared to its supervised counterpart. Like supervised learning, contrastive learning beneefac81ts from deeper and wider networks.  We combine these efac81ndings to achieve a new state-of-the-art in self-supervised and semi-supervised learning on Ima- geNet ILSVRC-2012 (Russakovsky et al., 2015). Under the linear evaluation protocol, SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art (Hc3a9naff et al., 2019). When efac81ne-tuned with only 1% of the ImageNet labels, SimCLR achieves 85.8% top-5 accuracy, a relative improvement of 10% (Hc3a9naff et al., 2019). When efac81ne-tuned on other natural image classiefac81ca- tion datasets, SimCLR performs on par with or better than a strong supervised baseline (Kornblith et al., 2019) on 10 out of 12 datasets.  2. Method 2.1. The Contrastive Learning Framework  Inspired by recent contrastive learning algorithms (see Sec- tion 7 for an overview), SimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space. As illustrated in Figure 2, this framework comprises the following four major components. e280a2 A stochastic data augmentation module that transforms any given data example randomly resulting in two cor- related views of the same example, denoted cb9cxi and cb9cxj, which we consider as a positive pair. In this work, we sequentially apply three simple augmentations: random cropping followed by resize back to the original size, ran- dom color distortions, and random Gaussian blur. As shown in Section 3, the combination of random crop and color distortion is crucial to achieve a good performance. e280a2 A neural network base encoder f (c2b7) that extracts repre- sentation vectors from augmented data examples. Our framework allows various choices of the network archi- tecture without any constraints. We opt for simplicity and adopt the commonly used ResNet (He et al., 2016)  zi g(c2b7)  hi f (c2b7)  cb9cxi  Maximize agreement  e28690e28892 Representatione28892e28692  te288bcT  x  T  e288bc  (cid:48) t  zj g(c2b7)  hj  f (c2b7)  cb9cxj  Figure 2. A simple framework for contrastive learning of visual representations. Two separate data augmentation operators are sampled from the same family of augmentations (t e288bc T and t(cid:48) e288bc T ) and applied to each data example to obtain two correlated views. A base encoder network f (c2b7) and a projection head g(c2b7) are trained to maximize agreement using a contrastive loss. After training is completed, we throw away the projection head g(c2b7) and use encoder f (c2b7) and representation h for downstream tasks.  to obtain hi = f ( cb9cxi) = ResNet( cb9cxi) where hi e28888 Rd is the output after the average pooling layer.  e280a2 A small neural network projection head g(c2b7) that maps representations to the space where contrastive loss is applied. We use a MLP with one hidden layer to obtain zi = g(hi) = W (2)cf83(W (1)hi) where cf83 is a ReLU non- linearity. As shown in section 4, we efac81nd it beneefac81cial to deefac81ne the contrastive loss on zie28099s rather than hie28099s.  e280a2 A contrastive loss function deefac81ned for a contrastive pre- diction task. Given a set { cb9cxk} including a positive pair of examples cb9cxi and cb9cxj, the contrastive prediction task aims to identify cb9cxj in { cb9cxk}k(cid:54)=i for a given cb9cxi.  We randomly sample a minibatch of N examples and deefac81ne the contrastive prediction task on pairs of augmented exam- ples derived from the minibatch, resulting in 2N data points. We do not sample negative examples explicitly. Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N e28892 1) augmented examples within a minibatch as negative examples. Let sim(u, v) = u(cid:62)v/(cid:107)u(cid:107)(cid:107)v(cid:107) de- note the dot product between (cid:96)2 normalized u and v (i.e. cosine similarity). Then the loss function for a positive pair of examples (i, j) is deefac81ned as  (cid:96)i,j = e28892 log  exp(sim(zi, zj)/cf84 ) 1[k(cid:54)=i] exp(sim(zi, zk)/cf84 )  ,  (1)  where 1[k(cid:54)=i] e28888 {0, 1} is an indicator function evaluating to 1 iff k (cid:54)= i and cf84 denotes a temperature parameter. The efac81- nal loss is computed across all positive pairs, both (i, j) and (j, i), in a mini-batch. This loss has been used in previous work (Sohn, 2016; Wu et al., 2018; Oord et al., 2018); for convenience, we term it NT-Xent (the normalized temperature-scaled cross entropy loss).  (cid:80)2N  k=1  0cA Simple Framework for Contrastive Learning of Visual Representations  Algorithm 1 SimCLRe28099s main learning algorithm.  input: batch size N, constant cf84, structure of f, g, T . for sampled minibatch {xk}N for all k e28888 {1, . . . , N} do  k=1 do  draw two augmentation functions te288bcT , t(cid:48)e288bcT # the efac81rst augmentation cb9cx2ke288921 = t(xk) h2ke288921 = f ( cb9cx2ke288921) z2ke288921 = g(h2ke288921) # the second augmentation cb9cx2k = t(cid:48)(xk) h2k = f ( cb9cx2k) z2k = g(h2k)  # representation # projection  # representation # projection  end for for all i e28888 {1, . . . , 2N} and j e28888 {1, . . . , 2N} do  si,j = z(cid:62)  i zj/((cid:107)zi(cid:107)(cid:107)zj(cid:107)) (cid:80)N k=1 [(cid:96)(2ke288921, 2k) + (cid:96)(2k, 2ke288921)]  end for (cid:80)2N deefac81ne (cid:96)(i, j) as (cid:96)(i, j) =e28892 log L = 1 update networks f and g to minimize L  2N  k=1  exp(si,j /cf84 ) 1[k(cid:54)=i] exp(si,k/cf84 )  # pairwise similarity  end for return encoder network f (c2b7), and throw away g(c2b7)  Algorithm 1 summarizes the proposed method.  2.2. Training with Large Batch Size  To keep it simple, we do not train the model with a memory bank (Wu et al., 2018; He et al., 2019). Instead, we vary the training batch size N from 256 to 8192. A batch size of 8192 gives us 16382 negative examples per positive pair from both augmentation views. Training with large batch size may be unstable when using standard SGD/Momentum with linear learning rate scaling (Goyal et al., 2017). To stabilize the training, we use the LARS optimizer (You et al., 2017) for all batch sizes. We train our model with Cloud TPUs, using 32 to 128 cores depending on the batch size.2 Global BN. Standard ResNets use batch normaliza- tion (Ioffe & Szegedy, 2015). In distributed training with data parallelism, the BN mean and variance are typically aggregated locally per device. In our contrastive learning, as positive pairs are computed in the same device, the model can exploit the local information leakage to improve pre- diction accuracy without improving representations. We ad- dress this issue by aggregating BN mean and variance over all devices during the training. Other approaches include shufefac82ing data examples across devices (He et al., 2019), or replacing BN with layer norm (Hc3a9naff et al., 2019).  2With 128 TPU v3 cores, it takes e288bc1.5 hours to train our  ResNet-50 with a batch size of 4096 for 100 epochs.  A B  D  C  (a) Global and local views.  (b) Adjacent views.  Figure 3. Solid rectangles are images, dashed rectangles are ran- dom crops. By randomly cropping images, we sample contrastive prediction tasks that include global to local view (B e28692 A) or adjacent view (D e28692 C) prediction.  2.3. Evaluation Protocol  Here we lay out the protocol for our empirical studies, which aim to understand different design choices in our framework. Dataset and Metrics. Most of our study for unsupervised pretraining (learning encoder network f without labels) is done using the ImageNet ILSVRC-2012 dataset (Rus- sakovsky et al., 2015). Some additional pretraining experi- ments on CIFAR-10 (Krizhevsky & Hinton, 2009) can be found in Appendix B.9. We also test the pretrained results on a wide range of datasets for transfer learning. To evalu- ate the learned representations, we follow the widely used linear evaluation protocol (Zhang et al., 2016; Oord et al., 2018; Bachman et al., 2019; Kolesnikov et al., 2019), where a linear classiefac81er is trained on top of the frozen base net- work, and test accuracy is used as a proxy for representation quality. Beyond linear evaluation, we also compare against state-of-the-art on semi-supervised and transfer learning. Default setting. Unless otherwise speciefac81ed, for data aug- mentation we use random crop and resize (with random efac82ip), color distortions, and Gaussian blur (for details, see Appendix A). We use ResNet-50 as the base encoder net- work, and a 2-layer MLP projection head to project the representation to a 128-dimensional latent space. As the loss, we use NT-Xent, optimized using LARS with learning rate of 4.8 (= 0.3 c397 BatchSize/256) and weight decay of 10e288926. We train at batch size 4096 for 100 epochs.3 Fur- thermore, we use linear warmup for the efac81rst 10 epochs, and decay the learning rate with the cosine decay schedule without restarts (Loshchilov & Hutter, 2016).  3. Data Augmentation for Contrastive  Representation Learning  Data augmentation deefac81nes predictive tasks. While data augmentation has been widely used in both supervised and unsupervised representation learning (Krizhevsky et al.,  3Although max performance is not reached in 100 epochs, rea- sonable results are achieved, allowing fair and efefac81cient ablations.  0cA Simple Framework for Contrastive Learning of Visual Representations  (a) Original  (b) Crop and resize  (c) Crop, resize (and efac82ip) (d) Color distort. (drop)  (e) Color distort. (jitter)  (f) Rotate {90e297a6, 180e297a6, 270e297a6}  (g) Cutout  (h) Gaussian noise  (i) Gaussian blur  (j) Sobel efac81ltering  Figure 4. Illustrations of the studied data augmentation operators. Each augmentation can transform data stochastically with some internal parameters (e.g. rotation degree, noise level). Note that we only test these operators in ablation, the augmentation policy used to train our models only includes random crop (with efac82ip and resize), color distortion, and Gaussian blur. (Original image cc-by: Von.grzanka)  2012; Hc3a9naff et al., 2019; Bachman et al., 2019), it has not been considered as a systematic way to deefac81ne the con- trastive prediction task. Many existing approaches deefac81ne contrastive prediction tasks by changing the architecture. For example, Hjelm et al. (2018); Bachman et al. (2019) achieve global-to-local view prediction via constraining the receptive efac81eld in the network architecture, whereas Oord et al. (2018); Hc3a9naff et al. (2019) achieve neighboring view prediction via a efac81xed image splitting procedure and a con- text aggregation network. We show that this complexity can be avoided by performing simple random cropping (with resizing) of target images, which creates a family of predic- tive tasks subsuming the above mentioned two, as shown in Figure 3. This simple design choice conveniently decouples the predictive task from other components such as the neural network architecture. Broader contrastive prediction tasks can be deefac81ned by extending the family of augmentations and composing them stochastically.  3.1. Composition of data augmentation operations is  crucial for learning good representations  To systematically study the impact of data augmentation, we consider several common augmentations here. One type of augmentation involves spatial/geometric transformation of data, such as cropping and resizing (with horizontal efac82ipping), rotation (Gidaris et al., 2018) and cutout (De- Vries & Taylor, 2017). The other type of augmentation involves appearance transformation, such as color distortion (including color dropping, brightness, contrast, saturation, hue) (Howard, 2013; Szegedy et al., 2015), Gaussian blur, and Sobel efac81ltering. Figure 4 visualizes the augmentations that we study in this work.  Figure 5. Linear evaluation (ImageNet top-1 accuracy) under in- dividual or composition of data augmentations, applied only to one branch. For all columns but the last, diagonal entries corre- spond to single transformation, and off-diagonals correspond to composition of two transformations (applied sequentially). The last column reefac82ects the average over the row.  To understand the effects of individual data augmentations and the importance of augmentation composition, we in- vestigate the performance of our framework when applying augmentations individually or in pairs. Since ImageNet images are of different sizes, we always apply crop and re- size images (Krizhevsky et al., 2012; Szegedy et al., 2015), which makes it difefac81cult to study other augmentations in the absence of cropping. To eliminate this confound, we consider an asymmetric data transformation setting for this ablation. Speciefac81cally, we always efac81rst randomly crop im- ages and resize them to the same resolution, and we then apply the targeted transformation(s) only to one branch of the framework in Figure 2, while leaving the other branch as the identity (i.e. t(xi) = xi). Note that this asymmet-  CropCutoutColorSobelNoiseBlurRotateAverage2nd transformationCropCutoutColorSobelNoiseBlurRotate1st transformation33.133.956.346.039.935.030.239.232.225.633.940.026.525.222.429.455.835.518.821.011.416.520.825.746.240.620.94.09.36.24.218.838.825.87.57.69.89.89.615.535.125.216.65.89.72.66.714.530.022.520.74.39.76.52.613.810203040500cA Simple Framework for Contrastive Learning of Visual Representations  (a) Without color distortion.  (b) With color distortion.  Figure 6. Histograms of pixel intensities (over all channels) for different crops of two different images (i.e. two rows). The image for the efac81rst row is from Figure 4. All axes have the same range.  Methods SimCLR Supervised  1/8 59.6 77.0  Color distortion strength 1/4 61.0 76.7  1/2 62.6 76.5  63.2 75.7  1  64.5 75.4  1 (+Blur) AutoAug  61.1 77.1  Table 1. Top-1 accuracy of unsupervised ResNet-50 using linear evaluation and supervised ResNet-505, under varied color distor- tion strength (see Appendix A) and other data transformations. Strength 1 (+Blur) is our default data augmentation policy.  ric data augmentation hurts the performance. Nonetheless, this setup should not substantively change the impact of individual data augmentations or their compositions. Figure 5 shows linear evaluation results under individual and composition of transformations. We observe that no single transformation sufefac81ces to learn good representations, even though the model can almost perfectly identify the positive pairs in the contrastive task. When composing aug- mentations, the contrastive prediction task becomes harder, but the quality of representation improves dramatically. Ap- pendix B.2 provides a further study on composing broader set of augmentations. One composition of augmentations stands out: random crop- ping and random color distortion. We conjecture that one serious issue when using only random cropping as data augmentation is that most patches from an image share a similar color distribution. Figure 6 shows that color his- tograms alone sufefac81ce to distinguish images. Neural nets may exploit this shortcut to solve the predictive task. There- fore, it is critical to compose cropping with color distortion in order to learn generalizable features.  3.2. Contrastive learning needs stronger data  augmentation than supervised learning  To further demonstrate the importance of the color aug- mentation, we adjust the strength of color augmentation as  5Supervised models are trained for 90 epochs; longer training  improves performance of stronger augmentation by e288bc 0.5%.  Figure 7. Linear evaluation of models with varied depth and width. Models in blue dots are ours trained for 100 epochs, models in red stars are ours trained for 1000 epochs, and models in green crosses are supervised ResNets trained for 90 epochs7 (He et al., 2016).  shown in Table 1. Stronger color augmentation substan- tially improves the linear evaluation of the learned unsuper- vised models. In this context, AutoAugment (Cubuk et al., 2019), a sophisticated augmentation policy found using su- pervised learning, does not work better than simple cropping + (stronger) color distortion. When training supervised mod- els with the same set of augmentations, we observe that stronger color augmentation does not improve or even hurts their performance. Thus, our experiments show that unsu- pervised contrastive learning beneefac81ts from stronger (color) data augmentation than supervised learning. Although pre- vious work has reported that data augmentation is useful for self-supervised learning (Doersch et al., 2015; Bachman et al., 2019; Hc3a9naff et al., 2019; Asano et al., 2019), we show that data augmentation that does not yield accuracy beneefac81ts for supervised learning can still help considerably with contrastive learning.  4. Architectures for Encoder and Head 4.1. Unsupervised contrastive learning beneefac81ts (more)  from bigger models  Figure 7 shows, perhaps unsurprisingly, that increasing depth and width both improve performance. While similar efac81ndings hold for supervised learning (He et al., 2016), we efac81nd the gap between supervised models and linear classiefac81ers trained on unsupervised models shrinks as the model size increases, suggesting that unsupervised learning beneefac81ts more from bigger models than its supervised counterpart.  7Training longer does not improve supervised ResNets (see  Appendix B.3).  050100150200250300350400450Number of Parameters (Millions)50556065707580Top 1R101R101(2x)R152R152(2x)R18R18(2x)R18(4x)R34R34(2x)R34(4x)R50R50(2x)R50(4x)Sup. R50Sup. R50(2x)Sup. R50(4x)R50*R50(2x)*R50(4x)*0cA Simple Framework for Contrastive Learning of Visual Representations  Name  NT-Xent  NT-Logistic  Margin Triplet  uT v+/cf84 e28892 log(cid:80)  Negative loss function  ve28888{v+,ve28892} exp(uT v/cf84 )  log cf83(uT v+/cf84 ) + log cf83(e28892uT ve28892/cf84 )  e28892 max(uT ve28892 e28892 uT v+ + m, 0)  Gradient w.r.t. u  )/cf84 v+ e28892(cid:80)  Z(u)  (1 e28892 exp(uT v+/cf84 )  ve28892 exp(uT ve28892/cf84 ) (cf83(e28892uT v+/cf84 ))/cf84 v+ e28892 cf83(uT ve28892/cf84 )/cf84 ve28892 v+ e28892 ve28892 if uT v+ e28892 uT ve28892 < m else 0  Z(u)  /cf84 ve28892  Table 2. Negative loss functions and their gradients. All input vectors, i.e. u, v+, ve28892, are (cid:96)2 normalized. NT-Xent is an abbreviation for e2809cNormalized Temperature-scaled Cross Entropye2809d. Different loss functions impose different weightings of positive and negative examples.  What to predict?  Color vs grayscale Rotation Orig. vs corrupted Orig. vs Sobel efac81ltered  Random guess Representation g(h) 97.4 25.6 59.6 56.3  h 99.3 67.6 99.5 96.6  80 25 50 50  Figure 8. Linear evaluation of representations with different pro- jection heads g(c2b7) and various dimensions of z = g(h). The representation h (before projection) is 2048-dimensional here. 4.2. A nonlinear projection head improves the representation quality of the layer before it  We then study the importance of including a projection head, i.e. g(h). Figure 8 shows linear evaluation results using three different architecture for the head: (1) identity mapping; (2) linear projection, as used by several previous approaches (Wu et al., 2018); and (3) the default nonlinear projection with one additional hidden layer (and ReLU acti- vation), similar to Bachman et al. (2019). We observe that a nonlinear projection is better than a linear projection (+3%), and much better than no projection (>10%). When a pro- jection head is used, similar results are observed regardless of output dimension. Furthermore, even when nonlinear projection is used, the layer before the projection head, h, is still much better (>10%) than the layer after, z = g(h), which shows that the hidden layer before the projection head is a better representation than the layer after. We conjecture that the importance of using the representa- tion before the nonlinear projection is due to loss of informa- tion induced by the contrastive loss. In particular, z = g(h) is trained to be invariant to data transformation. Thus, g can remove information that may be useful for the downstream task, such as the color or orientation of objects. By leverag- ing the nonlinear transformation g(c2b7), more information can be formed and maintained in h. To verify this hypothesis, we conduct experiments that use either h or g(h) to learn to predict the transformation applied during the pretraining. Here we set g(h) = W (2)cf83(W (1)h), with the same input and output dimensionality (i.e. 2048). Table 3 shows h contains much more information about the transformation applied, while g(h) loses information. Further analysis can  Table 3. Accuracy of training additional MLPs on different repre- sentations to predict the transformation applied. Other than crop and color augmentation, we additionally and independently add rotation (one of {0e297a6, 90e297a6, 180e297a6, 270e297a6}), Gaussian noise, and So- bel efac81ltering transformation during the pretraining for the last three rows. Both h and g(h) are of the same dimensionality, i.e. 2048.  be found in Appendix B.4.  5. Loss Functions and Batch Size 5.1. Normalized cross entropy loss with adjustable  temperature works better than alternatives  We compare the NT-Xent loss against other commonly used contrastive loss functions, such as logistic loss (Mikolov et al., 2013), and margin loss (Schroff et al., 2015). Table 2 shows the objective function as well as the gradient to the input of the loss function. Looking at the gradient, we observe 1) (cid:96)2 normalization (i.e. cosine similarity) along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives; and 2) unlike cross-entropy, other objec- tive functions do not weigh the negatives by their relative hardness. As a result, one must apply semi-hard negative mining (Schroff et al., 2015) for these loss functions: in- stead of computing the gradient over all loss terms, one can compute the gradient using semi-hard negative terms (i.e., those that are within the loss margin and closest in distance, but farther than positive examples). To make the comparisons fair, we use the same (cid:96)2 normaliza- tion for all loss functions, and we tune the hyperparameters, and report their best results.8 Table 4 shows that, while (semi-hard) negative mining helps, the best result is still much worse than our default NT-Xent loss.  8Details can be found in Appendix B.10. For simplicity, we  only consider the negatives from one augmentation view.  326412825651210242048Projection output dimensionality3040506070Top 1ProjectionLinearNon-linearNone0cA Simple Framework for Contrastive Learning of Visual Representations  Margin NT-Logi. Margin (sh) NT-Logi.(sh) NT-Xent 50.9  57.5  57.9  63.9  51.6  Table 4. Linear evaluation (top-1) for models trained with different loss functions. e2809cshe2809d means using semi-hard negative mining.  (cid:96)2 norm?  Yes  No  cf84 0.05 0.1 0.5 1 10 100  Entropy Contrastive acc.  1.0 4.5 8.2 8.3 0.5 0.5  90.5 87.8 68.2 59.1 91.7 92.1  Top 1 59.7 64.4 60.7 58.0 57.2 57.0  Table 5. Linear evaluation for models trained with different choices of (cid:96)2 norm and temperature cf84 for NT-Xent loss. The contrastive distribution is over 4096 examples.  Figure 9. Linear evaluation models (ResNet-50) trained with differ- ent batch size and epochs. Each bar is a single run from scratch.10  We next test the importance of the (cid:96)2 normalization (i.e. cosine similarity vs dot product) and temperature cf84 in our default NT-Xent loss. Table 5 shows that without normal- ization and proper temperature scaling, performance is sig- niefac81cantly worse. Without (cid:96)2 normalization, the contrastive task accuracy is higher, but the resulting representation is worse under linear evaluation.  5.2. Contrastive learning beneefac81ts (more) from larger  batch sizes and longer training  Figure 9 shows the impact of batch size when models are trained for different numbers of epochs. We efac81nd that, when the number of training epochs is small (e.g. 100 epochs), larger batch sizes have a signiefac81cant advantage over the smaller ones. With more training steps/epochs, the gaps between different batch sizes decrease or disappear, pro- vided the batches are randomly resampled. In contrast to  10A linear learning rate scaling is used here. Figure B.1 shows using a square root learning rate scaling can improve performance of ones with small batch sizes.  Architecture  Method Methods using ResNet-50: ResNet-50 Local Agg. ResNet-50 MoCo ResNet-50 PIRL ResNet-50 CPC v2 SimCLR (ours) ResNet-50 Methods using other architectures: RevNet-50 (4c397) Rotation RevNet-50 (4c397) BigBiGAN AMDIM Custom-ResNet ResNet-50 (2c397) CMC ResNet-50 (4c397) MoCo ResNet-161 (e28897) CPC v2 SimCLR (ours) ResNet-50 (2c397) SimCLR (ours) ResNet-50 (4c397)  Param (M) Top 1 Top 5  24 24 24 24 24  86 86 626 188 375 305 94 375  60.2 60.6 63.6 63.8 69.3  55.4 61.3 68.1 68.4 68.6 71.5 74.2 76.5  - - -  85.3 89.0  -  -  -  81.9  88.2  90.1 92.0 93.2  Table 6. ImageNet accuracies of linear classiefac81ers trained on repre- sentations learned with different self-supervised methods.  Method  Architecture  ResNet-50  Supervised baseline Methods using other label-propagation: ResNet-50 Pseudo-label ResNet-50 VAT+Entropy Min. UDA (w. RandAug) ResNet-50 FixMatch (w. RandAug) ResNet-50 S4L (Rot+VAT+En. M.) ResNet-50 (4c397) Methods using representation learning only: InstDisc ResNet-50 RevNet-50 (4c397) BigBiGAN PIRL ResNet-50 ResNet-161(e28897) CPC v2 ResNet-50 SimCLR (ours) ResNet-50 (2c397) SimCLR (ours) ResNet-50 (4c397) SimCLR (ours)  Label fraction 1% 10%  Top 5  48.4  80.4  51.6 47.0  - - -  39.2 55.2 57.2 77.9 75.5 83.0 85.8  82.4 83.4 88.5 89.1 91.2  77.4 78.8 83.8 91.2 87.8 91.2 92.6  Table 7. ImageNet accuracy of models trained with few labels.  supervised learning (Goyal et al., 2017), in contrastive learn- ing, larger batch sizes provide more negative examples, facilitating convergence (i.e. taking fewer epochs and steps for a given accuracy). Training longer also provides more negative examples, improving the results. In Appendix B.1, results with even longer training steps are provided.  6. Comparison with State-of-the-art In this subsection, similar to Kolesnikov et al. (2019); He et al. (2019), we use ResNet-50 in 3 different hidden layer widths (width multipliers of 1c397, 2c397, and 4c397). For better convergence, our models here are trained for 1000 epochs. Linear evaluation. Table 6 compares our results with previ- ous approaches (Zhuang et al., 2019; He et al., 2019; Misra & van der Maaten, 2019; Hc3a9naff et al., 2019; Kolesnikov et al., 2019; Donahue & Simonyan, 2019; Bachman et al.,  1002003004005006007008009001000Training epochs50.052.555.057.560.062.565.067.570.0Top 1Batch size25651210242048409681920cA Simple Framework for Contrastive Learning of Visual Representations  Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers  Linear evaluation: SimCLR (ours) 76.9 Supervised 75.2 Fine-tuned: SimCLR (ours) 89.4 88.7 Supervised Random init 88.3  95.3 95.7  98.6 98.3 96.0  80.2 81.2  89.0 88.7 81.9  48.4 56.4  78.2 77.8 77.0  65.9 64.9  68.1 67.0 53.7  60.0 68.8  92.1 91.4 91.3  61.2 63.8  87.0 88.0 84.8  84.2 83.8  86.6 86.5 69.4  78.9 89.2 78.7 92.3  77.8 92.1 78.8 93.2 64.1 82.7  93.9 94.1  94.1 94.2 72.5  95.0 94.2  97.6 98.0 92.5  Table 8. Comparison of transfer learning performance of our self-supervised approach with supervised baselines across 12 natural image classiefac81cation datasets, for ResNet-50 (4c397) models pretrained on ImageNet. Results not signiefac81cantly worse than the best (p > 0.05, permutation test) are shown in bold. See Appendix B.8 for experimental details and results with standard ResNet-50.  2019; Tian et al., 2019) in the linear evaluation setting (see Appendix B.6). Table 1 shows more numerical compar- isons among different methods. We are able to use standard networks to obtain substantially better results compared to previous methods that require speciefac81cally designed archi- tectures. The best result obtained with our ResNet-50 (4c397) can match the supervised pretrained ResNet-50. Semi-supervised learning. We follow Zhai et al. (2019) and sample 1% or 10% of the labeled ILSVRC-12 training datasets in a class-balanced way (e288bc12.8 and e288bc128 images per class respectively). 11 We simply efac81ne-tune the whole base network on the labeled data without regularization (see Appendix B.5). Table 7 shows the comparisons of our results against recent methods (Zhai et al., 2019; Xie et al., 2019; Sohn et al., 2020; Wu et al., 2018; Donahue & Simonyan, 2019; Misra & van der Maaten, 2019; Hc3a9naff et al., 2019). The supervised baseline from (Zhai et al., 2019) is strong due to intensive search of hyper-parameters (including augmentation). Again, our approach signiefac81cantly improves over state-of-the-art with both 1% and 10% of the labels. Interestingly, efac81ne-tuning our pretrained ResNet-50 (2c397, 4c397) on full ImageNet are also signiefac81cantly better then training from scratch (up to 2%, see Appendix B.2). Transfer learning. We evaluate transfer learning perfor- mance across 12 natural image datasets in both linear evalu- ation (efac81xed feature extractor) and efac81ne-tuning settings. Fol- lowing Kornblith et al. (2019), we perform hyperparameter tuning for each model-dataset combination and select the best hyperparameters on a validation set. Table 8 shows results with the ResNet-50 (4c397) model. When efac81ne-tuned, our self-supervised model signiefac81cantly outperforms the su- pervised baseline on 5 datasets, whereas the supervised baseline is superior on only 2 (i.e. Pets and Flowers). On the remaining 5 datasets, the models are statistically tied. Full experimental details as well as results with the standard ResNet-50 architecture are provided in Appendix B.8.  11The details of sampling and exact subsets can be found in https://www.tensorefac82ow.org/datasets/catalog/imagenet2012_subset.  7. Related Work The idea of making representations of an image agree with each other under small transformations dates back to Becker & Hinton (1992). We extend it by leveraging recent ad- vances in data augmentation, network architecture and con- trastive loss. A similar consistency idea, but for class label prediction, has been explored in other contexts such as semi- supervised learning (Xie et al., 2019; Berthelot et al., 2019). Handcrafted pretext tasks. The recent renaissance of self- supervised learning began with artiefac81cially designed pretext tasks, such as relative patch prediction (Doersch et al., 2015), solving jigsaw puzzles (Noroozi & Favaro, 2016), coloriza- tion (Zhang et al., 2016) and rotation prediction (Gidaris et al., 2018; Chen et al., 2019). Although good results can be obtained with bigger networks and longer train- ing (Kolesnikov et al., 2019), these pretext tasks rely on somewhat ad-hoc heuristics, which limits the generality of learned representations. Contrastive visual representation learning. Dating back to Hadsell et al. (2006), these approaches learn represen- tations by contrasting positive pairs against negative pairs. Along these lines, Dosovitskiy et al. (2014) proposes to treat each instance as a class represented by a feature vector (in a parametric form). Wu et al. (2018) proposes to use a memory bank to store the instance class representation vector, an approach adopted and extended in several recent papers (Zhuang et al., 2019; Tian et al., 2019; He et al., 2019; Misra & van der Maaten, 2019). Other work explores the use of in-batch samples for negative sampling instead of a memory bank (Doersch & Zisserman, 2017; Ye et al., 2019; Ji et al., 2019). Recent literature has attempted to relate the success of their methods to maximization of mutual information between latent representations (Oord et al., 2018; Hc3a9naff et al., 2019; Hjelm et al., 2018; Bachman et al., 2019). However, it is not clear if the success of contrastive approaches is determined by the mutual information, or by the speciefac81c form of the contrastive loss (Tschannen et al., 2019).  0cA Simple Framework for Contrastive Learning of Visual Representations  We note that almost all individual components of our frame- work have appeared in previous work, although the speciefac81c instantiations may be different. The superiority of our frame- work relative to previous work is not explained by any single design choice, but by their composition. We provide a com- prehensive comparison of our design choices with those of previous work in Appendix C.  8. Conclusion In this work, we present a simple framework and its in- stantiation for contrastive visual representation learning. We carefully study its components, and show the effects of different design choices. By combining our efac81ndings, we improve considerably over previous methods for self- supervised, semi-supervised, and transfer learning. Our approach differs from standard supervised learning on ImageNet only in the choice of data augmentation, the use of a nonlinear head at the end of the network, and the loss func- tion. The strength of this simple framework suggests that, despite a recent surge in interest, self-supervised learning remains undervalued.  Acknowledgements We would like to thank Xiaohua Zhai, Rafael Mc3bcller and Yani Ioannou for their feedback on the draft. We are also grateful for general support from Google Research teams in Toronto and elsewhere.  References Asano, Y. M., Rupprecht, C., and Vedaldi, A. A critical analysis of self-supervision, or what we can learn from a single image. arXiv preprint arXiv:1904.13132, 2019.  Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning rep- resentations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. 15509e2809315519, 2019.  Becker, S. and Hinton, G. E. Self-organizing neural network that discovers surfaces in random-dot stereograms. Nature, 355 (6356):161e28093163, 1992.  Berg, T., Liu, J., Lee, S. W., Alexander, M. L., Jacobs, D. W., and Belhumeur, P. N. Birdsnap: Large-scale efac81ne-grained visual categorization of birds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2019e280932026. IEEE, 2014.  Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and Raffel, C. A. Mixmatch: A holistic approach to semi- supervised learning. In Advances in Neural Information Pro- cessing Systems, pp. 5050e280935060, 2019.  Bossard, L., Guillaumin, M., and Van Gool, L. Food-101e28093mining discriminative components with random forests. In European conference on computer vision, pp. 446e28093461. Springer, 2014.  Chen, T., Sun, Y., Shi, Y., and Hong, L. On sampling strategies for neural network-based collaborative efac81ltering. In Proceed- ings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 767e28093776, 2017.  Chen, T., Zhai, X., Ritter, M., Lucic, M., and Houlsby, N. Self- supervised gans via auxiliary rotation loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 12154e2809312163, 2019.  Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3606e28093 3613. IEEE, 2014.  Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 113e28093123, 2019.  DeVries, T. and Taylor, G. W.  Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.  Doersch, C. and Zisserman, A. Multi-task self-supervised visual learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2051e280932060, 2017.  Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422e280931430, 2015.  Donahue, J. and Simonyan, K. Large scale adversarial representa- tion learning. In Advances in Neural Information Processing Systems, pp. 10541e2809310551, 2019.  Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, pp. 647e28093655, 2014.  Dosovitskiy, A., Springenberg, J. T., Riedmiller, M., and Brox, T. Discriminative unsupervised feature learning with convolutional neural networks. In Advances in neural information processing systems, pp. 766e28093774, 2014.  Everingham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303e28093338, 2010.  Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop on Generative-Model Based Vision, 2004.  Gidaris, S., Singh, P., and Komodakis, N. Unsupervised represen- tation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.  Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde- Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672e280932680, 2014.  0cA Simple Framework for Contrastive Learning of Visual Representations  Goyal, P., Dollc3a1r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.  Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer So- ciety Conference on Computer Vision and Pattern Recognition (CVPRe2809906), volume 2, pp. 1735e280931742. IEEE, 2006.  He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770e28093778, 2016.  He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.  Hc3a9naff, O. J., Razavi, A., Doersch, C., Eslami, S., and Oord, A. v. d. Data-efefac81cient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.  Hinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning al- gorithm for deep belief nets. Neural computation, 18(7):1527e28093 1554, 2006.  Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep repre- sentations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.  Howard, A. G. Some improvements on deep convolutional neural network based image classiefac81cation. arXiv preprint arXiv:1312.5402, 2013.  Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.  Ji, X., Henriques, J. F., and Vedaldi, A. Invariant information clustering for unsupervised image classiefac81cation and segmenta- tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 9865e280939874, 2019.  Kingma, D. P. and Welling, M. Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114, 2013.  Kolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-supervised In Proceedings of the IEEE visual representation learning. conference on Computer Vision and Pattern Recognition, pp. 1920e280931929, 2019.  Kornblith, S., Shlens, J., and Le, Q. V. Do better ImageNet models In Proceedings of the IEEE conference on transfer better? computer vision and pattern recognition, pp. 2661e280932671, 2019.  Krause, J., Deng, J., Stark, M., and Fei-Fei, L. Collecting a large-scale dataset of efac81ne-grained cars. In Second Workshop on Fine-Grained Visual Categorization, 2013.  Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. URL https://www.cs.toronto.edu/~kriz/ learning-features-2009-TR.pdf.  Loshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent  with warm restarts. arXiv preprint arXiv:1608.03983, 2016.  Maaten, L. v. d. and Hinton, G. Visualizing data using t-sne. Jour-  nal of machine learning research, 9(Nov):2579e280932605, 2008.  Maji, S., Kannala, J., Rahtu, E., Blaschko, M., and Vedaldi, A. Fine-grained visual classiefac81cation of aircraft. Technical report, 2013.  Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efefac81cient esti- mation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.  Misra,  I. and van der Maaten, L.  ing of pretext-invariant representations. arXiv:1912.01991, 2019.  Self-supervised learn- arXiv preprint  Nilsback, M.-E. and Zisserman, A. Automated efac82ower classiefac81cation over a large number of classes. In Computer Vision, Graphics & Image Processing, 2008. ICVGIPe2809908. Sixth Indian Conference on, pp. 722e28093729. IEEE, 2008.  Noroozi, M. and Favaro, P. Unsupervised learning of visual repre- sentations by solving jigsaw puzzles. In European Conference on Computer Vision, pp. 69e2809384. Springer, 2016.  Oord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.  Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3498e280933505. IEEE, 2012.  Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211e28093252, 2015.  Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A uniefac81ed In Proceed- embedding for face recognition and clustering. ings of the IEEE conference on computer vision and pattern recognition, pp. 815e28093823, 2015.  Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.  Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. In Advances in neural information processing systems, pp. 1857e280931865, 2016.  Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C. Fixmatch: Simpli- fying semi-supervised learning with consistency and conefac81dence. arXiv preprint arXiv:2001.07685, 2020.  Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1e280939, 2015.  Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding.  arXiv preprint arXiv:1906.05849, 2019.  Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiefac81- cation with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097e280931105, 2012.  Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S., and Lu- cic, M. On mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.  0cA Simple Framework for Contrastive Learning of Visual Representations  Wu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised feature learning via non-parametric instance discrimination. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3733e280933742, 2018.  Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun database: Large-scale scene recognition from abbey to zoo. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3485e280933492. IEEE, 2010.  Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. Unsu- pervised data augmentation. arXiv preprint arXiv:1904.12848, 2019.  Ye, M., Zhang, X., Yuen, P. C., and Chang, S.-F. Unsupervised embedding learning via invariant and spreading instance feature. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6210e280936219, 2019.  You, Y., Gitman, I., and Ginsburg, B. Large batch training of con- volutional networks. arXiv preprint arXiv:1708.03888, 2017.  Zhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S4l: Self- supervised semi-supervised learning. In The IEEE International Conference on Computer Vision (ICCV), October 2019.  Zhang, R., Isola, P., and Efros, A. A. Colorful image coloriza- tion. In European conference on computer vision, pp. 649e28093666. Springer, 2016.  Zhuang, C., Zhai, A. L., and Yamins, D. Local aggregation for unsupervised learning of visual embeddings. In Proceedings of the IEEE International Conference on Computer Vision, pp. 6002e280936012, 2019.  0cA Simple Framework for Contrastive Learning of Visual Representations  A. Data Augmentation Details In our default pretraining setting (which is used to train our best models), we utilize random crop (with resize and random efac82ip), random color distortion, and random Gaussian blur as the data augmentations. The details of these three augmentations are provided below.  Random crop and resize to 224x224 We use standard Inception-style random cropping (Szegedy et al., 2015). The crop of random size (uniform from 0.08 to 1.0 in area) of the original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop is efac81nally resized to the original size. This has been imple- mented in Tensorefac82ow as e2809cslim.preprocessing.inception_preprocessing.distorted_bounding_box_crope2809d, or in Pytorch as e2809ctorchvision.transforms.RandomResizedCrope2809d. Additionally, the random crop (with resize) is always followed by a random horizontal/left-to-right efac82ip with 50% probability. This is helpful but not essential. By removing this from our default augmentation policy, the top-1 linear evaluation drops from 64.5% to 63.4% for our ResNet-50 model trained in 100 epochs.  Color distortion Color distortion is composed by color jittering and color dropping. We efac81nd stronger color jittering usually helps, so we set a strength parameter. A pseudo-code for color distortion using TensorFlow is as follows.  import tensorflow as tf def color_distortion(image, s=1.0):  # image is a tensor with value range in [0, 1]. # s is the strength of color distortion.  def color_jitter(x):  # one can also shuffle the order of following augmentations # each time they are applied. x = tf.image.random_brightness(x, max_delta=0.8*s) x = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s) x = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s) x = tf.image.random_hue(x, max_delta=0.2*s) x = tf.clip_by_value(x, 0, 1) return x  def color_drop(x):  image = tf.image.rgb_to_grayscale(image) image = tf.tile(image, [1, 1, 3])  # randomly apply transformation with probability p. image = random_apply(color_jitter, image, p=0.8) image = random_apply(color_drop, image, p=0.2) return image  A pseudo-code for color distortion using Pytorch is as follows 12.  from torchvision import transforms def get_color_distortion(s=1.0):  # s is the strength of color distortion. color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s) rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8) rnd_gray = transforms.RandomGrayscale(p=0.2) color_distort = transforms.Compose([  rnd_color_jitter, rnd_gray])  12Our code and results are based on Tensorefac82ow, the Pytorch code here is a reference.  0cA Simple Framework for Contrastive Learning of Visual Representations  return color_distort  Gaussian blur This augmentation is in our default policy. We efac81nd it helpful, as it improves our ResNet-50 trained for 100 epochs from 63.2% to 64.5%. We blur the image 50% of the time using a Gaussian kernel. We randomly sample cf83 e28888 [0.1, 2.0], and the kernel size is set to be 10% of the image height/width.  B. Additional Experimental Results B.1. Batch Size and Training Steps  Figure B.1 shows the top-5 accuracy on linear evaluation when trained with different batch sizes and training epochs. The conclusion is very similar to top-1 accuracy shown before, except that the differences between different batch sizes and training steps seems slightly smaller here. In both Figure 9 and Figure B.1, we use a linear scaling of learning rate similar to (Goyal et al., 2017) when training with different batch sizes. Although linear learning rate scaling is popular with SGD/Momentum optimizer, we efac81nd a square root learning rate scaling is more desirable with LARS optimizer. With square root learning rate scaling, we have BatchSize, instead of LearningRate = 0.3 c397 BatchSize/256 in the linear scaling case, but the learning rate is the same under both scaling methods when batch size of 4096 (our default batch size). A comparison is presented in Table B.1, where we observe that square root learning rate scaling improves the performance for models trained with small batch sizes and in smaller number of epochs.  LearningRate = 0.075 c397 e2889a  Batch size \\\\\\\\ Epochs  100  256 512 1024 2048 4096 8192  57.5 / 62.8 60.7 / 63.8 62.8 / 64.3 64.0 / 64.7 64.6 / 64.5 64.8 / 64.8  200  61.9 / 64.3 64.0 / 65.6 65.3 / 66.1 66.1 / 66.8 66.5 / 66.8 66.6 / 67.0  400  64.7 / 65.7 66.2 / 66.7 67.2 / 67.2 68.1 / 67.9 68.2 / 68.0 67.8 / 68.3  800  66.6 / 66.5 67.8 / 67.4 68.5 / 68.3 68.9 / 68.8 68.9 / 69.1 69.0 / 69.1  Table B.1. Linear evaluation (top-1) under different batch sizes and training epochs. On the left side of slash sign are models trained with linear LR scaling, and on the right are models trained with square root LR scaling. The result is bolded if it is more than 0.5% better. Square root LR scaling works better for smaller batch size trained in fewer epochs (with LARS optimizer).  We also train with larger batch size (up to 32K) and longer (up to 3200 epochs), with the square root learning rate scaling. A shown in Figure B.2, the performance seems to saturate with a batch size of 8192, while training longer can still signiefac81cantly improve the performance.  Figure B.1. Linear evaluation (top-5) of ResNet-50 trained with different batch sizes and epochs. Each bar is a single run from scratch. See Figure 9 for top-1 accuracy.  Figure B.2. Linear evaluation (top-1) of ResNet-50 trained with different batch sizes and longer epochs. Here a square root learn- ing rate, instead of a linear one, is utilized.  1002003004005006007008009001000Training epochs70.072.575.077.580.082.585.087.590.0Top 5Batch size25651210242048409681925010020040080016003200Training epochs60626466687072Top 1Batch size256512102420484096819216384327680cA Simple Framework for Contrastive Learning of Visual Representations  B.2. Broader composition of data augmentations further improves performance  Our best results in the main text (Table 6 and 7) can be further improved when expanding the default augmentation policy to include the following: (1) Sobel efac81ltering, (2) additional color distortion (equalize, solarize), and (3) motion blur. For linear evaluation protocol, the ResNet-50 models (1c397, 2c397, 4c397) trained with broader data augmentations achieve 70.0 (+0.7), 74.4 (+0.2), 76.8 (+0.3), respectively. Table B.2 shows ImageNet accuracy obtained by efac81ne-tuning the SimCLR model (see Appendix B.5 for the details of efac81ne-tuning procedure). Interestingly, when efac81ne-tuned on full (100%) ImageNet training set, our ResNet (4c397) model achieves 80.4% top-1 / 95.4% top-5 13, which is signiefac81cantly better than that (78.4% top-1 / 94.2% top-5) of training from scratch using the same set of augmentations (i.e. random crop and horizontal efac82ip). For ResNet-50 (2c397), efac81ne-tuning our pre-trained ResNet-50 (2c397) is also better than training from scratch (77.8% top-1 / 93.9% top-5). There is no improvement from efac81ne-tuning for ResNet-50.  Architecture  ResNet-50 ResNet-50 (2c397) ResNet-50 (4c397)  1%  Label fraction  10%  100%  Top 1 Top 5 Top 1 Top 5 Top 1 Top 5 49.4 93.1 94.8 59.4 64.1 95.4  88.1 91.2 92.8  76.0 79.1 80.4  76.6 83.7 86.6  66.1 71.8 74.8  Table B.2. Classiefac81cation accuracy obtained by efac81ne-tuning the SimCLR (which is pretrained with broader data augmentations) on 1%, 10% and full of ImageNet. As a reference, our ResNet-50 (4c397) trained from scratch on 100% labels achieves 78.4% top-1 / 94.2% top-5.  B.3. Effects of Longer Training for Supervised Models  Here we perform experiments to see how training steps and stronger data augmentation affect supervised training. We test ResNet-50 and ResNet-50 (4c397) under the same set of data augmentations (random crops, color distortion, 50% Gaussian blur) as used in our unsupervised models. Figure B.3 shows the top-1 accuracy. We observe that there is no signiefac81cant beneefac81t from training supervised models longer on ImageNet. Stronger data augmentation slightly improves the accuracy of ResNet-50 (4c397) but does not help on ResNet-50. When stronger data augmentation is applied, ResNet-50 generally requires longer training (e.g. 500 epochs 14) to obtain the optimal result, while ResNet-50 (4c397) does not beneefac81t from longer training.  Model  Training epochs  ResNet-50  ResNet-50 (4c397)  90 500 1000 90 500 1000  Top 1  +Color 75.6 76.5 75.2 78.9 78.4 78.2  Crop 76.5 76.2 75.8 78.4 78.3 77.9  +Color+Blur  75.3 76.7 76.4 78.7 78.5 78.3  Table B.3. Top-1 accuracy of supervised models trained longer under various data augmentation procedures (from the same set of data augmentations for contrastive learning).  B.4. Understanding The Non-Linear Projection Head Figure B.3 shows the eigenvalue distribution of linear projection matrix W e28888 R2048c3972048 used to compute z = W h. This matrix has relatively few large eigenvalues, indicating that it is approximately low-rank. Figure B.4 shows t-SNE (Maaten & Hinton, 2008) visualizations of h and z = g(h) for randomly selected 10 classes by our best ResNet-50 (top-1 linear evaluation 69.3%). Classes represented by h are better separated compared to z.  13It is 80.1% top-1 / 95.2% top-5 without broader augmentations for pretraining SimCLR. 14With AutoAugment (Cubuk et al., 2019), optimal test accuracy can be achieved between 900 and 500 epochs.  0cA Simple Framework for Contrastive Learning of Visual Representations  (a) Y-axis in uniform scale.  (b) Y-axis in log scale.  Figure B.3. Squared real eigenvalue distribution of linear projection matrix W e28888 R2048c3972048 used to compute g(h) = W h.  (a) h  (b) z = g(h)  Figure B.4. t-SNE visualizations of hidden vectors of images from a randomly selected 10 classes in the validation set.  B.5. Semi-supervised Learning via Fine-Tuning Fine-tuning Procedure We efac81ne-tune using the Nesterov momentum optimizer with a batch size of 4096, momentum of 0.9, and a learning rate of 0.8 (following LearningRate = 0.05c397 BatchSize/256) without warmup. Only random cropping (with random left-to-right efac82ipping and resizing to 224x224) is used for preprocessing. We do not use any regularization (including weight decay). For 1% labeled data we efac81ne-tune for 60 epochs, and for 10% labeled data we efac81ne-tune for 30 epochs. For the inference, we resize the given image to 256x256, and take a single center crop of 224x224. Table B.4 shows the comparisons of top-1 accuracy for different methods for semi-supervised learning. Our models signiefac81cantly improve state-of-the-art.  Method  Architecture  Label fraction 1% 10%  Top 1  25.4  ResNet-50  ResNet-50 ResNet-50  Supervised baseline Methods using label-propagation: UDA (w. RandAug) FixMatch (w. RandAug) S4L (Rot+VAT+Ent. Min.) ResNet-50 (4c397) Methods using self-supervised representation learning only: CPC v2 SimCLR (ours) SimCLR (ours) SimCLR (ours)  ResNet-161(e28897) ResNet-50 ResNet-50 (2c397) ResNet-50 (4c397)  52.7 48.3 58.5 63.0  - - -  56.4  68.8 71.5 73.2  73.1 65.6 71.7 74.4  Table B.4. ImageNet top-1 accuracy of models trained with few labels. See Table 7 for top-5 accuracy.  B.6. Linear Evaluation  For linear evaluation, we follow similar procedure as efac81ne-tuning (described in Appendix B.5), except that a larger learning rate of 1.6 (following LearningRate = 0.1 c397 BatchSize/256) and longer training of 90 epochs. Alternatively, using LARS optimizer with the pretraining hyper-parameters also yield similar results. Furthermore, we efac81nd that attaching the linear classiefac81er on top of the base encoder (with a stop_gradient on the input to linear classiefac81er to prevent the label information from inefac82uencing the encoder) and train them simultaneously during the pretraining achieves similar performance.  B.7. Correlation Between Linear Evaluation and Fine-Tuning  Here we study the correlation between linear evaluation and efac81ne-tuning under different settings of training step and network architecture. Figure B.5 shows linear evaluation versus efac81ne-tuning when training epochs of a ResNet-50 (using batch size of 4096) are varied from 50 to 3200 as in Figure B.2. While they are almost linearly correlated, it seems efac81ne-tuning on a small fraction  0500100015002000Ranking0246810121416Squared eigenvalue0500100015002000Ranking10e288921110e28892910e28892710e28892510e28892310e288921101Squared eigenvalue0cA Simple Framework for Contrastive Learning of Visual Representations  of labels beneefac81ts more from training longer.  Figure B.5. Top-1 accuracy of models trained in different epochs (from Figure B.2), under linear evaluation and efac81ne-tuning.  Figure B.6 shows shows linear evaluation versus efac81ne-tuning for different architectures of choice.  Figure B.6. Top-1 accuracy of different architectures under linear evaluation and efac81ne-tuning.  B.8. Transfer Learning  We evaluated the performance of our self-supervised representation for transfer learning in two settings: linear evaluation, where a logistic regression classiefac81er is trained to classify a new dataset based on the self-supervised representation learned on ImageNet, and efac81ne-tuning, where we allow all weights to vary during training. In both cases, we follow the approach described by Kornblith et al. (2019), although our preprocessing differs slightly.  B.8.1. METHODS Datasets We investigated transfer learning performance on the Food-101 dataset (Bossard et al., 2014), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009), Birdsnap (Berg et al., 2014), the SUN397 scene dataset (Xiao et al., 2010), Stanford Cars (Krause et al., 2013), FGVC Aircraft (Maji et al., 2013), the PASCAL VOC 2007 classiefac81cation task (Everingham et al., 2010), the Describable Textures Dataset (DTD) (Cimpoi et al., 2014), Oxford-IIIT Pets (Parkhi et al., 2012), Caltech-101 (Fei-Fei et al., 2004), and Oxford 102 Flowers (Nilsback & Zisserman, 2008). We follow the evaluation protocols in the papers introducing these datasets, i.e., we report top-1 accuracy for Food-101, CIFAR-10, CIFAR-100, Birdsnap, SUN397, Stanford Cars, and DTD; mean per-class accuracy for FGVC Aircraft, Oxford-IIIT Pets, Caltech-101, and Oxford 102 Flowers; and the 11-point mAP metric as deefac81ned in Everingham et al. (2010) for PASCAL VOC 2007. For DTD and SUN397, the dataset creators deefac81ned multiple train/test splits; we report results only for the efac81rst split. Caltech-101 deefac81nes no train/test split, so we randomly chose 30 images per class and test on the remainder, for fair comparison with previous work (Donahue et al., 2014; Simonyan & Zisserman, 2014). We used the validation sets speciefac81ed by the dataset creators to select hyperparameters for FGVC Aircraft, PASCAL VOC  6264666870Linear eval35.037.540.042.545.047.550.0Fine-tuning on 1%6264666870Linear eval60626466Fine-tuning on 10%5055606570Linear eval303336394245485154Finetune (1%)Width1x2x4xDepth1834501011525055606570Linear eval51545760636669Finetune (10%)Width1x2x4xDepth1834501011520cA Simple Framework for Contrastive Learning of Visual Representations  2007, DTD, and Oxford 102 Flowers. For other datasets, we held out a subset of the training set for validation while performing hyperparameter tuning. After selecting the optimal hyperparameters on the validation set, we retrained the model using the selected parameters using all training and validation images. We report accuracy on the test set.  Transfer Learning via a Linear Classiefac81er We trained an (cid:96)2-regularized multinomial logistic regression classiefac81er on features extracted from the frozen pretrained network. We used L-BFGS to optimize the softmax cross-entropy objective and we did not apply data augmentation. As preprocessing, all images were resized to 224 pixels along the shorter side using bicubic resampling, after which we took a 224 c397 224 center crop. We selected the (cid:96)2 regularization parameter from a range of 45 logarithmically spaced values between 10e288926 and 105.  Transfer Learning via Fine-Tuning We efac81ne-tuned the entire network using the weights of the pretrained network as initialization. We trained for 20,000 steps at a batch size of 256 using SGD with Nesterov momentum with a momentum parameter of 0.9. We set the momentum parameter for the batch normalization statistics to max(1 e28892 10/s, 0.9) where s is the number of steps per epoch. As data augmentation during efac81ne-tuning, we performed only random crops with resize and efac82ips; in contrast to pretraining, we did not perform color augmentation or blurring. At test time, we resized images to 256 pixels along the shorter side and took a 224 c397 224 center crop. (Additional accuracy improvements may be possible with further optimization of data augmentation, particularly on the CIFAR-10 and CIFAR-100 datasets.) We selected the learning rate and weight decay, with a grid of 7 logarithmically spaced learning rates between 0.0001 and 0.1 and 7 logarithmically spaced values of weight decay between 10e288926 and 10e288923, as well as no weight decay. We divide these values of weight decay by the learning rate.  Training from Random Initialization We trained the network from random initialization using the same procedure as for efac81ne-tuning, but for longer, and with an altered hyperparameter grid. We chose hyperparameters from a grid of 7 logarithmically spaced learning rates between 0.001 and 1.0 and 8 logarithmically spaced values of weight decay between 10e288925 and 10e288921.5. Importantly, our random initialization baselines are trained for 40,000 steps, which is sufefac81ciently long to achieve near-maximal accuracy, as demonstrated in Figure 8 of Kornblith et al. (2019). On Birdsnap, there are no statistically signiefac81cant differences among methods, and on Food-101, Stanford Cars, and FGVC Aircraft datasets, efac81ne-tuning provides only a small advantage over training from random initialization. However, on the remaining 8 datasets, pretraining has clear advantages.  Supervised Baselines We compare against architecturally identical ResNet models trained on ImageNet with standard cross-entropy loss. These models are trained with the same data augmentation as our self-supervised models (crops, strong color augmentation, and blur) and are also trained for 1000 epochs. We found that, although stronger data augmentation and longer training time do not beneefac81t accuracy on ImageNet, these models performed signiefac81cantly better than a supervised baseline trained for 90 epochs and ordinary data augmentation for linear evaluation on a subset of transfer datasets. The supervised ResNet-50 baseline achieves 76.3% top-1 accuracy on ImageNet, vs. 69.3% for the self-supervised counterpart, while the ResNet-50 (4c397) baseline achieves 78.3%, vs. 76.5% for the self-supervised model.  Statistical Signiefac81cance Testing We test for the signiefac81cance of differences between model with a permutation test. Given predictions of two models, we generate 100,000 samples from the null distribution by randomly exchanging predictions for each example and computing the difference in accuracy after performing this randomization. We then compute the percentage of samples from the null distribution that are more extreme than the observed difference in predictions. For top-1 accuracy, this procedure yields the same result as the exact McNemar test. The assumption of exchangeability under the null hypothesis is also valid for mean per-class accuracy, but not when computing average precision curves. Thus, we perform signiefac81cance testing for a difference in accuracy on VOC 2007 rather than a difference in mAP. A caveat of this procedure is that it does not consider run-to-run variability when training the models, only variability arising from using a efac81nite sample of images for evaluation.  B.8.2. RESULTS WITH STANDARD RESNET The ResNet-50 (4c397) results shown in Table 8 of the text show no clear advantage to the supervised or self-supervised models. With the narrower ResNet-50 architecture, however, supervised learning maintains a clear advantage over self-supervised learning. The supervised ResNet-50 model outperforms the self-supervised model on all datasets with linear evaluation, and most (10 of 12) datasets with efac81ne-tuning. The weaker performance of the ResNet model compared to the ResNet (4c397)  0cA Simple Framework for Contrastive Learning of Visual Representations  Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers  Linear evaluation: SimCLR (ours) 68.4 72.3 Supervised Fine-tuned: SimCLR (ours) 88.2 88.3 Supervised Random init 86.9  90.6 93.6  97.7 97.5 95.9  71.6 78.3  85.9 86.4 80.2  37.4 53.7  75.9 75.8 76.1  58.8 61.9  63.5 64.3 53.6  50.3 66.7  91.3 92.1 91.4  50.3 61.0  88.1 86.0 85.9  80.5 82.8  84.1 85.0 67.3  74.5 83.6 74.9 91.5  73.2 89.2 74.6 92.1 64.8 81.5  90.3 94.5  92.1 93.3 72.6  91.2 94.7  97.0 97.6 92.0  Table B.5. Comparison of transfer learning performance of our self-supervised approach with supervised baselines across 12 natural image datasets, using ImageNet-pretrained ResNet models. See also Figure 8 for results with the ResNet (4c397) architecture.  model may relate to the accuracy gap between the supervised and self-supervised models on ImageNet. The self-supervised ResNet gets 69.3% top-1 accuracy, 6.8% worse than the supervised model in absolute terms, whereas the self-supervised ResNet (4c397) model gets 76.5%, which is only 1.8% worse than the supervised model.  B.9. CIFAR-10  While we focus on using ImageNet as the main dataset for pretraining our unsupervised model, our method also works with other datasets. We demonstrate it by testing on CIFAR-10 as follows.  Setup As our goal is not to optimize CIFAR-10 performance, but rather to provide further conefac81rmation of our observations on ImageNet, we use the same architecture (ResNet-50) for CIFAR-10 experiments. Because CIFAR-10 images are much smaller than ImageNet images, we replace the efac81rst 7x7 Conv of stride 2 with 3x3 Conv of stride 1, and also remove the efac81rst max pooling operation. For data augmentation, we use the same Inception crop (efac82ip and resize to 32x32) as ImageNet,15 and color distortion (strength=0.5), leaving out Gaussian blur. We pretrain with learning rate in {0.5, 1.0, 1.5}, temperature in {0.1, 0.5, 1.0}, and batch size in {256, 512, 1024, 2048, 4096}. The rest of the settings (including optimizer, weight decay, etc.) are the same as our ImageNet training. Our best model trained with batch size 1024 can achieve a linear evaluation accuracy of 94.0%, compared to 95.1% from the supervised baseline using the same architecture and batch size. The best self-supervised model that reports linear evaluation result on CIFAR-10 is AMDIM (Bachman et al., 2019), which achieves 91.2% with a model 25c397 larger than ours. We note that our model can be improved by incorporating extra data augmentations as well as using a more suitable base network.  Performance under different batch sizes and training steps Figure B.7 shows the linear evaluation performance under different batch sizes and training steps. The results are consistent with our observations on ImageNet, although the largest batch size of 4096 seems to cause a small degradation in performance on CIFAR-10.  Figure B.7. Linear evaluation of ResNet-50 (with ad- justed stem) trained with different batch size and epochs on CIFAR-10 dataset. Each bar is averaged over 3 runs with different learning rates (0.5, 1.0, 1.5) and temperature cf84 = 0.5. Error bar denotes standard deviation.  15It is worth noting that, although CIFAR-10 images are much smaller than ImageNet images and image size does not differ among  examples, cropping with resizing is still a very effective augmentation for contrastive learning.  1002003004005006007008009001000Training epochs8082848688909294Top 1Batch size2565121024204840960cA Simple Framework for Contrastive Learning of Visual Representations  Optimal temperature under different batch sizes Figure B.8 shows the linear evaluation of model trained with three different temperatures under various batch sizes. We efac81nd that when training to convergence (e.g. training epochs > 300), the optimal temperature in {0.1, 0.5, 1.0} is 0.5 and seems consistent regardless of the batch sizes. However, the performance with cf84 = 0.1 improves as batch size increases, which may suggest a small shift of optimal temperature towards 0.1.  (a) Training epochs e289a4 300  (b) Training epochs > 300  Figure B.8. Linear evaluation of the model (ResNet-50) trained with three temperatures on different batch sizes on CIFAR-10. Each bar is averaged over multiple runs with different learning rates and total train epochs. Error bar denotes standard deviation.  B.10. Tuning For Other Loss Functions  The learning rate that works best for NT-Xent loss may not be a good learning rate for other loss functions. To ensure a fair comparison, we also tune hyperparameters for both margin loss and logistic loss. Speciefac81cally, we tune learning rate in {0.01, 0.1, 0.3, 0.5, 1.0} for both loss functions. We further tune the margin in {0, 0.4, 0.8, 1.6} for margin loss, the temperature in {0.1, 0.2, 0.5, 1.0} for logistic loss. For simplicity, we only consider the negatives from one augmentation view (instead of both sides), which slightly impairs performance but ensures fair comparison.  C. Further Comparison to Related Methods As we have noted in the main text, most individual components of SimCLR have appeared in previous work, and the improved performance is a result of a combination of these design choices. Table C.1 provides a high-level comparison of the design choices of our method with those of previous methods. Compared with previous work, our design choices are generally simpler.  Data Augmentation Custom  Model CPC v2 AMDIM Fast AutoAug. CMC Fast AutoAug. Crop+color MoCo PIRL Crop+color SimCLR Crop+color+blur  Base Encoder ResNet-161 (modiefac81ed) Custom ResNet ResNet-50 (2c397, L+ab) ResNet-50 (4c397) ResNet-50 (2c397) ResNet-50 (4c397)  Loss Xent  Projection Head PixelCNN Non-linear MLP Xent w/ clip,reg Xent w/ (cid:96)2, cf84 Linear layer Xent w/ (cid:96)2, cf84 Linear layer Linear layer Xent w/ (cid:96)2, cf84 Non-linear MLP Xent w/ (cid:96)2, cf84  Batch Size 512# 1008# 156e28897 256e28897 1024e28897 4096  Train Epochs e288bc200 150 280 200 800 1000  Table C.1. A high-level comparison of design choices and training setup (for best result on ImageNet) for each method. Note that descriptions provided here are general; even when they match for two methods, formulations and implementations may differ (e.g. for color augmentation). Refer to the original papers for more details. #Examples are split into multiple patches, which enlarges the effective batch size. e28897A memory bank is employed.  In below, we provide an in-depth comparison of our method to the recently proposed contrastive representation learning methods:  e280a2 DIM/AMDIM (Hjelm et al., 2018; Bachman et al., 2019) achieve global-to-local/local-to-neighbor prediction by predicting the middle layer of ConvNet. The ConvNet is a ResNet that has bewen modiefac81ed to place signiefac81cant constraints on the receptive efac81elds of the network (e.g. replacing many 3x3 Convs with 1x1 Convs). In our framework, we decouple the prediction task and encoder architecture, by random cropping (with resizing) and using the efac81nal  256512102420484096Batch size75.077.580.082.585.087.590.092.595.0Top 1Temperature0.10.51.0256512102420484096Batch size909192939495Top 1Temperature0.10.51.00cA Simple Framework for Contrastive Learning of Visual Representations  representations of two augmented views for prediction, so we can use standard and more powerful ResNets. Our NT-Xent loss function leverages normalization and temperature to restrict the range of similarity scores, whereas they use a tanh function with regularization. We use a simpler data augmentation policy, while they use FastAutoAugment for their best result.  e280a2 CPC v1 and v2 (Oord et al., 2018; Hc3a9naff et al., 2019) deefac81ne the context prediction task using a deterministic strategy to split examples into patches, and a context aggregation network (a PixelCNN) to aggregate these patches. The base encoder network sees only patches, which are considerably smaller than the original image. We decouple the prediction task and the encoder architecture, so we do not require a context aggregation network, and our encoder can look at the images of wider spectrum of resolutions. In addition, we use the NT-Xent loss function, which leverages normalization and temperature, whereas they use an unnormalized cross-entropy-based objective. We use simpler data augmentation. e280a2 InstDisc, MoCo, PIRL (Wu et al., 2018; He et al., 2019; Misra & van der Maaten, 2019) generalize the Exemplar approach originally proposed by Dosovitskiy et al. (2014) and leverage an explicit memory bank. We do not use a memory bank; we efac81nd that, with a larger batch size, in-batch negative example sampling sufefac81ces. We also utilize a nonlinear projection head, and use the representation before the projection head. Although we use similar types of augmentations (e.g., random crop and color distortion), we expect speciefac81c parameters may be different.  e280a2 CMC (Tian et al., 2019) uses a separated network for each view, while we simply use a single network shared for all randomly augmented views. The data augmentation, projection head and loss function are also different. We use larger batch size instead of a memory bank.  e280a2 Whereas Ye et al. (2019) maximize similarity between augmented and unaugmented copies of the same image, we apply data augmentation symmetrically to both branches of our framework (Figure 2). We also apply a nonlinear projection on the output of base feature network, and use the representation before projection network, whereas Ye et al. (2019) use the linearly projected efac81nal hidden vector as the representation. When training with large batch sizes using multiple accelerators, we use global BN to avoid shortcuts that can greatly decrease representation quality.  0c'\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-dubai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-phoenix",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-defeat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-logging",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "respected-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "from pororo import Pororo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "according-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfSummarizer:\n",
    "    def __init__(self, pdf_dir):\n",
    "        self.mt = Pororo(task=\"translation\", lang=\"multi\")\n",
    "        self.summarizer = Pororo(task='summary', model='extractive', lang='ko')\n",
    "\n",
    "        self.txt = self.pdf2text(pdf_dir)\n",
    "        self.trans = self.text_translation(self.txt[:1000], source='en', target='ko')\n",
    "        self.sum = self.summarize(self.trans, gap=30)\n",
    "\n",
    "    def pdf2text(self, pdf_dir):\n",
    "        txt = textract.process(pdf_name, method='pdfminer')\n",
    "        txt_new = str(txt)\n",
    "        txt_new = txt_new.replace(\"\\\\n\", \" \")\n",
    "        txt_new = txt_new.replace('\\\\x', '')\n",
    "        return txt_new\n",
    "\n",
    "    def text_translation(self, txt_in, source='en', target='ko'):\n",
    "        trans = self.mt(txt_in, src=source, tgt=target)\n",
    "        return trans\n",
    "\n",
    "    def summarize(self, src, gap=10):\n",
    "        parsed = src.split('.')\n",
    "\n",
    "        sum_ = []\n",
    "        for i in range(0, len(parsed), gap):\n",
    "            sum_.append(self.summarizer('. '.join(parsed[i:i+gap])))\n",
    "        sum__ = '\\\\n'.join(sum_)\n",
    "        return sum__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "motivated-device",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() should return None, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e74b4619748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpdf_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'simclr.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPdfSummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() should return None, not 'str'"
     ]
    }
   ],
   "source": [
    "pdf_name = 'pdf/simclr.pdf'\n",
    "pdf = PdfSummarizer(pdf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-shoot",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-honor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "super-logging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 최근 잠재적 공간에서의 대조적 학습을 바탕으로 한 차별적 접근법이 큰 약속을 보이면서 최첨단 예술 결과(2006년 하드셀  et 알, 2014년 도소비츠키  et 아르, 오르트  et 르,2018년, 바흐만 등)를 달성하고 있다'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_parsed[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "italic-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "summm = Pororo(task='summarization', model='bullet', lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "agreed-corps",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 7.79 GiB total capacity; 5.58 GiB already allocated; 132.81 MiB free; 6.04 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-f2876cf43f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_parsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/pororo/tasks/text_summarization.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, beam, temperature, top_k, top_p, no_repeat_ngram_size, len_penalty)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mlen_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         )\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/pororo/tasks/text_summarization.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text, beam, temperature, top_k, top_p, no_repeat_ngram_size, len_penalty)\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"▁]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"】\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"【\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             ],\n\u001b[1;32m    352\u001b[0m         )\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/pororo/models/bart/KoBART.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, beam, sampling, temperature, sampling_topk, sampling_topp, length_penalty, max_len_a, max_len_b, no_repeat_ngram_size, return_tokens, bad_words_ids)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mlength_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len_a\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_len_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         )\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m                 \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             )\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m             )\n\u001b[1;32m   1612\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1297\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         )\n\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, encoder_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m                 )\n\u001b[1;32m   1049\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, encoder_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_layer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             )\n\u001b[1;32m    424\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;31m# cross_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_value_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_value_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# reuse k, v, self_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/pororo/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36m_shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     def forward(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 7.79 GiB total capacity; 5.58 GiB already allocated; 132.81 MiB free; 6.04 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "print(summm(txt_parsed[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "unlimited-better",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"간단한 시각대표 컨트라스트 학습 기본 Ting Chen 1 Simon Kornblith 1 Mohammad Noruzi 1 George Hunton 1,02 2 luj 1 1) GL'이다. sc [3 대 9 0 7 5 0] 2002: viXr a bustruct is SimCLR: 시각적 표현에 대한 대조적 학습을 위한 간단한 틀을 제시한다. 최근 제안된 대조적 자기감독 알고리즘을 전문 건축가나 메모리 뱅크가 필요 없이 간소화한다. 대조적인 예측 과제가 유용한 논평을 배울 수 있는 것을 이해하기 위해 우리는 체계적으로 우리 틀의 주요 부품을 연구한다. 데이터 조작의 구성(1)이 효과적인 예측 과제인 데파크 81회에서 중요한 역할을 하고, 보고서 송환과 대조적 손실 사이에서 학습 가능한 비라인어 변신을 도입하는 것을 보여주고, (3) 대조 학습에 비해 더 큰 부분 크기와 더 많은 훈련 단계에서 비교적 학습 이러한 스파크81회를 결합해 이미지넷에서 자기초과,준초과 학습을 위한 기\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_trans[4:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "chicken-peace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40883"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "aes = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "advised-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'We present YOLO, a new approach to object detection.Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "revolutionary-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trans = mt(sample, src='en', tgt='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "surprised-translation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'목적물 탐지에 대한 새로운 접근법인 YOLO를 제시하고 있다. 목적 탐지를 위한 프리어 작업은 탐지 수행을 위해 클래식기를 재포함한다. 대신 우리는 공간적으로 분리된 상자와 관련된 계층 분리를 위한 감압 문제로 물체 감지를 틀어놓는다. 한 개의 신경망은 한 가지 평가에서 완전한 이미지에서 직접 상자와 수업이 이루어질 것으로 예측한다. 검출 파이프라인 전체가 단일 네트워크이기 때문에 검출 성능에 직접 최적화된 엔드 투 엔드가 가능하다. 우리의 단일 건축물은 매우 빠르다 우리 기지 YOLO 모델이 초당 45개의 프레임으로 실시간으로 이미지를 처리한다. 네트워크의 작은 버전인 패스트 YOLO는 여전히 다른 실시간 감지기의 MAP 두 배를 달성하면서도 초당 155개의 프레임을 처리한다. 최첨단 감지 시스템에 비해 YOLO는 국내화 오류를 더 많이 만들지만 배경에 허위 양식을 예측할 가능성은 떨어진다. 마지막으로 YOLO는 아주 일반적인 물체 표현을 배운다 자연 영상부터 다른 영역까지 예술작품처럼 일반화할 때 DPM, R-CNN 등 기타 탐지 방식을 뛰어넘는다.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "adaptive-flush",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['검출 파이프라인 전체가 단일 네트워크', ' 검출 성능에 직접 최적화된 엔드 투 엔드가 가능']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summm(sample_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "intensive-trailer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'목적물 탐지에 대한 새로운 접근법인 YOLO를 제시하고 있다. 검출 파이프라인 전체가 단일 네트워크이기 때문에 검출 성능에 직접 최적화된 엔드 투 엔드가 가능하다. 최첨단 감지 시스템에 비해 YOLO는 국내화 오류를 더 많이 만들지만 배경에 허위 양식을 예측할 가능성은 떨어진다.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summarizer(sample_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-thunder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-trick",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-tackle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-spirituality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pororo",
   "language": "python",
   "name": "pororo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
